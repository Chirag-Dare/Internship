{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f47dfb",
   "metadata": {},
   "source": [
    "Web Scraping Assignment - 4\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ccb2d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException,ElementClickInterceptedException,StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad52e3",
   "metadata": {},
   "source": [
    "1. Program to scrape the details of most viewed videos on YouTube from Wikipedia. \n",
    "    \n",
    "    Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos \n",
    "    \n",
    "    Details to be scraped are:\n",
    "    \n",
    "    A) Rank\n",
    "    \n",
    "    B) Name\n",
    "    \n",
    "    C) Artist\n",
    "    \n",
    "    D) Upload date\n",
    "    \n",
    "    E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4545d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sending a request to a website for scraping data\n",
    "page = requests.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53bbe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating WebDriver instance for controlling the Chrome browser using Selenium\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dacaae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening webpage in the browser window controlled by the WebDriver instance\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a44bd62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for storing the details of the most viewed YouTube videos \n",
    "video_rank = []\n",
    "video_name = []\n",
    "video_artist = []\n",
    "video_upload_date = []\n",
    "views_on_video = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7689ea7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 ', '2 ', '3 ', '4 ', '5 ', '6 ', '7 ', '8 ', '9 ', '10 ', '11 ', '12 ', '13 ', '14 ', '15 ', '16 ', '17 ', '18 ', '19 ', '20 ', '21 ', '22 ', '23 ', '24 ', '25 ', '26 ', '27 ', '28 ', '29 ', '30 ']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the videos rank in video_rank list\n",
    "rank = driver.find_elements(By.XPATH,\"/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr/td[1]\")\n",
    "for i in rank:\n",
    "    video_rank.append(i.text.replace('.',' '))\n",
    "print(video_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e764bd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Baby Shark Dance\"', '\"Despacito\"', '\"Johny Johny Yes Papa\"', '\"Bath Song\"', '\"Shape of You\"', '\"See You Again\"', '\"Phonics Song with Two Words\"', '\"Wheels on the Bus\"', '\"Uptown Funk\"', '\"Learning Colors – Colorful Eggs on a Farm\"', '\"Gangnam Style\"', '\"Masha and the Bear – Recipe for Disaster\"', '\"Dame Tu Cosita\"', '\"Axel F\"', '\"Sugar\"', '\"Roar\"', '\"Counting Stars\"', '\"Sorry\"', '\"Baa Baa Black Sheep\"', '\"Thinking Out Loud\"', '\"Waka Waka (This Time for Africa)\"', '\"Dark Horse\"', '\"Lakdi Ki Kathi\"', '\"Faded\"', '\"Perfect\"', '\"Let Her Go\"', '\"Girls Like You\"', '\"Humpty the train on a fruits ride\"', '\"Lean On\"', '\"Bailando\"']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the videos name in video_name list\n",
    "name = driver.find_elements(By.XPATH,\"/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr/td[2]\")\n",
    "for i in name:\n",
    "    video_name.append(i.text.split('[')[0])\n",
    "print(video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99b77aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Pinkfong Baby Shark - Kids' Songs & Stories\", 'Luis Fonsi', 'LooLoo Kids', 'Cocomelon – Nursery Rhymes', 'Ed Sheeran', 'Wiz Khalifa', 'ChuChu TV', 'Cocomelon – Nursery Rhymes', 'Mark Ronson', 'Miroshka TV', 'Psy', 'Get Movies', 'El Chombo', 'Crazy Frog', 'Maroon 5', 'Katy Perry', 'OneRepublic', 'Justin Bieber', 'Cocomelon – Nursery Rhymes', 'Ed Sheeran', 'Shakira', 'Katy Perry', 'Jingle Toons', 'Alan Walker', 'Ed Sheeran', 'Passenger', 'Maroon 5', 'Kiddiestv Hindi – Nursery Rhymes & Kids Songs', 'Major Lazer', 'Enrique Iglesias']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the artists name in video_artist list\n",
    "artist = driver.find_elements(By.XPATH,\"/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr/td[3]\")\n",
    "for i in artist:\n",
    "    video_artist.append(i.text)\n",
    "print(video_artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3982bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['June 17, 2016', 'January 12, 2017', 'October 8, 2016', 'May 2, 2018', 'January 30, 2017', 'April 6, 2015', 'March 6, 2014', 'May 24, 2018', 'November 19, 2014', 'February 27, 2018', 'July 15, 2012', 'January 31, 2012', 'April 5, 2018', 'June 16, 2009', 'January 14, 2015', 'September 5, 2013', 'May 31, 2013', 'October 22, 2015', 'June 25, 2018', 'October 7, 2014', 'June 4, 2010', 'February 20, 2014', 'June 14, 2018', 'December 3, 2015', 'November 9, 2017', 'July 25, 2012', 'May 31, 2018', 'January 26, 2018', 'March 22, 2015', 'April 11, 2014']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the videos upload date in video_upload_date list\n",
    "upload_date = driver.find_elements(By.XPATH,\"/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr/td[5]\")\n",
    "for i in upload_date:\n",
    "    video_upload_date.append(i.text)\n",
    "print(video_upload_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82cce0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12.85', '8.16', '6.70', '6.20', '6.00', '5.89', '5.30', '5.24', '4.92', '4.89', '4.80', '4.55', '4.35', '3.91', '3.87', '3.80', '3.79', '3.66', '3.64', '3.60', '3.59', '3.52', '3.48', '3.45', '3.45', '3.44', '3.42', '3.41', '3.38', '3.38']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the number of views on videos in views_on_video list\n",
    "views = driver.find_elements(By.XPATH,\"/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr/td[4]\")\n",
    "for i in views:\n",
    "    views_on_video.append(i.text)\n",
    "print(views_on_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0226548a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30 30 30 30\n"
     ]
    }
   ],
   "source": [
    "# Checking the lengths of all the lists\n",
    "print(len(video_rank),len(video_name),len(video_artist),len(video_upload_date),len(views_on_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "548414a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload Date</th>\n",
       "      <th>Views (Billions)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Baby Shark Dance\"</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>12.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Despacito\"</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>8.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Johny Johny Yes Papa\"</td>\n",
       "      <td>LooLoo Kids</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td>6.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Bath Song\"</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>6.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Shape of You\"</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>January 30, 2017</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"See You Again\"</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>5.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"Phonics Song with Two Words\"</td>\n",
       "      <td>ChuChu TV</td>\n",
       "      <td>March 6, 2014</td>\n",
       "      <td>5.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Wheels on the Bus\"</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 24, 2018</td>\n",
       "      <td>5.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Uptown Funk\"</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>November 19, 2014</td>\n",
       "      <td>4.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>4.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"Gangnam Style\"</td>\n",
       "      <td>Psy</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>January 31, 2012</td>\n",
       "      <td>4.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"Dame Tu Cosita\"</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>April 5, 2018</td>\n",
       "      <td>4.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"Axel F\"</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>June 16, 2009</td>\n",
       "      <td>3.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"Sugar\"</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>January 14, 2015</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"Roar\"</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>September 5, 2013</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"Counting Stars\"</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>May 31, 2013</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"Sorry\"</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>3.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\"Baa Baa Black Sheep\"</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>June 25, 2018</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\"Thinking Out Loud\"</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>October 7, 2014</td>\n",
       "      <td>3.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>June 4, 2010</td>\n",
       "      <td>3.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"Dark Horse\"</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>February 20, 2014</td>\n",
       "      <td>3.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"Lakdi Ki Kathi\"</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>June 14, 2018</td>\n",
       "      <td>3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"Faded\"</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>December 3, 2015</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>\"Perfect\"</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>November 9, 2017</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\"Let Her Go\"</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>July 25, 2012</td>\n",
       "      <td>3.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>\"Girls Like You\"</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>May 31, 2018</td>\n",
       "      <td>3.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>\"Humpty the train on a fruits ride\"</td>\n",
       "      <td>Kiddiestv Hindi – Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>January 26, 2018</td>\n",
       "      <td>3.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\"Lean On\"</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>March 22, 2015</td>\n",
       "      <td>3.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>\"Bailando\"</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>April 11, 2014</td>\n",
       "      <td>3.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Video Name  \\\n",
       "Rank                                                \n",
       "1                              \"Baby Shark Dance\"   \n",
       "2                                     \"Despacito\"   \n",
       "3                          \"Johny Johny Yes Papa\"   \n",
       "4                                     \"Bath Song\"   \n",
       "5                                  \"Shape of You\"   \n",
       "6                                 \"See You Again\"   \n",
       "7                   \"Phonics Song with Two Words\"   \n",
       "8                             \"Wheels on the Bus\"   \n",
       "9                                   \"Uptown Funk\"   \n",
       "10    \"Learning Colors – Colorful Eggs on a Farm\"   \n",
       "11                                \"Gangnam Style\"   \n",
       "12     \"Masha and the Bear – Recipe for Disaster\"   \n",
       "13                               \"Dame Tu Cosita\"   \n",
       "14                                       \"Axel F\"   \n",
       "15                                        \"Sugar\"   \n",
       "16                                         \"Roar\"   \n",
       "17                               \"Counting Stars\"   \n",
       "18                                        \"Sorry\"   \n",
       "19                          \"Baa Baa Black Sheep\"   \n",
       "20                            \"Thinking Out Loud\"   \n",
       "21             \"Waka Waka (This Time for Africa)\"   \n",
       "22                                   \"Dark Horse\"   \n",
       "23                               \"Lakdi Ki Kathi\"   \n",
       "24                                        \"Faded\"   \n",
       "25                                      \"Perfect\"   \n",
       "26                                   \"Let Her Go\"   \n",
       "27                               \"Girls Like You\"   \n",
       "28            \"Humpty the train on a fruits ride\"   \n",
       "29                                      \"Lean On\"   \n",
       "30                                     \"Bailando\"   \n",
       "\n",
       "                                             Artist        Upload Date  \\\n",
       "Rank                                                                     \n",
       "1       Pinkfong Baby Shark - Kids' Songs & Stories      June 17, 2016   \n",
       "2                                        Luis Fonsi   January 12, 2017   \n",
       "3                                       LooLoo Kids    October 8, 2016   \n",
       "4                        Cocomelon – Nursery Rhymes        May 2, 2018   \n",
       "5                                        Ed Sheeran   January 30, 2017   \n",
       "6                                       Wiz Khalifa      April 6, 2015   \n",
       "7                                         ChuChu TV      March 6, 2014   \n",
       "8                        Cocomelon – Nursery Rhymes       May 24, 2018   \n",
       "9                                       Mark Ronson  November 19, 2014   \n",
       "10                                      Miroshka TV  February 27, 2018   \n",
       "11                                              Psy      July 15, 2012   \n",
       "12                                       Get Movies   January 31, 2012   \n",
       "13                                        El Chombo      April 5, 2018   \n",
       "14                                       Crazy Frog      June 16, 2009   \n",
       "15                                         Maroon 5   January 14, 2015   \n",
       "16                                       Katy Perry  September 5, 2013   \n",
       "17                                      OneRepublic       May 31, 2013   \n",
       "18                                    Justin Bieber   October 22, 2015   \n",
       "19                       Cocomelon – Nursery Rhymes      June 25, 2018   \n",
       "20                                       Ed Sheeran    October 7, 2014   \n",
       "21                                          Shakira       June 4, 2010   \n",
       "22                                       Katy Perry  February 20, 2014   \n",
       "23                                     Jingle Toons      June 14, 2018   \n",
       "24                                      Alan Walker   December 3, 2015   \n",
       "25                                       Ed Sheeran   November 9, 2017   \n",
       "26                                        Passenger      July 25, 2012   \n",
       "27                                         Maroon 5       May 31, 2018   \n",
       "28    Kiddiestv Hindi – Nursery Rhymes & Kids Songs   January 26, 2018   \n",
       "29                                      Major Lazer     March 22, 2015   \n",
       "30                                 Enrique Iglesias     April 11, 2014   \n",
       "\n",
       "     Views (Billions)  \n",
       "Rank                   \n",
       "1               12.85  \n",
       "2                8.16  \n",
       "3                6.70  \n",
       "4                6.20  \n",
       "5                6.00  \n",
       "6                5.89  \n",
       "7                5.30  \n",
       "8                5.24  \n",
       "9                4.92  \n",
       "10               4.89  \n",
       "11               4.80  \n",
       "12               4.55  \n",
       "13               4.35  \n",
       "14               3.91  \n",
       "15               3.87  \n",
       "16               3.80  \n",
       "17               3.79  \n",
       "18               3.66  \n",
       "19               3.64  \n",
       "20               3.60  \n",
       "21               3.59  \n",
       "22               3.52  \n",
       "23               3.48  \n",
       "24               3.45  \n",
       "25               3.45  \n",
       "26               3.44  \n",
       "27               3.42  \n",
       "28               3.41  \n",
       "29               3.38  \n",
       "30               3.38  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_details = {\n",
    "    'Video Name':video_name,\n",
    "    'Artist':video_artist,\n",
    "    'Upload Date':video_upload_date,\n",
    "    'Views (Billions)':views_on_video\n",
    "}\n",
    "\n",
    "# Creating dataframe\n",
    "df = pd.DataFrame(video_details,index=video_rank)\n",
    "\n",
    "# Displaying dataframe\n",
    "df.index.name = 'Rank'\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06c6a7e",
   "metadata": {},
   "source": [
    "2. Program to scrape the details of team India’s international fixtures from bcci.tv.\n",
    "\n",
    "    Url = https://www.bcci.tv/.\n",
    "\n",
    "    Details to be scraped are:\n",
    "\n",
    "    A) Match title (I.e. 1st ODI)\n",
    "\n",
    "    B) Series\n",
    "\n",
    "    C) Place\n",
    "\n",
    "    D) Date\n",
    "\n",
    "    E) Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72ef27bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sending a request to a website for scraping data\n",
    "page = requests.get(\"https://www.bcci.tv/\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa852c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating WebDriver instance for controlling the Chrome browser using Selenium\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "622203d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening webpage in the browser window controlled by the WebDriver instance\n",
    "driver.get(\"https://www.bcci.tv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37ebc965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cliking on the international option\n",
    "international_fixtures = driver.find_element(By.XPATH,\"/html/body/nav/div[1]/div[2]/ul[1]/li[2]/a\")\n",
    "international_fixtures.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d71028e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    \n",
    "    # Scrolling the scroll bar for loading more fixtures\n",
    "    driver.execute_script(\"window.scrollBy(0,700)\")\n",
    "    \n",
    "    # Cliking on more fixtures option\n",
    "    more_fixtures = driver.find_element(By.XPATH,\"/html/body/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/button\")\n",
    "    more_fixtures.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6c971e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for storing the details of the team India’s international fixtures\n",
    "match_title = []\n",
    "series_name = []\n",
    "place = []\n",
    "match_date = [] \n",
    "match_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5473e4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1st T20I ', '2nd T20I ', '1st Test ', '3rd T20I ', '1st ODI ', '2nd ODI ', '2nd Test ', '3rd ODI ', '1st ODI ', '2nd ODI ', '3rd ODI ', '1st T20I ', '2nd T20I ', '3rd T20I ', '4th T20I ', '5th T20I ', '1st ODI ', '2nd ODI ', '3rd ODI ', '4th ODI ', '5th ODI ', '6th ODI ', '7th ODI ', '8th ODI ', '9th ODI ']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the match titles in the match_title list\n",
    "title = driver.find_elements(By.XPATH,\"//span[@class='matchOrderText ng-binding ng-scope']\")\n",
    "for i in title:\n",
    "    match_title.append(i.text.split('-')[0])\n",
    "print(match_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99dedc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INDIA WOMEN TOUR OF BANGLADESH 2023', 'INDIA WOMEN TOUR OF BANGLADESH 2023', 'INDIA TOUR OF WEST INDIES 2023', 'INDIA WOMEN TOUR OF BANGLADESH 2023', 'INDIA WOMEN TOUR OF BANGLADESH 2023', 'INDIA WOMEN TOUR OF BANGLADESH 2023', 'INDIA TOUR OF WEST INDIES 2023', 'INDIA WOMEN TOUR OF BANGLADESH 2023', 'INDIA TOUR OF WEST INDIES 2023', 'INDIA TOUR OF WEST INDIES 2023', 'INDIA TOUR OF WEST INDIES 2023', 'INDIA TOUR OF WEST INDIES 2023', 'INDIA TOUR OF WEST INDIES 2023', 'INDIA TOUR OF WEST INDIES 2023', 'INDIA TOUR OF WEST INDIES 2023', 'INDIA TOUR OF WEST INDIES 2023', 'ICC MENS WORLD CUP 2023', 'ICC MENS WORLD CUP 2023', 'ICC MENS WORLD CUP 2023', 'ICC MENS WORLD CUP 2023', 'ICC MENS WORLD CUP 2023', 'ICC MENS WORLD CUP 2023', 'ICC MENS WORLD CUP 2023', 'ICC MENS WORLD CUP 2023', 'ICC MENS WORLD CUP 2023']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the series name in the series_name list\n",
    "series = driver.find_elements(By.XPATH,\"//h5[@class='match-tournament-name ng-binding']\")\n",
    "for i in series:\n",
    "    series_name.append(i.text)\n",
    "print(series_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0163549a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shere Bangla National Stadium, Mirpur, Dhaka', 'Shere Bangla National Stadium, Mirpur, Dhaka', 'Windsor Park, Dominica', 'Shere Bangla National Stadium, Mirpur, Dhaka', 'Shere Bangla National Stadium, Mirpur, Dhaka', 'Shere Bangla National Stadium, Mirpur, Dhaka', \"Queen's Park Oval, Trinidad\", 'Shere Bangla National Stadium, Mirpur, Dhaka', 'Kensington Oval, Barbados', 'Kensington Oval, Barbados', 'Brian Lara Stadium, Trinidad', 'Brian Lara Stadium, Trinidad', 'National Stadium, Guyana', 'National Stadium, Guyana', 'Central Broward Regional Park Stadium Turf Ground, Florida', 'Central Broward Regional Park Stadium Turf Ground, Florida', 'MA Chidambaram Stadium, Chennai', 'Arun Jaitley Stadium, Delhi', 'Narendra Modi Stadium, Ahmedabad', 'Maharashtra Cricket Association Stadium, Pune', 'Himachal Pradesh Cricket Association Stadium, Dharamsala', 'Bharat Ratna Shri Atal Bihari Vajpayee Ekana Cricket Stadium, Lucknow', 'Wankhede Stadium, Mumbai', 'Eden Gardens, Kolkata', 'M Chinnaswamy Stadium, Bengaluru']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the match places name in the place list\n",
    "ground_name = []\n",
    "ground_city_name = []\n",
    "\n",
    "ground = driver.find_elements(By.XPATH,\"//span[@class='ng-binding ng-scope']\")\n",
    "for i in ground:\n",
    "    ground_name.append(i.text)\n",
    "\n",
    "city = driver.find_elements(By.XPATH,\"//span[@class='ng-binding']\")\n",
    "for i in city:\n",
    "    ground_city_name.append(i.text)\n",
    "\n",
    "# Combining both ground name and ground city name\n",
    "for i in range(len(ground)):\n",
    "    place.append(ground_name[i] + ' ' + ground_city_name[i])\n",
    "print(place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a92306ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9 JUL 2023', '11 JUL 2023', '12 JUL 2023', '13 JUL 2023', '16 JUL 2023', '19 JUL 2023', '20 JUL 2023', '22 JUL 2023', '27 JUL 2023', '29 JUL 2023', '1 AUG 2023', '3 AUG 2023', '6 AUG 2023', '8 AUG 2023', '12 AUG 2023', '13 AUG 2023', '8 OCT 2023', '11 OCT 2023', '15 OCT 2023', '19 OCT 2023', '22 OCT 2023', '29 OCT 2023', '2 NOV 2023', '5 NOV 2023', '11 NOV 2023']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the match dates in the match_date list\n",
    "date = driver.find_elements(By.XPATH,\"//div[@class='match-dates ng-binding']\")\n",
    "for i in date:\n",
    "    match_date.append(i.text)\n",
    "print(match_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b0446f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1:30 PM IST', '1:30 PM IST', '7:30 PM IST', '1:30 PM IST', '9:00 AM IST', '9:00 AM IST', '7:30 PM IST', '9:00 AM IST', '7:00 PM IST', '7:00 PM IST', '7:00 PM IST', '8:00 PM IST', '8:00 PM IST', '8:00 PM IST', '8:00 PM IST', '8:00 PM IST', '2:00 PM IST', '2:00 PM IST', '2:00 PM IST', '2:00 PM IST', '2:00 PM IST', '2:00 PM IST', '2:00 PM IST', '2:00 PM IST', '2:00 PM IST']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the match timings in the match_time list\n",
    "time = driver.find_elements(By.XPATH,\"//div[@class='match-time no-margin ng-binding']\")\n",
    "for i in time:\n",
    "    match_time.append(i.text)\n",
    "print(match_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1d15354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25 25 25 25\n"
     ]
    }
   ],
   "source": [
    "# Checking the lengths of all the lists\n",
    "print(len(match_title),len(series_name),len(place),len(match_date),len(match_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89f02f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match Title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur, Dhaka</td>\n",
       "      <td>9 JUL 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur, Dhaka</td>\n",
       "      <td>11 JUL 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1st Test</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Windsor Park, Dominica</td>\n",
       "      <td>12 JUL 2023</td>\n",
       "      <td>7:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur, Dhaka</td>\n",
       "      <td>13 JUL 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur, Dhaka</td>\n",
       "      <td>16 JUL 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur, Dhaka</td>\n",
       "      <td>19 JUL 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2nd Test</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Queen's Park Oval, Trinidad</td>\n",
       "      <td>20 JUL 2023</td>\n",
       "      <td>7:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3rd ODI</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur, Dhaka</td>\n",
       "      <td>22 JUL 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Kensington Oval, Barbados</td>\n",
       "      <td>27 JUL 2023</td>\n",
       "      <td>7:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Kensington Oval, Barbados</td>\n",
       "      <td>29 JUL 2023</td>\n",
       "      <td>7:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3rd ODI</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Brian Lara Stadium, Trinidad</td>\n",
       "      <td>1 AUG 2023</td>\n",
       "      <td>7:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Brian Lara Stadium, Trinidad</td>\n",
       "      <td>3 AUG 2023</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>National Stadium, Guyana</td>\n",
       "      <td>6 AUG 2023</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>National Stadium, Guyana</td>\n",
       "      <td>8 AUG 2023</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4th T20I</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Central Broward Regional Park Stadium Turf Gro...</td>\n",
       "      <td>12 AUG 2023</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5th T20I</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Central Broward Regional Park Stadium Turf Gro...</td>\n",
       "      <td>13 AUG 2023</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>MA Chidambaram Stadium, Chennai</td>\n",
       "      <td>8 OCT 2023</td>\n",
       "      <td>2:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Arun Jaitley Stadium, Delhi</td>\n",
       "      <td>11 OCT 2023</td>\n",
       "      <td>2:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3rd ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Narendra Modi Stadium, Ahmedabad</td>\n",
       "      <td>15 OCT 2023</td>\n",
       "      <td>2:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4th ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Maharashtra Cricket Association Stadium, Pune</td>\n",
       "      <td>19 OCT 2023</td>\n",
       "      <td>2:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5th ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Himachal Pradesh Cricket Association Stadium, ...</td>\n",
       "      <td>22 OCT 2023</td>\n",
       "      <td>2:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6th ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Bharat Ratna Shri Atal Bihari Vajpayee Ekana C...</td>\n",
       "      <td>29 OCT 2023</td>\n",
       "      <td>2:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7th ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Wankhede Stadium, Mumbai</td>\n",
       "      <td>2 NOV 2023</td>\n",
       "      <td>2:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8th ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Eden Gardens, Kolkata</td>\n",
       "      <td>5 NOV 2023</td>\n",
       "      <td>2:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9th ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>M Chinnaswamy Stadium, Bengaluru</td>\n",
       "      <td>11 NOV 2023</td>\n",
       "      <td>2:00 PM IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Match Title                               Series  \\\n",
       "0    1st T20I   INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "1    2nd T20I   INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "2    1st Test        INDIA TOUR OF WEST INDIES 2023   \n",
       "3    3rd T20I   INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "4     1st ODI   INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "5     2nd ODI   INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "6    2nd Test        INDIA TOUR OF WEST INDIES 2023   \n",
       "7     3rd ODI   INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "8     1st ODI        INDIA TOUR OF WEST INDIES 2023   \n",
       "9     2nd ODI        INDIA TOUR OF WEST INDIES 2023   \n",
       "10    3rd ODI        INDIA TOUR OF WEST INDIES 2023   \n",
       "11   1st T20I        INDIA TOUR OF WEST INDIES 2023   \n",
       "12   2nd T20I        INDIA TOUR OF WEST INDIES 2023   \n",
       "13   3rd T20I        INDIA TOUR OF WEST INDIES 2023   \n",
       "14   4th T20I        INDIA TOUR OF WEST INDIES 2023   \n",
       "15   5th T20I        INDIA TOUR OF WEST INDIES 2023   \n",
       "16    1st ODI               ICC MENS WORLD CUP 2023   \n",
       "17    2nd ODI               ICC MENS WORLD CUP 2023   \n",
       "18    3rd ODI               ICC MENS WORLD CUP 2023   \n",
       "19    4th ODI               ICC MENS WORLD CUP 2023   \n",
       "20    5th ODI               ICC MENS WORLD CUP 2023   \n",
       "21    6th ODI               ICC MENS WORLD CUP 2023   \n",
       "22    7th ODI               ICC MENS WORLD CUP 2023   \n",
       "23    8th ODI               ICC MENS WORLD CUP 2023   \n",
       "24    9th ODI               ICC MENS WORLD CUP 2023   \n",
       "\n",
       "                                                Place         Date  \\\n",
       "0        Shere Bangla National Stadium, Mirpur, Dhaka   9 JUL 2023   \n",
       "1        Shere Bangla National Stadium, Mirpur, Dhaka  11 JUL 2023   \n",
       "2                              Windsor Park, Dominica  12 JUL 2023   \n",
       "3        Shere Bangla National Stadium, Mirpur, Dhaka  13 JUL 2023   \n",
       "4        Shere Bangla National Stadium, Mirpur, Dhaka  16 JUL 2023   \n",
       "5        Shere Bangla National Stadium, Mirpur, Dhaka  19 JUL 2023   \n",
       "6                         Queen's Park Oval, Trinidad  20 JUL 2023   \n",
       "7        Shere Bangla National Stadium, Mirpur, Dhaka  22 JUL 2023   \n",
       "8                           Kensington Oval, Barbados  27 JUL 2023   \n",
       "9                           Kensington Oval, Barbados  29 JUL 2023   \n",
       "10                       Brian Lara Stadium, Trinidad   1 AUG 2023   \n",
       "11                       Brian Lara Stadium, Trinidad   3 AUG 2023   \n",
       "12                           National Stadium, Guyana   6 AUG 2023   \n",
       "13                           National Stadium, Guyana   8 AUG 2023   \n",
       "14  Central Broward Regional Park Stadium Turf Gro...  12 AUG 2023   \n",
       "15  Central Broward Regional Park Stadium Turf Gro...  13 AUG 2023   \n",
       "16                    MA Chidambaram Stadium, Chennai   8 OCT 2023   \n",
       "17                        Arun Jaitley Stadium, Delhi  11 OCT 2023   \n",
       "18                   Narendra Modi Stadium, Ahmedabad  15 OCT 2023   \n",
       "19      Maharashtra Cricket Association Stadium, Pune  19 OCT 2023   \n",
       "20  Himachal Pradesh Cricket Association Stadium, ...  22 OCT 2023   \n",
       "21  Bharat Ratna Shri Atal Bihari Vajpayee Ekana C...  29 OCT 2023   \n",
       "22                           Wankhede Stadium, Mumbai   2 NOV 2023   \n",
       "23                              Eden Gardens, Kolkata   5 NOV 2023   \n",
       "24                   M Chinnaswamy Stadium, Bengaluru  11 NOV 2023   \n",
       "\n",
       "           Time  \n",
       "0   1:30 PM IST  \n",
       "1   1:30 PM IST  \n",
       "2   7:30 PM IST  \n",
       "3   1:30 PM IST  \n",
       "4   9:00 AM IST  \n",
       "5   9:00 AM IST  \n",
       "6   7:30 PM IST  \n",
       "7   9:00 AM IST  \n",
       "8   7:00 PM IST  \n",
       "9   7:00 PM IST  \n",
       "10  7:00 PM IST  \n",
       "11  8:00 PM IST  \n",
       "12  8:00 PM IST  \n",
       "13  8:00 PM IST  \n",
       "14  8:00 PM IST  \n",
       "15  8:00 PM IST  \n",
       "16  2:00 PM IST  \n",
       "17  2:00 PM IST  \n",
       "18  2:00 PM IST  \n",
       "19  2:00 PM IST  \n",
       "20  2:00 PM IST  \n",
       "21  2:00 PM IST  \n",
       "22  2:00 PM IST  \n",
       "23  2:00 PM IST  \n",
       "24  2:00 PM IST  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating dataframe\n",
    "df = pd.DataFrame({'Match Title':match_title,'Series':series_name,'Place':place,'Date':match_date,'Time':match_time})\n",
    "\n",
    "# Displaying dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cf104d",
   "metadata": {},
   "source": [
    "3. Program to scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "\n",
    "   Url = http://statisticstimes.com/\n",
    "\n",
    "   Details to be scraped are:\n",
    "\n",
    "   A) Rank\n",
    "\n",
    "   B) State\n",
    "\n",
    "   C) GSDP(18-19)- at current prices\n",
    "\n",
    "   D) GSDP(19-20)- at current prices\n",
    "\n",
    "   E) Share(18-19)\n",
    "\n",
    "   F) GDP($ billion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0e84fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sending a request to a website for scraping data\n",
    "page = requests.get(\"https://www.statisticstimes.com/\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb0b86bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating WebDriver instance for controlling the Chrome browser using Selenium\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc00a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening webpage in the browser window controlled by the WebDriver instance\n",
    "driver.get(\"https://www.statisticstimes.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e6c601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on the economy option\n",
    "economy = driver.find_element(By.XPATH,\"/html/body/div[2]/div[1]/div[2]/div[2]/button\")\n",
    "economy.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "894e33c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting and clciking on the India option\n",
    "India = driver.find_element(By.XPATH,\"/html/body/div[2]/div[1]/div[2]/div[2]/div/a[3]\")\n",
    "India.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efb080e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on the GDP of Indian states option\n",
    "gdp_states = driver.find_element(By.XPATH,\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\")\n",
    "gdp_states.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "006dead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists for storing the details of state wise GDB of India\n",
    "state_rank = []\n",
    "state_name = []\n",
    "gsdp_18_19 = []\n",
    "gsdp_19_20 = []\n",
    "share_18_19 = []\n",
    "state_gdp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "619a49f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrolling the scroll bar downwards for scraping the details of state wise GDB of India\n",
    "driver.execute_script(\"window.scrollBy(0,1500)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e54fd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the ranks of states in terms of GDP in state_rank list\n",
    "rank = driver.find_elements(By.XPATH,\"/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[1]\")\n",
    "for i in rank:\n",
    "    state_rank.append(i.text)\n",
    "print(state_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb1db553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Maharashtra', 'Tamil Nadu', 'Uttar Pradesh', 'Gujarat', 'Karnataka', 'West Bengal', 'Rajasthan', 'Andhra Pradesh', 'Telangana', 'Madhya Pradesh', 'Kerala', 'Delhi', 'Haryana', 'Bihar', 'Punjab', 'Odisha', 'Assam', 'Chhattisgarh', 'Jharkhand', 'Uttarakhand', 'Jammu & Kashmir', 'Himachal Pradesh', 'Goa', 'Tripura', 'Chandigarh', 'Puducherry', 'Meghalaya', 'Sikkim', 'Manipur', 'Nagaland', 'Arunachal Pradesh', 'Mizoram', 'Andaman & Nicobar Islands']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the names of states in state_name list\n",
    "state = driver.find_elements(By.XPATH,\"/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[2]\")\n",
    "for i in state:\n",
    "    state_name.append(i.text)\n",
    "print(state_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3d05e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2,632,792', '1,630,208', '1,584,764', '1,502,899', '1,493,127', '1,089,898', '942,586', '862,957', '861,031', '809,592', '781,653', '774,870', '734,163', '530,363', '526,376', '487,805', '315,881', '304,063', '297,204', '245,895', '155,956', '153,845', '73,170', '49,845', '42,114', '34,433', '33,481', '28,723', '27,870', '27,283', '24,603', '22,287', '-']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the data of GSDP (18-19) - at current prices in gsdp_18_19 list\n",
    "gsdp_1819 = driver.find_elements(By.XPATH,\"/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[4]\")\n",
    "for i in gsdp_1819:\n",
    "    gsdp_18_19.append(i.text)\n",
    "print(gsdp_18_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26f67192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '1,845,853', '1,687,818', '-', '1,631,977', '1,253,832', '1,020,989', '972,782', '969,604', '906,672', '-', '856,112', '831,610', '611,804', '574,760', '521,275', '-', '329,180', '328,598', '-', '-', '165,472', '80,449', '55,984', '-', '38,253', '36,572', '32,496', '31,790', '-', '-', '26,503', '-']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the data of GSDP (19-20) - at current prices in gsdp_19_20 list\n",
    "gsdp_1920 = driver.find_elements(By.XPATH,\"/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[3]\")\n",
    "for i in gsdp_1920:\n",
    "    gsdp_19_20.append(i.text)\n",
    "print(gsdp_19_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5159f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['13.94%', '8.63%', '8.39%', '7.96%', '7.91%', '5.77%', '4.99%', '4.57%', '4.56%', '4.29%', '4.14%', '4.10%', '3.89%', '2.81%', '2.79%', '2.58%', '1.67%', '1.61%', '1.57%', '1.30%', '0.83%', '0.81%', '0.39%', '0.26%', '0.22%', '0.18%', '0.18%', '0.15%', '0.15%', '0.14%', '0.13%', '0.12%', '-']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the data of Share(18-19) in share_18_!9 list\n",
    "share = driver.find_elements(By.XPATH,\"/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[5]\")\n",
    "for i in share:\n",
    "    share_18_19.append(i.text)\n",
    "print(share_18_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "752a880d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['399.921', '247.629', '240.726', '228.290', '226.806', '165.556', '143.179', '131.083', '130.791', '122.977', '118.733', '117.703', '111.519', '80.562', '79.957', '74.098', '47.982', '46.187', '45.145', '37.351', '23.690', '23.369', '11.115', '7.571', '6.397', '5.230', '5.086', '4.363', '4.233', '4.144', '3.737', '3.385', '-']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the GDP($ billion) of states in state_gdp list\n",
    "gdp = driver.find_elements(By.XPATH,\"/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[6]\")\n",
    "for i in gdp:\n",
    "    state_gdp.append(i.text)\n",
    "print(state_gdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6808bd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33 33 33 33 33\n"
     ]
    }
   ],
   "source": [
    "# Checking the lengths of all the lists\n",
    "print(len(state_rank),len(state_name),len(gsdp_18_19),len(gsdp_19_20),len(share_18_19),len(state_gdp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7b94a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP(18-19)- at current prices</th>\n",
       "      <th>GSDP(19-20)- at current prices</th>\n",
       "      <th>Share(18-19)</th>\n",
       "      <th>GDP($ billion)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>-</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>-</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>942,586</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>862,957</td>\n",
       "      <td>972,782</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Telangana</td>\n",
       "      <td>861,031</td>\n",
       "      <td>969,604</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>809,592</td>\n",
       "      <td>906,672</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Kerala</td>\n",
       "      <td>781,653</td>\n",
       "      <td>-</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>774,870</td>\n",
       "      <td>856,112</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Haryana</td>\n",
       "      <td>734,163</td>\n",
       "      <td>831,610</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bihar</td>\n",
       "      <td>530,363</td>\n",
       "      <td>611,804</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Punjab</td>\n",
       "      <td>526,376</td>\n",
       "      <td>574,760</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Odisha</td>\n",
       "      <td>487,805</td>\n",
       "      <td>521,275</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Assam</td>\n",
       "      <td>315,881</td>\n",
       "      <td>-</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>304,063</td>\n",
       "      <td>329,180</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>297,204</td>\n",
       "      <td>328,598</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>245,895</td>\n",
       "      <td>-</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>155,956</td>\n",
       "      <td>-</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>153,845</td>\n",
       "      <td>165,472</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Goa</td>\n",
       "      <td>73,170</td>\n",
       "      <td>80,449</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Tripura</td>\n",
       "      <td>49,845</td>\n",
       "      <td>55,984</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>42,114</td>\n",
       "      <td>-</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Puducherry</td>\n",
       "      <td>34,433</td>\n",
       "      <td>38,253</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>33,481</td>\n",
       "      <td>36,572</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sikkim</td>\n",
       "      <td>28,723</td>\n",
       "      <td>32,496</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Manipur</td>\n",
       "      <td>27,870</td>\n",
       "      <td>31,790</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Nagaland</td>\n",
       "      <td>27,283</td>\n",
       "      <td>-</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>24,603</td>\n",
       "      <td>-</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Mizoram</td>\n",
       "      <td>22,287</td>\n",
       "      <td>26,503</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          State GSDP(18-19)- at current prices  \\\n",
       "Rank                                                             \n",
       "1                   Maharashtra                      2,632,792   \n",
       "2                    Tamil Nadu                      1,630,208   \n",
       "3                 Uttar Pradesh                      1,584,764   \n",
       "4                       Gujarat                      1,502,899   \n",
       "5                     Karnataka                      1,493,127   \n",
       "6                   West Bengal                      1,089,898   \n",
       "7                     Rajasthan                        942,586   \n",
       "8                Andhra Pradesh                        862,957   \n",
       "9                     Telangana                        861,031   \n",
       "10               Madhya Pradesh                        809,592   \n",
       "11                       Kerala                        781,653   \n",
       "12                        Delhi                        774,870   \n",
       "13                      Haryana                        734,163   \n",
       "14                        Bihar                        530,363   \n",
       "15                       Punjab                        526,376   \n",
       "16                       Odisha                        487,805   \n",
       "17                        Assam                        315,881   \n",
       "18                 Chhattisgarh                        304,063   \n",
       "19                    Jharkhand                        297,204   \n",
       "20                  Uttarakhand                        245,895   \n",
       "21              Jammu & Kashmir                        155,956   \n",
       "22             Himachal Pradesh                        153,845   \n",
       "23                          Goa                         73,170   \n",
       "24                      Tripura                         49,845   \n",
       "25                   Chandigarh                         42,114   \n",
       "26                   Puducherry                         34,433   \n",
       "27                    Meghalaya                         33,481   \n",
       "28                       Sikkim                         28,723   \n",
       "29                      Manipur                         27,870   \n",
       "30                     Nagaland                         27,283   \n",
       "31            Arunachal Pradesh                         24,603   \n",
       "32                      Mizoram                         22,287   \n",
       "33    Andaman & Nicobar Islands                              -   \n",
       "\n",
       "     GSDP(19-20)- at current prices Share(18-19) GDP($ billion)  \n",
       "Rank                                                             \n",
       "1                                 -       13.94%        399.921  \n",
       "2                         1,845,853        8.63%        247.629  \n",
       "3                         1,687,818        8.39%        240.726  \n",
       "4                                 -        7.96%        228.290  \n",
       "5                         1,631,977        7.91%        226.806  \n",
       "6                         1,253,832        5.77%        165.556  \n",
       "7                         1,020,989        4.99%        143.179  \n",
       "8                           972,782        4.57%        131.083  \n",
       "9                           969,604        4.56%        130.791  \n",
       "10                          906,672        4.29%        122.977  \n",
       "11                                -        4.14%        118.733  \n",
       "12                          856,112        4.10%        117.703  \n",
       "13                          831,610        3.89%        111.519  \n",
       "14                          611,804        2.81%         80.562  \n",
       "15                          574,760        2.79%         79.957  \n",
       "16                          521,275        2.58%         74.098  \n",
       "17                                -        1.67%         47.982  \n",
       "18                          329,180        1.61%         46.187  \n",
       "19                          328,598        1.57%         45.145  \n",
       "20                                -        1.30%         37.351  \n",
       "21                                -        0.83%         23.690  \n",
       "22                          165,472        0.81%         23.369  \n",
       "23                           80,449        0.39%         11.115  \n",
       "24                           55,984        0.26%          7.571  \n",
       "25                                -        0.22%          6.397  \n",
       "26                           38,253        0.18%          5.230  \n",
       "27                           36,572        0.18%          5.086  \n",
       "28                           32,496        0.15%          4.363  \n",
       "29                           31,790        0.15%          4.233  \n",
       "30                                -        0.14%          4.144  \n",
       "31                                -        0.13%          3.737  \n",
       "32                           26,503        0.12%          3.385  \n",
       "33                                -            -              -  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_gdp = {\n",
    "    'State':state_name,\n",
    "    'GSDP(18-19)- at current prices':gsdp_18_19,\n",
    "    'GSDP(19-20)- at current prices':gsdp_19_20,\n",
    "    'Share(18-19)':share_18_19,\n",
    "    'GDP($ billion)':state_gdp\n",
    "}\n",
    "\n",
    "# Creating dataframe\n",
    "df = pd.DataFrame(states_gdp,index=state_rank)\n",
    "\n",
    "# Displaying dataframe\n",
    "df.index.name = 'Rank'\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec0f620",
   "metadata": {},
   "source": [
    "4. Program to scrape the details of trending repositories on Github.com.\n",
    "\n",
    "    Url = https://github.com/\n",
    "\n",
    "    Details to be scraped are:\n",
    "\n",
    "    A) Repository title\n",
    "    \n",
    "    B) Repository description\n",
    "\n",
    "    C) Contributors count\n",
    "\n",
    "    D) Language used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d62a687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sending a request to a website for scraping data\n",
    "page = requests.get(\"https://github.com/\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d7f6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating WebDriver instance for controlling the Chrome browser using Selenium\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de334d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening webpage in the browser window controlled by the WebDriver instance\n",
    "driver.get(\"https://github.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3d3018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on the open source option for selecting trending option\n",
    "open_source = driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/button\")\n",
    "open_source.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a520d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on the trending option\n",
    "trending = driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a\")\n",
    "trending.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e5aa15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for storing the details of the repositories\n",
    "repository_title = []\n",
    "repository_description = []\n",
    "contributors_count = []\n",
    "programming_language = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9cab3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping and storing the repositories title in repository_title list\n",
    "title = driver.find_elements(By.XPATH,\"//h2[@class='h3 lh-condensed']\")\n",
    "for i in title:\n",
    "    repository_title.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30569025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping and storing the urls of repositories\n",
    "urls = []\n",
    "u = driver.find_elements(By.XPATH,\"//h2[@class='h3 lh-condensed']//a\")\n",
    "for i in u:\n",
    "    urls.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cab22684",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "program_lang = []\n",
    "\n",
    "# Scraping the details of repositories using urls\n",
    "for url in urls:\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scraping and storing the repositories description in repository_description list\n",
    "    try:\n",
    "        description = driver.find_element(By.XPATH,\"//p[@class='f4 my-3']\")\n",
    "        repository_description.append(description.text)\n",
    "    except NoSuchElementException:\n",
    "        repository_description.append('-')    \n",
    "\n",
    "    # Scraping and storing the contributors count in contributors_count list\n",
    "    try:\n",
    "        contributors = driver.find_element(By.XPATH,\"//a[contains(text(),'Contributors')]//span\")\n",
    "        contributors_count.append(contributors.text)\n",
    "    except NoSuchElementException:\n",
    "        contributors_count.append('1')\n",
    "    \n",
    "    # Scraping and storing the programming languages used in language_used list\n",
    "    try:\n",
    "        lang1 = []\n",
    "        lang2 = driver.find_elements(By.XPATH,\"//span[@class='color-fg-default text-bold mr-1']\")\n",
    "        for i in lang2:\n",
    "            lang1.append(i.text)\n",
    "        program_lang.append(', '.join(lang1))\n",
    "    except NoSuchElementException:\n",
    "        program_lang.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "edc45846",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ChaoningZhang / MobileSAM', 'abacaj / mpt-30B-inference', 'slarkvan / Block-Pornographic-Replies', 'WeMakeDevs / open-source-course', 'PowerShell / PowerShell', 'XingangPan / DragGAN', 'facebook / folly', 'ParthJadhav / Tkinter-Designer', 'papers-we-love / papers-we-love', 'wgwang / LLMs-In-China', 'practical-tutorials / project-based-learning', 'mengjian-github / copilot-analysis', 'dotnet-architecture / eShopOnContainers', 'EbookFoundation / free-programming-books', 'chinese-poetry / chinese-poetry', 'alexbei / telegram-groups', 'questdb / questdb', 'fuqiuluo / unidbg-fetch-qsign', 'mosaicml / composer', 'alibaba / DataX', 'toeverything / AFFiNE', 'phodal / aigc', 'imgly / background-removal-js', 'sourcegraph / sourcegraph', 'buqiyuan / vue3-antd-admin']\n"
     ]
    }
   ],
   "source": [
    "print(repository_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a2b052b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the official code for Faster Segment Anything (MobileSAM) project that makes SAM lightweight for mobile applications and beyond!', 'Run inference on MPT-30B using CPU', '屏蔽推特回复下的黄推。Block pornographic replies below the tweet.', '-', 'PowerShell for every system!', 'Official Code for DragGAN (SIGGRAPH 2023)', 'An open-source C++ library developed and used at Facebook.', 'An easy and fast way to create a Python GUI 🐍', 'Papers from the computer science community to read and discuss.', '中国大模型', 'Curated list of project-based tutorials', '-', 'Cross-platform .NET sample microservices and container based application that runs on Linux Windows and macOS. Powered by .NET 7, Docker Containers and Azure Kubernetes Services. Supports Visual Studio, VS for Mac and CLI based environments with Docker CLI, dotnet CLI, VS Code or any other code editor.', '📚 Freely available programming books', 'The most comprehensive database of Chinese poetry 🧶最全中华古诗词数据库, 唐宋两朝近一万四千古诗人, 接近5.5万首唐诗加26万宋诗. 两宋时期1564位词人，21050首词。', '经过精心筛选，从5000+个电报群组/频道/机器人中挑选出的优质推荐！如果您有更多值得推荐的电报群组/频道/机器人，欢迎在issue中留言或提交pull requests。感谢您的关注！', 'An open source time-series database for fast ingest and SQL queries', '获取QQSign通过Unidbg', 'Train neural networks up to 7x faster', 'DataX是阿里云DataWorks数据集成的开源版本。', 'There can be more than Notion and Miro. AFFiNE is a next-gen knowledge base that brings planning, sorting and creating all together. Privacy first, open-source, customizable and ready to use.', '《构筑大语言模型应用：应用开发与架构设计》一本关于 LLM 在真实世界应用的开源电子书，介绍了大语言模型的基础知识和应用，以及如何构建自己的模型。其中包括Prompt的编写、开发和管理，探索最好的大语言模型能带来什么，以及LLM应用开发的模式和架构设计。', 'Remove backgrounds from images directly in the browser environment with ease and no additional costs or privacy concerns. Explore an interactive demo.', 'Code Intelligence Platform', '基于vue-cli5.x/vite2.x + vue3.x + ant-design-vue3.x + typescript hooks 的基础后台管理系统模板 RBAC的权限系统, JSON Schema动态表单,动态表格,漂亮锁屏界面']\n"
     ]
    }
   ],
   "source": [
    "print(repository_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "990849d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '1', '2', '1', '435', '14', '727', '36', '247', '1', '104', '2', '167', '2,517', '57', '1', '114', '7', '77', '76', '87', '5', '2', '347', '7']\n"
     ]
    }
   ],
   "source": [
    "print(contributors_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aebeec8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jupyter Notebook, Other', 'Python', 'JavaScript, CSS, HTML', '-', 'C#, PowerShell, Roff, Shell, Rich Text Format, HTML', 'Python, Cuda, C++', 'C++, Python, CMake, C, Cython, Assembly, Other', 'Python, Makefile', 'Shell', '-', '-', 'JavaScript', 'C#, HTML, Dockerfile, TypeScript, CSS, SCSS, Other', '-', 'JavaScript, Python', 'Python, Batchfile', 'Java, C++, C, Assembly, CMake, Shell', 'Kotlin, Java', 'Python, Other', 'Java, Other', 'TypeScript, JavaScript, Rust, CSS, Shell, HTML', 'Rust', 'TypeScript, JavaScript', 'Go, TypeScript, SCSS, PLpgSQL, Java, Starlark, Other', 'Vue, TypeScript, JavaScript, Less, HTML, Dockerfile, Shell']\n"
     ]
    }
   ],
   "source": [
    "for i in program_lang:\n",
    "    if i == '':\n",
    "        programming_language.append('-')\n",
    "    else:\n",
    "        programming_language.append(i)\n",
    "\n",
    "print(programming_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db8a9385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25 25 25\n"
     ]
    }
   ],
   "source": [
    "# Checking lengths of all the lists\n",
    "print(len(repository_title),len(repository_description),len(contributors_count),len(programming_language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8814bc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository Title</th>\n",
       "      <th>Repository Description</th>\n",
       "      <th>Contributors Count</th>\n",
       "      <th>Programming Languages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ChaoningZhang / MobileSAM</td>\n",
       "      <td>This is the official code for Faster Segment A...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jupyter Notebook, Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abacaj / mpt-30B-inference</td>\n",
       "      <td>Run inference on MPT-30B using CPU</td>\n",
       "      <td>1</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slarkvan / Block-Pornographic-Replies</td>\n",
       "      <td>屏蔽推特回复下的黄推。Block pornographic replies below th...</td>\n",
       "      <td>2</td>\n",
       "      <td>JavaScript, CSS, HTML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WeMakeDevs / open-source-course</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PowerShell / PowerShell</td>\n",
       "      <td>PowerShell for every system!</td>\n",
       "      <td>435</td>\n",
       "      <td>C#, PowerShell, Roff, Shell, Rich Text Format,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XingangPan / DragGAN</td>\n",
       "      <td>Official Code for DragGAN (SIGGRAPH 2023)</td>\n",
       "      <td>14</td>\n",
       "      <td>Python, Cuda, C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>facebook / folly</td>\n",
       "      <td>An open-source C++ library developed and used ...</td>\n",
       "      <td>727</td>\n",
       "      <td>C++, Python, CMake, C, Cython, Assembly, Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ParthJadhav / Tkinter-Designer</td>\n",
       "      <td>An easy and fast way to create a Python GUI 🐍</td>\n",
       "      <td>36</td>\n",
       "      <td>Python, Makefile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>papers-we-love / papers-we-love</td>\n",
       "      <td>Papers from the computer science community to ...</td>\n",
       "      <td>247</td>\n",
       "      <td>Shell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wgwang / LLMs-In-China</td>\n",
       "      <td>中国大模型</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>practical-tutorials / project-based-learning</td>\n",
       "      <td>Curated list of project-based tutorials</td>\n",
       "      <td>104</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mengjian-github / copilot-analysis</td>\n",
       "      <td>-</td>\n",
       "      <td>2</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dotnet-architecture / eShopOnContainers</td>\n",
       "      <td>Cross-platform .NET sample microservices and c...</td>\n",
       "      <td>167</td>\n",
       "      <td>C#, HTML, Dockerfile, TypeScript, CSS, SCSS, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>EbookFoundation / free-programming-books</td>\n",
       "      <td>📚 Freely available programming books</td>\n",
       "      <td>2,517</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>chinese-poetry / chinese-poetry</td>\n",
       "      <td>The most comprehensive database of Chinese poe...</td>\n",
       "      <td>57</td>\n",
       "      <td>JavaScript, Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>alexbei / telegram-groups</td>\n",
       "      <td>经过精心筛选，从5000+个电报群组/频道/机器人中挑选出的优质推荐！如果您有更多值得推荐的...</td>\n",
       "      <td>1</td>\n",
       "      <td>Python, Batchfile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>questdb / questdb</td>\n",
       "      <td>An open source time-series database for fast i...</td>\n",
       "      <td>114</td>\n",
       "      <td>Java, C++, C, Assembly, CMake, Shell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fuqiuluo / unidbg-fetch-qsign</td>\n",
       "      <td>获取QQSign通过Unidbg</td>\n",
       "      <td>7</td>\n",
       "      <td>Kotlin, Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mosaicml / composer</td>\n",
       "      <td>Train neural networks up to 7x faster</td>\n",
       "      <td>77</td>\n",
       "      <td>Python, Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>alibaba / DataX</td>\n",
       "      <td>DataX是阿里云DataWorks数据集成的开源版本。</td>\n",
       "      <td>76</td>\n",
       "      <td>Java, Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>toeverything / AFFiNE</td>\n",
       "      <td>There can be more than Notion and Miro. AFFiNE...</td>\n",
       "      <td>87</td>\n",
       "      <td>TypeScript, JavaScript, Rust, CSS, Shell, HTML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>phodal / aigc</td>\n",
       "      <td>《构筑大语言模型应用：应用开发与架构设计》一本关于 LLM 在真实世界应用的开源电子书，介绍...</td>\n",
       "      <td>5</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>imgly / background-removal-js</td>\n",
       "      <td>Remove backgrounds from images directly in the...</td>\n",
       "      <td>2</td>\n",
       "      <td>TypeScript, JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sourcegraph / sourcegraph</td>\n",
       "      <td>Code Intelligence Platform</td>\n",
       "      <td>347</td>\n",
       "      <td>Go, TypeScript, SCSS, PLpgSQL, Java, Starlark,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>buqiyuan / vue3-antd-admin</td>\n",
       "      <td>基于vue-cli5.x/vite2.x + vue3.x + ant-design-vue...</td>\n",
       "      <td>7</td>\n",
       "      <td>Vue, TypeScript, JavaScript, Less, HTML, Docke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Repository Title  \\\n",
       "0                      ChaoningZhang / MobileSAM   \n",
       "1                     abacaj / mpt-30B-inference   \n",
       "2          slarkvan / Block-Pornographic-Replies   \n",
       "3                WeMakeDevs / open-source-course   \n",
       "4                        PowerShell / PowerShell   \n",
       "5                           XingangPan / DragGAN   \n",
       "6                               facebook / folly   \n",
       "7                 ParthJadhav / Tkinter-Designer   \n",
       "8                papers-we-love / papers-we-love   \n",
       "9                         wgwang / LLMs-In-China   \n",
       "10  practical-tutorials / project-based-learning   \n",
       "11            mengjian-github / copilot-analysis   \n",
       "12       dotnet-architecture / eShopOnContainers   \n",
       "13      EbookFoundation / free-programming-books   \n",
       "14               chinese-poetry / chinese-poetry   \n",
       "15                     alexbei / telegram-groups   \n",
       "16                             questdb / questdb   \n",
       "17                 fuqiuluo / unidbg-fetch-qsign   \n",
       "18                           mosaicml / composer   \n",
       "19                               alibaba / DataX   \n",
       "20                         toeverything / AFFiNE   \n",
       "21                                 phodal / aigc   \n",
       "22                 imgly / background-removal-js   \n",
       "23                     sourcegraph / sourcegraph   \n",
       "24                    buqiyuan / vue3-antd-admin   \n",
       "\n",
       "                               Repository Description Contributors Count  \\\n",
       "0   This is the official code for Faster Segment A...                  4   \n",
       "1                  Run inference on MPT-30B using CPU                  1   \n",
       "2   屏蔽推特回复下的黄推。Block pornographic replies below th...                  2   \n",
       "3                                                   -                  1   \n",
       "4                        PowerShell for every system!                435   \n",
       "5           Official Code for DragGAN (SIGGRAPH 2023)                 14   \n",
       "6   An open-source C++ library developed and used ...                727   \n",
       "7       An easy and fast way to create a Python GUI 🐍                 36   \n",
       "8   Papers from the computer science community to ...                247   \n",
       "9                                               中国大模型                  1   \n",
       "10            Curated list of project-based tutorials                104   \n",
       "11                                                  -                  2   \n",
       "12  Cross-platform .NET sample microservices and c...                167   \n",
       "13               📚 Freely available programming books              2,517   \n",
       "14  The most comprehensive database of Chinese poe...                 57   \n",
       "15  经过精心筛选，从5000+个电报群组/频道/机器人中挑选出的优质推荐！如果您有更多值得推荐的...                  1   \n",
       "16  An open source time-series database for fast i...                114   \n",
       "17                                   获取QQSign通过Unidbg                  7   \n",
       "18              Train neural networks up to 7x faster                 77   \n",
       "19                       DataX是阿里云DataWorks数据集成的开源版本。                 76   \n",
       "20  There can be more than Notion and Miro. AFFiNE...                 87   \n",
       "21  《构筑大语言模型应用：应用开发与架构设计》一本关于 LLM 在真实世界应用的开源电子书，介绍...                  5   \n",
       "22  Remove backgrounds from images directly in the...                  2   \n",
       "23                         Code Intelligence Platform                347   \n",
       "24  基于vue-cli5.x/vite2.x + vue3.x + ant-design-vue...                  7   \n",
       "\n",
       "                                Programming Languages  \n",
       "0                             Jupyter Notebook, Other  \n",
       "1                                              Python  \n",
       "2                               JavaScript, CSS, HTML  \n",
       "3                                                   -  \n",
       "4   C#, PowerShell, Roff, Shell, Rich Text Format,...  \n",
       "5                                   Python, Cuda, C++  \n",
       "6      C++, Python, CMake, C, Cython, Assembly, Other  \n",
       "7                                    Python, Makefile  \n",
       "8                                               Shell  \n",
       "9                                                   -  \n",
       "10                                                  -  \n",
       "11                                         JavaScript  \n",
       "12  C#, HTML, Dockerfile, TypeScript, CSS, SCSS, O...  \n",
       "13                                                  -  \n",
       "14                                 JavaScript, Python  \n",
       "15                                  Python, Batchfile  \n",
       "16               Java, C++, C, Assembly, CMake, Shell  \n",
       "17                                       Kotlin, Java  \n",
       "18                                      Python, Other  \n",
       "19                                        Java, Other  \n",
       "20     TypeScript, JavaScript, Rust, CSS, Shell, HTML  \n",
       "21                                               Rust  \n",
       "22                             TypeScript, JavaScript  \n",
       "23  Go, TypeScript, SCSS, PLpgSQL, Java, Starlark,...  \n",
       "24  Vue, TypeScript, JavaScript, Less, HTML, Docke...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trending_repositories = {\n",
    "    'Repository Title':repository_title,\n",
    "    'Repository Description':repository_description,\n",
    "    'Contributors Count':contributors_count,\n",
    "    'Programming Languages':programming_language\n",
    "}\n",
    "\n",
    "# Creating dataframe\n",
    "df = pd.DataFrame(trending_repositories)\n",
    "\n",
    "# Displaying dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1582e3a",
   "metadata": {},
   "source": [
    "5. Program to scrape the details of top 100 songs on billiboard.com.\n",
    "\n",
    "   Url = https:/www.billboard.com/\n",
    "\n",
    "   Details to be scraped are:\n",
    "\n",
    "   A) Song name\n",
    "\n",
    "   B) Artist name\n",
    "\n",
    "   C) Last week rank\n",
    "\n",
    "   D) Peak rank\n",
    "\n",
    "   E) Weeks on board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee6bc461",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sending a request to a website for scraping data\n",
    "page = requests.get(\"https://www.billboard.com/\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "64f15698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating WebDriver instance for controlling the Chrome browser using Selenium\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ce9c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening webpage in the browser window controlled by the WebDriver instance\n",
    "driver.get(\"https://www.billboard.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "acdd8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cliciking on the charts option for selecting Billboard hot 100 option\n",
    "charts = driver.find_element(By.XPATH,\"/html/body/div[3]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a\")\n",
    "charts.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "82fa017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrolling scroll bar downwards for selecting view chart option\n",
    "driver.execute_script(\"window.scrollBy(0,800)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c31f63d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on the Billboard hot 100's View chart option\n",
    "billboard_hot_100 = driver.find_element(By.XPATH,\"/html/body/div[3]/main/div[2]/div[1]/div[1]/div/div/div[3]/a\")\n",
    "billboard_hot_100.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ff08433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for storing the details of songs\n",
    "song_name = []\n",
    "artist_name = []\n",
    "last_week_rank = []\n",
    "peak_rank = []\n",
    "weeks_on_board = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6566699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping and storing the songs details in the song_details list\n",
    "song_details = []\n",
    "songs = driver.find_elements(By.XPATH,\"//li[@class='lrv-u-width-100p']\")\n",
    "for i in songs:\n",
    "    song_details.append(i.text.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b7f79ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,100):\n",
    "    \n",
    "    # Extracting the songs name from song_details list and storing them in song_name list\n",
    "    song_name.append(song_details[i][0])\n",
    "    \n",
    "    # Extracting the artists name from song_details list and storing them in artist_name list\n",
    "    artist_name.append(song_details[i][1])\n",
    "    \n",
    "    # Extracting the songs last week rank from song_details list and storing them in last_week_rank list\n",
    "    last_week_rank.append(song_details[i][2])\n",
    "    \n",
    "    # Extracting the songs peak rank from song_details list and storing them in peak_rank list\n",
    "    peak_rank.append(song_details[i][3])\n",
    "    \n",
    "    # Extracting the songs weeks on board from song_details list and storing them in weeks_on_board list\n",
    "    weeks_on_board.append(song_details[i][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0319ad6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Last Night', 'Fast Car', 'Calm Down', 'Flowers', 'All My Life', 'Favorite Song', 'Karma', 'Kill Bill', \"Creepin'\", 'Ella Baila Sola', 'Sure Thing', 'Anti-Hero', 'Snooze', 'Something In The Orange', 'Die For You', 'Fukumean', 'Need A Favor', 'Cruel Summer', 'La Bebe', 'You Proof', 'Un x100to', \"Thinkin' Bout Me\", 'Rock And A Hard Place', 'Cupid', 'Search & Rescue', 'Chemical', 'Eyes Closed', 'Next Thing You Know', 'Back To The Moon', 'Where She Goes', 'Attention', \"I'm Good (Blue)\", 'Thought You Should Know', 'Dance The Night', \"Dancin' In The Country\", 'Religiously', \"Boy's A Liar, Pt. 2\", 'Memory Lane', 'Put It On Da Floor Again', 'Area Codes', 'Stand By Me', 'Rodeo Dr', 'Tennessee Orange', 'Bzrp Music Sessions, Vol. 55', 'Love You Anyway', 'One Thing At A Time', 'TQM', 'Players', 'Under The Influence', 'Thank God', 'Back At It', 'Bread & Butter', 'Princess Diana', 'Bye', 'Calling', 'Daylight', 'What It Is (Block Boy)', 'Annihilate', 'Bury Me In Georgia', 'Ca$h $hit', 'Self Love', 'Bottom', 'It Matters To Her', 'Dial Drunk', 'PRC', 'Por Las Noches', 'Mourning', 'Am I Dreaming', 'Waffle House', 'See You Again', 'TQG', 'Your Heart Or Mine', 'Peaches & Eggplants', 'El Azul', 'You, Me, & Whiskey', 'IDK NoMore', 'P Angels', 'Cowgirls', \"Baby Don't Hurt Me\", 'Popular', 'Fight The Feeling', 'Slut Me Out', 'Paybach', 'Plebada', 'Save Me', 'Fragil', 'Jaded', 'ICU', 'Truck Bed', \"Ain't That Some\", 'Shake Sumn', 'Trustfall', 'People', 'Go Crazy', 'Chanel', 'Angel, Pt. 1', 'Girl In Mine', 'Moonlight', 'Classy 101', 'Bluffin']\n"
     ]
    }
   ],
   "source": [
    "print(song_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f763d46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Morgan Wallen', 'Luke Combs', 'Rema & Selena Gomez', 'Miley Cyrus', 'Lil Durk Featuring J. Cole', 'Toosii', 'Taylor Swift Featuring Ice Spice', 'SZA', 'Metro Boomin, The Weeknd & 21 Savage', 'Eslabon Armado X Peso Pluma', 'Miguel', 'Taylor Swift', 'SZA', 'Zach Bryan', 'The Weeknd & Ariana Grande', 'Gunna', 'Jelly Roll', 'Taylor Swift', 'Yng Lvcas x Peso Pluma', 'Morgan Wallen', 'Grupo Frontera X Bad Bunny', 'Morgan Wallen', 'Bailey Zimmerman', 'Fifty Fifty', 'Drake', 'Post Malone', 'Ed Sheeran', 'Jordan Davis', 'Gunna', 'Bad Bunny', 'Doja Cat', 'David Guetta & Bebe Rexha', 'Morgan Wallen', 'Dua Lipa', 'Tyler Hubbard', 'Bailey Zimmerman', 'PinkPantheress & Ice Spice', 'Old Dominion', 'Latto Featuring Cardi B', 'Kali', 'Lil Durk Featuring Morgan Wallen', 'Gunna', 'Megan Moroney', 'Bizarrap & Peso Pluma', 'Luke Combs', 'Morgan Wallen', 'Fuerza Regida', 'Coi Leray', 'Chris Brown', 'Kane Brown With Katelyn Brown', 'Gunna', 'Gunna', 'Ice Spice & Nicki Minaj', 'Peso Pluma', 'Metro Boomin, Swae Lee & NAV Featuring A Boogie Wit da Hoodie', 'David Kushner', 'Doechii Featuring Kodak Black', 'Metro Boomin, Swae Lee, Lil Wayne & Offset', 'Kane Brown', 'Gunna', 'Metro Boomin & Coi Leray', 'Gunna', 'Scotty McCreery', 'Noah Kahan', 'Peso Pluma X Natanael Cano', 'Peso Pluma', 'Post Malone', 'Metro Boomin, A$AP Rocky & Roisee', 'Jonas Brothers', 'Tyler, The Creator Featuring Kali Uchis', 'Karol G x Shakira', 'Jon Pardi', 'Young Nudy Featuring 21 Savage', 'Junior H x Peso Pluma', 'Justin Moore & Priscilla Block', 'Gunna', 'Gunna', 'Morgan Wallen Featuring ERNEST', 'David Guetta, Anne-Marie & Coi Leray', 'The Weeknd, Playboi Carti & Madonna', 'Rod Wave', 'NLE Choppa', 'Gunna', 'El Alfa x Peso Pluma', 'Jelly Roll With Lainey Wilson', 'Yahritza y Su Esencia x Grupo Frontera', 'Miley Cyrus', 'Coco Jones', 'HARDY', 'Morgan Wallen', 'DaBaby', 'P!nk', 'Libianca', 'Gunna', 'Becky G & Peso Pluma', 'Kodak Black, NLE Choppa, Jimin, JVKE & Muni Long', 'Parmalee', 'Kali Uchis', 'Feid x Young Miko', 'Gucci Mane & Lil Baby']\n"
     ]
    }
   ],
   "source": [
    "print(artist_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4349d4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '3', '4', '2', '5', '6', '9', '7', '8', '10', '11', '12', '15', '14', '13', '-', '19', '39', '16', '21', '18', '22', '25', '24', '20', '23', '26', '28', '-', '17', '-', '31', '30', '32', '34', '42', '27', '36', '29', '33', '41', '-', '38', '37', '49', '35', '40', '45', '44', '50', '-', '81', '52', '53', '46', '56', '60', '47', '64', '-', '54', '-', '55', '43', '57', '58', '59', '51', '65', '61', '62', '67', '74', '63', '66', '-', '-', '69', '71', '72', '76', '73', '-', '68', '96', '83', '85', '70', '89', '77', '78', '82', '80', '-', '79', '-', '-', '90', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "print(last_week_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b3680576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '1', '2', '5', '2', '1', '3', '4', '11', '1', '13', '10', '1', '16', '17', '18', '11', '5', '5', '9', '10', '17', '2', '13', '19', '28', '29', '8', '31', '4', '7', '32', '23', '36', '3', '36', '13', '33', '22', '42', '30', '31', '15', '10', '34', '9', '12', '13', '51', '48', '4', '53', '41', '47', '57', '44', '59', '60', '54', '62', '49', '43', '33', '28', '36', '51', '57', '44', '7', '67', '73', '55', '66', '76', '77', '40', '71', '43', '16', '28', '83', '68', '85', '69', '56', '63', '89', '11', '69', '82', '80', '94', '55', '65', '97', '80', '99', '100']\n"
     ]
    }
   ],
   "source": [
    "print(peak_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cabd487c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['21', '13', '42', '23', '6', '18', '15', '28', '29', '14', '47', '35', '28', '61', '47', '1', '12', '7', '14', '58', '10', '16', '54', '14', '11', '10', '13', '22', '1', '5', '1', '43', '45', '4', '18', '7', '20', '12', '3', '7', '4', '1', '28', '3', '19', '29', '5', '25', '41', '41', '1', '3', '10', '4', '3', '10', '7', '3', '6', '1', '3', '1', '10', '2', '19', '15', '5', '3', '8', '10', '17', '6', '3', '11', '7', '1', '1', '16', '5', '3', '12', '14', '1', '2', '3', '9', '6', '14', '2', '16', '7', '4', '8', '1', '10', '2', '1', '11', '1', '1']\n"
     ]
    }
   ],
   "source": [
    "print(weeks_on_board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "52a4fa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "# Checking the lengths of all the lists\n",
    "print(len(song_name),len(artist_name),len(last_week_rank),len(peak_rank),len(weeks_on_board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "167217fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song Name</th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Last Week Rank</th>\n",
       "      <th>Peak Rank</th>\n",
       "      <th>Weeks on Board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Last Night</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fast Car</td>\n",
       "      <td>Luke Combs</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Calm Down</td>\n",
       "      <td>Rema &amp; Selena Gomez</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flowers</td>\n",
       "      <td>Miley Cyrus</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All My Life</td>\n",
       "      <td>Lil Durk Featuring J. Cole</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Angel, Pt. 1</td>\n",
       "      <td>Kodak Black, NLE Choppa, Jimin, JVKE &amp; Muni Long</td>\n",
       "      <td>-</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Girl In Mine</td>\n",
       "      <td>Parmalee</td>\n",
       "      <td>-</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Moonlight</td>\n",
       "      <td>Kali Uchis</td>\n",
       "      <td>90</td>\n",
       "      <td>80</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Classy 101</td>\n",
       "      <td>Feid x Young Miko</td>\n",
       "      <td>-</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Bluffin</td>\n",
       "      <td>Gucci Mane &amp; Lil Baby</td>\n",
       "      <td>-</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Song Name                                       Artist Name  \\\n",
       "1      Last Night                                     Morgan Wallen   \n",
       "2        Fast Car                                        Luke Combs   \n",
       "3       Calm Down                               Rema & Selena Gomez   \n",
       "4         Flowers                                       Miley Cyrus   \n",
       "5     All My Life                        Lil Durk Featuring J. Cole   \n",
       "..            ...                                               ...   \n",
       "96   Angel, Pt. 1  Kodak Black, NLE Choppa, Jimin, JVKE & Muni Long   \n",
       "97   Girl In Mine                                          Parmalee   \n",
       "98      Moonlight                                        Kali Uchis   \n",
       "99     Classy 101                                 Feid x Young Miko   \n",
       "100       Bluffin                             Gucci Mane & Lil Baby   \n",
       "\n",
       "    Last Week Rank Peak Rank Weeks on Board  \n",
       "1                1         1             21  \n",
       "2                3         2             13  \n",
       "3                4         3             42  \n",
       "4                2         1             23  \n",
       "5                5         2              6  \n",
       "..             ...       ...            ...  \n",
       "96               -        65              2  \n",
       "97               -        97              1  \n",
       "98              90        80             11  \n",
       "99               -        99              1  \n",
       "100              -       100              1  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_details = {\n",
    "    'Song Name':song_name,\n",
    "    'Artist Name':artist_name,\n",
    "    'Last Week Rank':last_week_rank,\n",
    "    'Peak Rank':peak_rank,\n",
    "    'Weeks on Board':weeks_on_board\n",
    "}\n",
    "\n",
    "# Creating dataframe\n",
    "df = pd.DataFrame(songs_details,index=range(1,101))\n",
    "\n",
    "# Displaying dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35284404",
   "metadata": {},
   "source": [
    "6. Program to scrape the details of Highest selling novels.\n",
    "\n",
    "   Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\n",
    "\n",
    "   Details to be scraped are:\n",
    "\n",
    "   A) Book name\n",
    "\n",
    "   B) Author name\n",
    "\n",
    "   C) Volumes sold\n",
    "\n",
    "   D) Publisher\n",
    "\n",
    "   E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2bb345bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sending a request to a website for scraping data\n",
    "page = requests.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "feaefa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating WebDriver instance for controlling the Chrome browser using Selenium\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1390b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening webpage in the browser window controlled by the WebDriver instance\n",
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a122490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for storing the details of the highest selling novels\n",
    "book_name = []\n",
    "author_name = []\n",
    "volumes_sold =[]\n",
    "publisher_name = []\n",
    "genre = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "355f42fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Da Vinci Code,The', 'Harry Potter and the Deathly Hallows', \"Harry Potter and the Philosopher's Stone\", 'Harry Potter and the Order of the Phoenix', 'Fifty Shades of Grey', 'Harry Potter and the Goblet of Fire', 'Harry Potter and the Chamber of Secrets', 'Harry Potter and the Prisoner of Azkaban', 'Angels and Demons', \"Harry Potter and the Half-blood Prince:Children's Edition\", 'Fifty Shades Darker', 'Twilight', 'Girl with the Dragon Tattoo,The:Millennium Trilogy', 'Fifty Shades Freed', 'Lost Symbol,The', 'New Moon', 'Deception Point', 'Eclipse', 'Lovely Bones,The', 'Curious Incident of the Dog in the Night-time,The', 'Digital Fortress', 'Short History of Nearly Everything,A', 'Girl Who Played with Fire,The:Millennium Trilogy', 'Breaking Dawn', 'Very Hungry Caterpillar,The:The Very Hungry Caterpillar', 'Gruffalo,The', \"Jamie's 30-Minute Meals\", 'Kite Runner,The', 'One Day', 'Thousand Splendid Suns,A', \"Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy\", \"Time Traveler's Wife,The\", 'Atonement', \"Bridget Jones's Diary:A Novel\", 'World According to Clarkson,The', \"Captain Corelli's Mandolin\", 'Sound of Laughter,The', 'Life of Pi', 'Billy Connolly', 'Child Called It,A', \"Gruffalo's Child,The\", \"Angela's Ashes:A Memoir of a Childhood\", 'Birdsong', 'Northern Lights:His Dark Materials S.', 'Labyrinth', 'Harry Potter and the Half-blood Prince', 'Help,The', 'Man and Boy', 'Memoirs of a Geisha', \"No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S.\", 'Island,The', 'PS, I Love You', 'You are What You Eat:The Plan That Will Change Your Life', 'Shadow of the Wind,The', 'Tales of Beedle the Bard,The', 'Broker,The', \"Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P\", 'Subtle Knife,The:His Dark Materials S.', 'Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation', \"Delia's How to Cook:(Bk.1)\", 'Chocolat', 'Boy in the Striped Pyjamas,The', \"My Sister's Keeper\", 'Amber Spyglass,The:His Dark Materials S.', 'To Kill a Mockingbird', 'Men are from Mars, Women are from Venus:A Practical Guide for Improvin', 'Dear Fatty', 'Short History of Tractors in Ukrainian,A', 'Hannibal', 'Lord of the Rings,The', 'Stupid White Men:...and Other Sorry Excuses for the State of the Natio', 'Interpretation of Murder,The', 'Sharon Osbourne Extreme:My Autobiography', 'Alchemist,The:A Fable About Following Your Dream', \"At My Mother's Knee ...:and Other Low Joints\", 'Notes from a Small Island', 'Return of the Naked Chef,The', 'Bridget Jones: The Edge of Reason', \"Jamie's Italy\", 'I Can Make You Thin', 'Down Under', 'Summons,The', 'Small Island', 'Nigella Express', 'Brick Lane', \"Memory Keeper's Daughter,The\", 'Room on the Broom', 'About a Boy', 'My Booky Wook', 'God Delusion,The', '\"Beano\" Annual,The', 'White Teeth', 'House at Riverton,The', 'Book Thief,The', 'Nights of Rain and Stars', 'Ghost,The', 'Happy Days with the Naked Chef', 'Hunger Games,The:Hunger Games Trilogy', \"Lost Boy,The:A Foster Child's Search for the Love of a Family\", \"Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours\"]\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the books name in book_name list\n",
    "books = driver.find_elements(By.XPATH,\"//table//tr//td[2]\")\n",
    "for i in books:\n",
    "    book_name.append(i.text)\n",
    "print(book_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6411dada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brown, Dan', 'Rowling, J.K.', 'Rowling, J.K.', 'Rowling, J.K.', 'James, E. L.', 'Rowling, J.K.', 'Rowling, J.K.', 'Rowling, J.K.', 'Brown, Dan', 'Rowling, J.K.', 'James, E. L.', 'Meyer, Stephenie', 'Larsson, Stieg', 'James, E. L.', 'Brown, Dan', 'Meyer, Stephenie', 'Brown, Dan', 'Meyer, Stephenie', 'Sebold, Alice', 'Haddon, Mark', 'Brown, Dan', 'Bryson, Bill', 'Larsson, Stieg', 'Meyer, Stephenie', 'Carle, Eric', 'Donaldson, Julia', 'Oliver, Jamie', 'Hosseini, Khaled', 'Nicholls, David', 'Hosseini, Khaled', 'Larsson, Stieg', 'Niffenegger, Audrey', 'McEwan, Ian', 'Fielding, Helen', 'Clarkson, Jeremy', 'Bernieres, Louis de', 'Kay, Peter', 'Martel, Yann', 'Stephenson, Pamela', 'Pelzer, Dave', 'Donaldson, Julia', 'McCourt, Frank', 'Faulks, Sebastian', 'Pullman, Philip', 'Mosse, Kate', 'Rowling, J.K.', 'Stockett, Kathryn', 'Parsons, Tony', 'Golden, Arthur', 'McCall Smith, Alexander', 'Hislop, Victoria', 'Ahern, Cecelia', 'McKeith, Gillian', 'Zafon, Carlos Ruiz', 'Rowling, J.K.', 'Grisham, John', 'Atkins, Robert C.', 'Pullman, Philip', 'Truss, Lynne', 'Smith, Delia', 'Harris, Joanne', 'Boyne, John', 'Picoult, Jodi', 'Pullman, Philip', 'Lee, Harper', 'Gray, John', 'French, Dawn', 'Lewycka, Marina', 'Harris, Thomas', 'Tolkien, J. R. R.', 'Moore, Michael', 'Rubenfeld, Jed', 'Osbourne, Sharon', 'Coelho, Paulo', \"O'Grady, Paul\", 'Bryson, Bill', 'Oliver, Jamie', 'Fielding, Helen', 'Oliver, Jamie', 'McKenna, Paul', 'Bryson, Bill', 'Grisham, John', 'Levy, Andrea', 'Lawson, Nigella', 'Ali, Monica', 'Edwards, Kim', 'Donaldson, Julia', 'Hornby, Nick', 'Brand, Russell', 'Dawkins, Richard', '0', 'Smith, Zadie', 'Morton, Kate', 'Zusak, Markus', 'Binchy, Maeve', 'Harris, Robert', 'Oliver, Jamie', 'Collins, Suzanne', 'Pelzer, Dave', 'Oliver, Jamie']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the authors name in author_name list\n",
    "authors = driver.find_elements(By.XPATH,\"//table//tr//td[3]\")\n",
    "for i in authors:\n",
    "    author_name.append(i.text)\n",
    "print(author_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4803a762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5,094,805', '4,475,152', '4,200,654', '4,179,479', '3,758,936', '3,583,215', '3,484,047', '3,377,906', '3,193,946', '2,950,264', '2,479,784', '2,315,405', '2,233,570', '2,193,928', '2,183,031', '2,152,737', '2,062,145', '2,052,876', '2,005,598', '1,979,552', '1,928,900', '1,852,919', '1,814,784', '1,787,118', '1,783,535', '1,781,269', '1,743,266', '1,629,119', '1,616,068', '1,583,992', '1,555,135', '1,546,886', '1,539,428', '1,508,205', '1,489,403', '1,352,318', '1,310,207', '1,310,176', '1,231,957', '1,217,712', '1,208,711', '1,204,058', '1,184,967', '1,181,503', '1,181,093', '1,153,181', '1,132,336', '1,130,802', '1,126,337', '1,115,549', '1,108,328', '1,107,379', '1,104,403', '1,092,349', '1,090,847', '1,087,262', '1,054,196', '1,037,160', '1,023,688', '1,015,956', '1,009,873', '1,004,414', '1,003,780', '1,002,314', '998,213', '992,846', '986,753', '986,115', '970,509', '967,466', '963,353', '962,515', '959,496', '956,114', '945,640', '931,312', '925,425', '924,695', '906,968', '905,086', '890,847', '869,671', '869,659', '862,602', '856,540', '845,858', '842,535', '828,215', '820,563', '816,907', '816,585', '815,586', '814,370', '809,641', '808,900', '807,311', '794,201', '792,187', '791,507', '791,095']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the volumes sold details in volumes_sold list\n",
    "vol_sold = driver.find_elements(By.XPATH,\"//table//tr//td[4]\")\n",
    "for i in vol_sold:\n",
    "    volumes_sold.append(i.text)\n",
    "print(volumes_sold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "485c3be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Transworld', 'Bloomsbury', 'Bloomsbury', 'Bloomsbury', 'Random House', 'Bloomsbury', 'Bloomsbury', 'Bloomsbury', 'Transworld', 'Bloomsbury', 'Random House', 'Little, Brown Book', 'Quercus', 'Random House', 'Transworld', 'Little, Brown Book', 'Transworld', 'Little, Brown Book', 'Pan Macmillan', 'Random House', 'Transworld', 'Transworld', 'Quercus', 'Little, Brown Book', 'Penguin', 'Pan Macmillan', 'Penguin', 'Bloomsbury', 'Hodder & Stoughton', 'Bloomsbury', 'Quercus', 'Random House', 'Random House', 'Pan Macmillan', 'Penguin', 'Random House', 'Random House', 'Canongate', 'HarperCollins', 'Orion', 'Pan Macmillan', 'HarperCollins', 'Random House', 'Scholastic Ltd.', 'Orion', 'Bloomsbury', 'Penguin', 'HarperCollins', 'Random House', 'Little, Brown Book', 'Headline', 'HarperCollins', 'Penguin', 'Orion', 'Bloomsbury', 'Random House', 'Random House', 'Scholastic Ltd.', 'Profile Books Group', 'Random House', 'Transworld', 'Random House Childrens Books G', 'Hodder & Stoughton', 'Scholastic Ltd.', 'Random House', 'HarperCollins', 'Random House', 'Penguin', 'Random House', 'HarperCollins', 'Penguin', 'Headline', 'Little, Brown Book', 'HarperCollins', 'Transworld', 'Transworld', 'Penguin', 'Pan Macmillan', 'Penguin', 'Transworld', 'Transworld', 'Random House', 'Headline', 'Random House', 'Transworld', 'Penguin', 'Pan Macmillan', 'Penguin', 'Hodder & Stoughton', 'Transworld', 'D.C. Thomson', 'Penguin', 'Pan Macmillan', 'Transworld', 'Orion', 'Random House', 'Penguin', 'Scholastic Ltd.', 'Orion', 'Penguin']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the publishers name in publisher_name list\n",
    "publishers = driver.find_elements(By.XPATH,\"//table//tr//td[5]\")\n",
    "for i in publishers:\n",
    "    publisher_name.append(i.text)\n",
    "print(publisher_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9aa7f39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Crime, Thriller & Adventure', \"Children's Fiction\", \"Children's Fiction\", \"Children's Fiction\", 'Romance & Sagas', \"Children's Fiction\", \"Children's Fiction\", \"Children's Fiction\", 'Crime, Thriller & Adventure', \"Children's Fiction\", 'Romance & Sagas', 'Young Adult Fiction', 'Crime, Thriller & Adventure', 'Romance & Sagas', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'Popular Science', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'Picture Books', 'Picture Books', 'Food & Drink: General', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Humour: Collections & General', 'General & Literary Fiction', 'Autobiography: General', 'General & Literary Fiction', 'Biography: The Arts', 'Autobiography: General', 'Picture Books', 'Autobiography: General', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Science Fiction & Fantasy', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'General & Literary Fiction', 'Fitness & Diet', 'General & Literary Fiction', \"Children's Fiction\", 'Crime, Thriller & Adventure', 'Fitness & Diet', 'Young Adult Fiction', 'Usage & Writing Guides', 'Food & Drink: General', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Popular Culture & Media: General Interest', 'Autobiography: The Arts', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'Science Fiction & Fantasy', 'Current Affairs & Issues', 'Crime, Thriller & Adventure', 'Autobiography: The Arts', 'General & Literary Fiction', 'Autobiography: The Arts', 'Travel Writing', 'Food & Drink: General', 'General & Literary Fiction', 'National & Regional Cuisine', 'Fitness & Diet', 'Travel Writing', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'Food & Drink: General', 'General & Literary Fiction', 'General & Literary Fiction', 'Picture Books', 'General & Literary Fiction', 'Autobiography: The Arts', 'Popular Science', \"Children's Annuals\", 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Food & Drink: General', 'Young Adult Fiction', 'Biography: General', 'Food & Drink: General']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the books genre in genre list\n",
    "genres = driver.find_elements(By.XPATH,\"//table//tr//td[6]\")\n",
    "for i in genres:\n",
    "    genre.append(i.text)\n",
    "print(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e1f5aa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "# Checking the lengths of all the lists\n",
    "print(len(book_name),len(author_name),len(volumes_sold),len(publisher_name),len(genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a3074993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>Volumes Sold</th>\n",
       "      <th>Publisher Name</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Book Name       Author Name  \\\n",
       "1                                    Da Vinci Code,The        Brown, Dan   \n",
       "2                 Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "3             Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "4            Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "5                                 Fifty Shades of Grey      James, E. L.   \n",
       "..                                                 ...               ...   \n",
       "96                                           Ghost,The    Harris, Robert   \n",
       "97                      Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "98               Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "99   Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "100  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "    Volumes Sold   Publisher Name                        Genre  \n",
       "1      5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "2      4,475,152       Bloomsbury           Children's Fiction  \n",
       "3      4,200,654       Bloomsbury           Children's Fiction  \n",
       "4      4,179,479       Bloomsbury           Children's Fiction  \n",
       "5      3,758,936     Random House              Romance & Sagas  \n",
       "..           ...              ...                          ...  \n",
       "96       807,311     Random House   General & Literary Fiction  \n",
       "97       794,201          Penguin        Food & Drink: General  \n",
       "98       792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "99       791,507            Orion           Biography: General  \n",
       "100      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_details = {\n",
    "    'Book Name':book_name,\n",
    "    'Author Name':author_name,\n",
    "    'Volumes Sold':volumes_sold,\n",
    "    'Publisher Name':publisher_name,\n",
    "    'Genre':genre\n",
    "}\n",
    "\n",
    "# Creating dataframe\n",
    "df = pd.DataFrame(books_details,index=range(1,101))\n",
    "\n",
    "# Displaying dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5779ab",
   "metadata": {},
   "source": [
    "7. Program to scrape the details of the most watched tv series of all time from imdb.com.\n",
    "   \n",
    "   Url = https://www.imdb.com/list/ls095964455/\n",
    "   \n",
    "   Details to be scraped are:\n",
    "   \n",
    "   A) Name\n",
    "   \n",
    "   B) Year span\n",
    "\n",
    "   C) Genre\n",
    "\n",
    "   D) Run time\n",
    "\n",
    "   E) Ratings\n",
    "\n",
    "   F) Votes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8580a35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sending a request to a website for scraping data\n",
    "page = requests.get(\"https://www.imdb.com/list/ls095964455/\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d7948be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating WebDriver instance for controlling the Chrome browser using Selenium\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9271f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening webpage in the browser window controlled by the WebDriver instance\n",
    "driver.get(\"https://www.imdb.com/list/ls095964455/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "17f28856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for storing the details of the most watched TV series\n",
    "series_name = []\n",
    "series_year_span = []\n",
    "series_genre = []\n",
    "series_runtime = []\n",
    "series_ratings = []\n",
    "series_votes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "30798744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Game of Thrones', 'Stranger Things', 'The Walking Dead', '13 Reasons Why', 'The 100', 'Orange Is the New Black', 'Riverdale', \"Grey's Anatomy\", 'The Flash', 'Arrow', 'Money Heist', 'The Big Bang Theory', 'Black Mirror', 'Sherlock', 'Vikings', 'Pretty Little Liars', 'The Vampire Diaries', 'American Horror Story', 'Breaking Bad', 'Lucifer', 'Supernatural', 'Prison Break', 'How to Get Away with Murder', 'Teen Wolf', 'The Simpsons', 'Once Upon a Time', 'Narcos', 'Daredevil', 'Friends', 'How I Met Your Mother', 'Suits', 'Mr. Robot', 'The Originals', 'Supergirl', 'Gossip Girl', 'Sense8', 'Gotham', 'Westworld', 'Jessica Jones', 'Modern Family', 'Rick and Morty', 'Shadowhunters', 'The End of the F***ing World', 'House of Cards', 'Dark', 'Elite', 'Sex Education', 'Shameless', 'New Girl', 'Agents of S.H.I.E.L.D.', 'You', 'Dexter', 'Fear the Walking Dead', 'Family Guy', 'The Blacklist', 'Lost', 'Peaky Blinders', 'House', 'Quantico', 'Orphan Black', 'Homeland', 'Blindspot', \"DC's Legends of Tomorrow\", \"The Handmaid's Tale\", 'Chilling Adventures of Sabrina', 'The Good Doctor', 'Jane the Virgin', 'Glee', 'South Park', 'Brooklyn Nine-Nine', 'Under the Dome', 'The Umbrella Academy', 'True Detective', 'The OA', 'Desperate Housewives', 'Better Call Saul', 'Bates Motel', 'The Punisher', 'Atypical', 'Dynasty', 'This Is Us', 'The Good Place', 'Iron Fist', 'The Rain', 'Mindhunter', 'Revenge', 'Luke Cage', 'Scandal', 'The Defenders', 'Big Little Lies', 'Insatiable', 'The Mentalist', 'The Crown', 'Chernobyl', 'iZombie', 'Reign', 'A Series of Unfortunate Events', 'Criminal Minds', 'Scream: The TV Series', 'The Haunting of Hill House']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the series name in series_name list\n",
    "series = driver.find_elements(By.XPATH,\"//h3[@class='lister-item-header']//a\")\n",
    "for i in series:\n",
    "    series_name.append(i.text)\n",
    "print(series_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dada1575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(2011–2019)', '(2016–2024)', '(2010–2022)', '(2017–2020)', '(2014–2020)', '(2013–2019)', '(2017–2023)', '(2005– )', '(2014–2023)', '(2012–2020)', '(2017–2021)', '(2007–2019)', '(2011– )', '(2010–2017)', '(2013–2020)', '(2010–2017)', '(2009–2017)', '(2011– )', '(2008–2013)', '(2016–2021)', '(2005–2020)', '(2005–2017)', '(2014–2020)', '(2011–2017)', '(1989– )', '(2011–2018)', '(2015–2017)', '(2015–2018)', '(1994–2004)', '(2005–2014)', '(2011–2019)', '(2015–2019)', '(2013–2018)', '(2015–2021)', '(2007–2012)', '(2015–2018)', '(2014–2019)', '(2016–2022)', '(2015–2019)', '(2009–2020)', '(2013– )', '(2016–2019)', '(2017–2019)', '(2013–2018)', '(2017–2020)', '(2018– )', '(2019– )', '(2011–2021)', '(2011–2018)', '(2013–2020)', '(2018–2024)', '(2006–2013)', '(2015–2023)', '(1999– )', '(2013–2023)', '(2004–2010)', '(2013–2022)', '(2004–2012)', '(2015–2018)', '(2013–2017)', '(2011–2020)', '(2015–2020)', '(2016–2022)', '(2017– )', '(2018–2020)', '(2017– )', '(2014–2019)', '(2009–2015)', '(1997– )', '(2013–2021)', '(2013–2015)', '(2019–2023)', '(2014– )', '(2016–2019)', '(2004–2012)', '(2015–2022)', '(2013–2017)', '(2017–2019)', '(2017–2021)', '(2017–2022)', '(2016–2022)', '(2016–2020)', '(2017–2018)', '(2018–2020)', '(2017–2019)', '(2011–2015)', '(2016–2018)', '(2012–2018)', '(2017)', '(2017–2019)', '(2018–2019)', '(2008–2015)', '(2016–2023)', '(2019)', '(2015–2019)', '(2013–2017)', '(2017–2019)', '(2005– )', '(2015–2019)', '(2018)']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the series year span in series_year_span list\n",
    "year_span = driver.find_elements(By.XPATH,\"//span[@class='lister-item-year text-muted unbold']\")\n",
    "for i in year_span:\n",
    "    series_year_span.append(i.text)\n",
    "print(series_year_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4b8a00e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Action, Adventure, Drama', 'Drama, Fantasy, Horror', 'Drama, Horror, Thriller', 'Drama, Mystery, Thriller', 'Drama, Mystery, Sci-Fi', 'Comedy, Crime, Drama', 'Crime, Drama, Mystery', 'Drama, Romance', 'Action, Adventure, Drama', 'Action, Adventure, Crime', 'Action, Crime, Drama', 'Comedy, Romance', 'Drama, Mystery, Sci-Fi', 'Crime, Drama, Mystery', 'Action, Adventure, Drama', 'Drama, Mystery, Romance', 'Drama, Fantasy, Horror', 'Drama, Horror, Sci-Fi', 'Crime, Drama, Thriller', 'Crime, Drama, Fantasy', 'Drama, Fantasy, Horror', 'Action, Crime, Drama', 'Crime, Drama, Mystery', 'Action, Drama, Fantasy', 'Animation, Comedy', 'Adventure, Fantasy, Romance', 'Biography, Crime, Drama', 'Action, Crime, Drama', 'Comedy, Romance', 'Comedy, Drama, Romance', 'Comedy, Drama', 'Crime, Drama, Thriller', 'Drama, Fantasy, Horror', 'Action, Adventure, Drama', 'Drama, Romance', 'Drama, Mystery, Sci-Fi', 'Action, Crime, Drama', 'Drama, Mystery, Sci-Fi', 'Action, Crime, Drama', 'Comedy, Drama, Romance', 'Animation, Adventure, Comedy', 'Action, Drama, Fantasy', 'Adventure, Comedy, Crime', 'Drama', 'Crime, Drama, Mystery', 'Crime, Drama, Thriller', 'Comedy, Drama', 'Comedy, Drama', 'Comedy, Romance', 'Action, Adventure, Drama', 'Crime, Drama, Romance', 'Crime, Drama, Mystery', 'Drama, Horror, Sci-Fi', 'Animation, Comedy', 'Crime, Drama, Mystery', 'Adventure, Drama, Fantasy', 'Crime, Drama', 'Drama, Mystery', 'Crime, Drama, Mystery', 'Drama, Sci-Fi, Thriller', 'Crime, Drama, Mystery', 'Action, Crime, Drama', 'Action, Adventure, Drama', 'Drama, Sci-Fi, Thriller', 'Drama, Fantasy, Horror', 'Drama', 'Comedy', 'Comedy, Drama, Music', 'Animation, Comedy', 'Comedy, Crime', 'Drama, Mystery, Sci-Fi', 'Action, Adventure, Comedy', 'Crime, Drama, Mystery', 'Drama, Fantasy, Mystery', 'Comedy, Drama, Mystery', 'Crime, Drama', 'Drama, Horror, Mystery', 'Action, Crime, Drama', 'Comedy, Drama', 'Drama', 'Comedy, Drama, Romance', 'Comedy, Drama, Fantasy', 'Action, Adventure, Crime', 'Drama, Sci-Fi, Thriller', 'Crime, Drama, Mystery', 'Drama, Mystery, Thriller', 'Action, Crime, Drama', 'Drama, Thriller', 'Action, Adventure, Crime', 'Crime, Drama, Mystery', 'Comedy, Drama, Thriller', 'Crime, Drama, Mystery', 'Biography, Drama, History', 'Drama, History, Thriller', 'Comedy, Crime, Drama', 'Drama', 'Adventure, Comedy, Drama', 'Crime, Drama, Mystery', 'Comedy, Crime, Drama', 'Drama, Horror, Mystery']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the series genre in series_genre list\n",
    "genre = driver.find_elements(By.XPATH,\"//span[@class='genre']\")\n",
    "for i in genre:\n",
    "    series_genre.append(i.text)\n",
    "print(series_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7a9fec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['57 min', '51 min', '44 min', '60 min', '43 min', '59 min', '45 min', '41 min', '43 min', '42 min', '70 min', '22 min', '60 min', '88 min', '44 min', '44 min', '43 min', '60 min', '49 min', '42 min', '44 min', '44 min', '43 min', '41 min', '22 min', '60 min', '49 min', '54 min', '22 min', '22 min', '44 min', '49 min', '45 min', '43 min', '42 min', '60 min', '42 min', '62 min', '56 min', '22 min', '23 min', '42 min', '25 min', '51 min', '60 min', '60 min', '45 min', '46 min', '22 min', '45 min', '45 min', '53 min', '44 min', '22 min', '43 min', '44 min', '60 min', '44 min', '42 min', '44 min', '55 min', '42 min', '42 min', '60 min', '60 min', '41 min', '60 min', '44 min', '22 min', '22 min', '43 min', '60 min', '55 min', '60 min', '45 min', '46 min', '45 min', '53 min', '30 min', '42 min', '45 min', '22 min', '55 min', '45 min', '60 min', '44 min', '55 min', '43 min', '50 min', '60 min', '45 min', '43 min', '58 min', '330 min', '42 min', '42 min', '50 min', '42 min', '45 min', '572 min']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the series runtime in series_runtime list\n",
    "runtime = driver.find_elements(By.XPATH,\"//span[@class='runtime']\")\n",
    "for i in runtime:\n",
    "    series_runtime.append(i.text)\n",
    "print(series_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5e32b081",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9.2', '8.7', '8.1', '7.5', '7.6', '8.1', '6.5', '7.6', '7.5', '7.5', '8.2', '8.2', '8.8', '9.1', '8.5', '7.4', '7.7', '8', '9.5', '8.1', '8.4', '8.3', '8.1', '7.7', '8.7', '7.7', '8.8', '8.6', '8.9', '8.3', '8.4', '8.5', '8.3', '6.2', '7.5', '8.2', '7.8', '8.5', '7.9', '8.5', '9.1', '6.5', '8', '8.7', '8.7', '7.3', '8.3', '8.6', '7.8', '7.5', '7.7', '8.7', '6.8', '8.2', '8', '8.3', '8.8', '8.7', '6.7', '8.3', '8.3', '7.3', '6.8', '8.4', '7.4', '8', '7.9', '6.8', '8.7', '8.4', '6.5', '7.9', '8.9', '7.8', '7.6', '9', '8.1', '8.5', '8.2', '7.3', '8.7', '8.2', '6.4', '6.3', '8.6', '7.8', '7.3', '7.7', '7.2', '8.5', '6.5', '8.2', '8.6', '9.4', '7.8', '7.4', '7.8', '8.1', '7', '8.6']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the series ratings in series_ratings list\n",
    "ratings = driver.find_elements(By.XPATH,\"//div[@class='ipl-rating-star small']//span[@class='ipl-rating-star__rating']\")\n",
    "for i in ratings:\n",
    "    series_ratings.append(i.text)\n",
    "print(series_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bc98a8d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2,174,635', '1,252,359', '1,032,909', '303,703', '262,848', '310,738', '149,537', '323,997', '359,867', '438,805', '499,067', '832,390', '591,118', '954,939', '554,204', '172,576', '332,927', '328,219', '1,994,051', '338,133', '461,091', '554,364', '158,412', '156,638', '419,605', '230,198', '444,889', '455,258', '1,031,628', '703,476', '429,211', '400,512', '141,400', '127,009', '181,811', '158,438', '235,555', '517,097', '220,180', '453,539', '556,444', '66,955', '204,538', '516,218', '412,264', '84,859', '303,099', '257,769', '234,887', '221,266', '280,515', '740,923', '136,182', '351,881', '264,705', '570,668', '586,648', '482,387', '62,439', '113,720', '350,555', '76,598', '107,715', '247,730', '101,529', '104,179', '54,846', '152,042', '390,184', '335,452', '109,453', '259,317', '598,835', '109,156', '133,564', '584,368', '112,226', '249,227', '96,521', '23,847', '150,369', '173,774', '135,343', '39,421', '308,430', '122,364', '135,356', '76,909', '112,220', '211,440', '30,740', '191,846', '231,249', '803,662', '71,296', '51,984', '64,014', '208,616', '43,411', '260,405']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the series votes in series_votes list\n",
    "votes = driver.find_elements(By.XPATH,\"//span[@name='nv']\")\n",
    "for i in votes:\n",
    "    series_votes.append(i.text)\n",
    "print(series_votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "62f6b155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "# Checking lengths of all the lists\n",
    "print(len(series_name),len(series_year_span),len(series_genre),len(series_runtime),len(series_ratings),len(series_votes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "454b1c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series Name</th>\n",
       "      <th>Year Span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run Time</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,174,635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016–2024)</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,252,359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1,032,909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>303,703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>262,848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Reign</td>\n",
       "      <td>(2013–2017)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.4</td>\n",
       "      <td>51,984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>A Series of Unfortunate Events</td>\n",
       "      <td>(2017–2019)</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>64,014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Criminal Minds</td>\n",
       "      <td>(2005– )</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>208,616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Scream: The TV Series</td>\n",
       "      <td>(2015–2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7</td>\n",
       "      <td>43,411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>Drama, Horror, Mystery</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>260,405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Series Name    Year Span                     Genre  \\\n",
       "1                   Game of Thrones  (2011–2019)  Action, Adventure, Drama   \n",
       "2                   Stranger Things  (2016–2024)    Drama, Fantasy, Horror   \n",
       "3                  The Walking Dead  (2010–2022)   Drama, Horror, Thriller   \n",
       "4                    13 Reasons Why  (2017–2020)  Drama, Mystery, Thriller   \n",
       "5                           The 100  (2014–2020)    Drama, Mystery, Sci-Fi   \n",
       "..                              ...          ...                       ...   \n",
       "96                            Reign  (2013–2017)                     Drama   \n",
       "97   A Series of Unfortunate Events  (2017–2019)  Adventure, Comedy, Drama   \n",
       "98                   Criminal Minds     (2005– )     Crime, Drama, Mystery   \n",
       "99            Scream: The TV Series  (2015–2019)      Comedy, Crime, Drama   \n",
       "100      The Haunting of Hill House       (2018)    Drama, Horror, Mystery   \n",
       "\n",
       "    Run Time Ratings      Votes  \n",
       "1     57 min     9.2  2,174,635  \n",
       "2     51 min     8.7  1,252,359  \n",
       "3     44 min     8.1  1,032,909  \n",
       "4     60 min     7.5    303,703  \n",
       "5     43 min     7.6    262,848  \n",
       "..       ...     ...        ...  \n",
       "96    42 min     7.4     51,984  \n",
       "97    50 min     7.8     64,014  \n",
       "98    42 min     8.1    208,616  \n",
       "99    45 min       7     43,411  \n",
       "100  572 min     8.6    260,405  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_series_details = {\n",
    "    'Series Name':series_name,\n",
    "    'Year Span':series_year_span,\n",
    "    'Genre':series_genre,\n",
    "    'Run Time':series_runtime,\n",
    "    'Ratings':series_ratings,\n",
    "    'Votes':series_votes\n",
    "}\n",
    "\n",
    "# Creating dataframe\n",
    "df = pd.DataFrame(tv_series_details,index=range(1,101))\n",
    "\n",
    "# Displaying dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8938a1d1",
   "metadata": {},
   "source": [
    "8. Details of Datasets from UCI machine learning repositories.\n",
    "    \n",
    "    Url = https://archive.ics.uci.edu/\n",
    "    \n",
    "    Details to be scraped are:\n",
    "    \n",
    "    A) Dataset name\n",
    "    \n",
    "    B) Data type\n",
    "    \n",
    "    C) Task\n",
    "    \n",
    "    D) Attribute type\n",
    "    \n",
    "    E) No of instances\n",
    "    \n",
    "    F) No of attribute\n",
    "    \n",
    "    G) Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8041a34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sending a request to a website for scraping data\n",
    "page = requests.get(\"https://archive.ics.uci.edu/\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3296470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating WebDriver instance for controlling the Chrome browser using Selenium\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "29dfa267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening webpage in the browser window controlled by the WebDriver instance\n",
    "driver.get(\"https://archive.ics.uci.edu/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "68b27357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on the show all dataset option\n",
    "show_all_datasets = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/header/nav/ul/li[1]/a\")\n",
    "show_all_datasets.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b6a9a9b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clicking on the expand all option for more details \n",
    "expand_all = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[2]/div[1]/div/label[2]\")\n",
    "expand_all.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "89dd88e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for storing the details of the datasets\n",
    "dataset_name = []\n",
    "data_type = []\n",
    "task = []\n",
    "attribute_type = []\n",
    "no_of_instances = []\n",
    "no_of_attributes = []\n",
    "year = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c1b6ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrolling scroll bar downwards for clicking the next button\n",
    "driver.execute_script(\"window.scrollBy(0,2000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1a03f8dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = 0\n",
    "end = 63 # There are total 623 datasets 1O in each page\n",
    "\n",
    "for page in range(start,end):\n",
    "    \n",
    "    # Scraping and storing the datasets name in dataset_name list\n",
    "    dtst_name = driver.find_elements(By.XPATH,\"//a[@class='link-hover link text-xl font-semibold']\")\n",
    "    for i in dtst_name:\n",
    "        dataset_name.append(i.text)\n",
    "        \n",
    "    # Scraping and storing the data type of datasets in data_type list\n",
    "    dt_type = driver.find_elements(By.XPATH,\"//div[@class='my-2 hidden gap-4 md:grid grid-cols-12']//div[2]//span\")\n",
    "    for i in dt_type:\n",
    "        if i.text == '':\n",
    "            data_type.append(i.text.replace('','-'))\n",
    "        elif i.text == 'null':\n",
    "            data_type.append(i.text.replace('null','-'))\n",
    "        else:\n",
    "            data_type.append(i.text)\n",
    "        \n",
    "    # Scraping and storing the task of datasets in task list\n",
    "    tsk = driver.find_elements(By.XPATH,\"//div[@class='my-2 hidden gap-4 md:grid grid-cols-12']//div[1]//span\")\n",
    "    for i in tsk:\n",
    "        if i.text == '':\n",
    "            task.append(i.text.replace('','-'))\n",
    "        else:\n",
    "            task.append(i.text)\n",
    "    \n",
    "    # Scraping and storing the attribute type of datasets in attribute_type list\n",
    "    atb_type = driver.find_elements(By.XPATH,\"//tbody//tr//td[2]\")\n",
    "    for i in atb_type:\n",
    "        if i.text == 'N/A':\n",
    "            attribute_type.append(i.text.replace('N/A','-'))\n",
    "        elif i.text == '':\n",
    "            attribute_type.append(i.text.replace('','-'))\n",
    "        else:\n",
    "            attribute_type.append(i.text)\n",
    "    \n",
    "    # Scraping and storing the no. of instances in no_of_instances list\n",
    "    no_of_inst = driver.find_elements(By.XPATH,\"//div[@class='my-2 hidden gap-4 md:grid grid-cols-12']//div[3]//span\")\n",
    "    for i in no_of_inst:\n",
    "        if i.text == '':\n",
    "            no_of_instances.append(i.text.replace('','-'))\n",
    "        else:\n",
    "            no_of_instances.append(i.text)\n",
    "        \n",
    "    # Scraping and storing the no. of attributes in no_of_attributes list\n",
    "    no_of_atb = driver.find_elements(By.XPATH,\"//div[@class='my-2 hidden gap-4 md:grid grid-cols-12']//div[4]//span\")\n",
    "    for i in no_of_atb:\n",
    "        if i.text == '':\n",
    "            no_of_attributes.append(i.text.replace('','-'))\n",
    "        else:\n",
    "            no_of_attributes.append(i.text)\n",
    "    \n",
    "    # Scraping and storing the datasets published year in year list\n",
    "    yrs = driver.find_elements(By.XPATH,\"//tbody//tr//td[3]\")\n",
    "    for i in yrs:\n",
    "        if i.text == 'N/A':\n",
    "            year.append(i.text.replace('N/A','-'))\n",
    "        else:\n",
    "            year.append(i.text)\n",
    "    \n",
    "    # Clicking on the next button\n",
    "    try:\n",
    "        next_btn = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[2]/div[3]/div/button[2]\")\n",
    "        next_btn.click()\n",
    "        time.sleep(1)\n",
    "    except ElementClickInterceptedException:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "36ac10fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris', 'Heart Disease', 'Adult', 'Dry Bean Dataset', 'Diabetes', 'Rice (Cammeo and Osmancik)', 'Wine', 'Car Evaluation', 'Breast Cancer Wisconsin (Diagnostic)', 'Mushroom', 'Abalone', 'Breast Cancer', 'Glass Identification', 'Statlog (German Credit Data)', 'Census Income', 'Breast Cancer Wisconsin (Original)', 'Thyroid Disease', 'Auto MPG', 'Optical Recognition of Handwritten Digits', 'Spambase', 'Abalone', 'Breast Cancer', 'Glass Identification', 'Statlog (German Credit Data)', 'Census Income', 'Breast Cancer Wisconsin (Original)', 'Thyroid Disease', 'Auto MPG', 'Optical Recognition of Handwritten Digits', 'Spambase', 'Letter Recognition', 'Congressional Voting Records', 'Image Segmentation', 'Bank Marketing', 'Hepatitis', 'Yeast', 'Tic-Tac-Toe Endgame', 'Computer Hardware', 'Online Retail II', 'Reuters-21578 Text Categorization Collection', 'Connectionist Bench (Sonar, Mines vs. Rocks)', 'Nursery', 'Covertype', 'Statlog (Australian Credit Approval)', 'Student Performance', 'Soybean (Large)', 'Breast Cancer Wisconsin (Prognostic)', 'Statlog Project', 'Arrhythmia', 'Soybean (Small)', 'Connectionist Bench (Sonar, Mines vs. Rocks)', 'Nursery', 'Covertype', 'Statlog (Australian Credit Approval)', 'Student Performance', 'Soybean (Large)', 'Breast Cancer Wisconsin (Prognostic)', 'Statlog Project', 'Arrhythmia', 'Soybean (Small)', 'Statlog (Heart)', 'Protein Data', 'Ecoli', 'Anonymous Microsoft Web Data', 'Insurance Company Benchmark (COIL 2000)', 'Dermatology', 'Online Retail', 'Contraceptive Method Choice', 'Online Shoppers Purchasing Intention Dataset', 'Musk (Version 1)', 'Solar Flare', 'Air Quality', 'default of credit card clients', 'Lenses', 'Movie', 'Servo', \"Haberman's Survival\", 'CMU Face Images', 'Flags', 'SPECT Heart', 'Chess (King-Rook vs. King-Pawn)', 'Horse Colic', 'KDD Cup 1999 Data', 'Estimation of obesity levels based on eating habits and physical condition', 'Primary Tumor', 'Census-Income (KDD)', 'Echocardiogram', 'US Census Data (1990)', 'Annealing', 'Balloons', 'Chess (King-Rook vs. King-Pawn)', 'Horse Colic', 'KDD Cup 1999 Data', 'Estimation of obesity levels based on eating habits and physical condition', 'Primary Tumor', 'Census-Income (KDD)', 'Echocardiogram', 'US Census Data (1990)', 'Annealing', 'Balloons', 'Japanese Credit Screening', 'Individual household electric power consumption', 'Chess (King-Rook vs. King-Knight)', 'Apartment for rent classified', 'Audiology (Standardized)', 'Australian Sign Language signs', 'Connect-4', 'Corel Image Features', 'LED Display Domain', 'Waveform Database Generator (Version 2)', 'Japanese Credit Screening', 'Individual household electric power consumption', 'Chess (King-Rook vs. King-Knight)', 'Apartment for rent classified', 'Audiology (Standardized)', 'Australian Sign Language signs', 'Connect-4', 'Corel Image Features', 'LED Display Domain', 'Waveform Database Generator (Version 2)', 'Traffic Flow Forecasting', 'Connectionist Bench (Vowel Recognition - Deterding Data)', 'Forest Fires', 'Hayes-Roth', 'Land Mines', 'MetroPT-3 Dataset', 'Drug Review Dataset (Drugs.com)', 'Musk (Version 2)', 'DeFungi', 'El Nino', 'KDD Cup 1998 Data', 'Shuttle Landing Control', 'Pittsburgh Bridges', 'Bike Sharing Dataset', 'Entree Chicago Recommendation Data', 'SPECTF Heart', 'Australian Sign Language signs (High Quality)', 'SMS Spam Collection', 'HAR70+', 'Real estate valuation data set', 'Smartphone Dataset for anomaly detection in Crowds.', 'Early stage diabetes risk prediction dataset.', 'Trains', 'DARWIN', 'Economic Sanctions', 'Productivity Prediction of Garment Employees', 'Syskill and Webert Web Page Ratings', 'Mobile Robots', 'Traffic Flow Forecasting', 'Online News Popularity', 'IPUMS Census Database', 'Energy efficiency', 'seeds', 'Wholesale customers', 'Absenteeism at work', 'UNIX User Data', 'clickstream data for online shopping', 'Turkish User Review Dataset', 'HARTH', 'AI4I 2020 Predictive Maintenance Dataset', 'Seoul Bike Sharing Demand', 'Chess (Domain Theories)', 'Concrete Compressive Strength', 'WESAD (Wearable Stress and Affect Detection)', 'MAGIC Gamma Telescope', 'Tennis Major Tournament Match Statistics', 'Appliances energy prediction', 'Hierarchical Sales Data', 'Drug consumption (quantified)', 'Bar Crawl: Detecting Heavy Drinking', 'Beijing PM2.5 Data', 'Communities and Crime', 'Parkinsons', 'Beijing Multi-Site Air-Quality Data', 'banknote authentication', 'Taiwanese Bankruptcy Prediction', 'SoDA', 'Multivariate Gait Data', 'ImageNet', 'Sales_Transactions_Dataset_Weekly', 'Iranian Churn Dataset', 'ElectricityLoadDiagrams20112014', 'Physical Therapy Exercises Dataset', 'Phishing Websites', 'MNIST Database of Handwritten Digits', 'South German Credit (UPDATE)', 'Travel Reviews', 'Polish companies bankruptcy data', 'Poker Hand', 'Palmer penguins', 'Iranian Churn Dataset', 'ElectricityLoadDiagrams20112014', 'Physical Therapy Exercises Dataset', 'Phishing Websites', 'MNIST Database of Handwritten Digits', 'South German Credit (UPDATE)', 'Travel Reviews', 'Polish companies bankruptcy data', 'Poker Hand', 'Palmer penguins', 'NATICUSdroid (Android Permissions) Dataset', 'Chronic_Kidney_Disease', 'Facebook metrics', 'ISTANBUL STOCK EXCHANGE', 'Breast Cancer Coimbra', 'Metro Interstate Traffic Volume', 'HCV data', 'Dow Jones Index', 'Divorce Predictors data set', 'Cervical cancer (Risk Factors)', 'Myocardial infarction complications', 'accelerometer_gyro_mobile_phone_dataset', 'Sentiment Labelled Sentences', 'gene expression cancer RNA-Seq', 'Stock portfolio performance', 'in-vehicle coupon recommendation', 'MaskReminder', 'Cardiotocography', 'Synchronous Machine Data Set', 'Amazon Access Samples', 'Parkinsons Telemonitoring', 'Auction Verification', 'Detect Malware Types', 'Gender by Name', 'Pedestrian in Traffic Dataset', 'Basketball dataset', 'Daily and Sports Activities', 'COVID-19 Surveillance', 'FMA: A Dataset For Music Analysis', 'Blood Transfusion Service Center', 'Gas Sensor Array Drift Dataset at Different Concentrations', 'Human Activity Recognition from Continuous Ambient Sensor Data', 'Open Web Text Corpus', 'Drug Review Dataset (Druglib.com)', 'Algerian Forest Fires Dataset', 'Student Academics Performance', 'South German Credit', 'Website Phishing', 'detection_of_IoT_botnet_attacks_N_BaIoT', 'Superconductivty Data', 'HIGGS', 'Electrical Grid Stability Simulated Data', 'Autism Screening Adult', 'Tarvel Review Ratings', 'Gas Turbine CO and NOx Emission Data Set', 'Mammographic Mass', 'Tamilnadu Electricity Board Hourly Readings', 'Occupancy Detection', 'Kitsune Network Attack Dataset', 'EEG Eye State', 'Hungarian Chickenpox Cases', 'Wikipedia Math Essentials', 'Dota2 Games Results', 'Student Performance on an entrance examination', 'Residential Building Data Set', 'CIFAR-10', 'Cuff-Less Blood Pressure Estimation', 'Gas sensor array temperature modulation', 'Ozone Level Detection', 'Dataset based on UWB for Clinical Establishments', 'Heterogeneity Activity Recognition', 'Alcohol QCM Sensor Dataset', 'ILPD (Indian Liver Patient Dataset)', 'Perfume Data', 'Othello Domain Theory', 'Las Vegas Strip', 'Smartphone-Based Recognition of Human Activities and Postural Transitions', 'Bone marrow transplant: children', 'Plants', 'BitcoinHeistRansomwareAddressDataset', 'Open University Learning Analytics dataset', 'Unmanned Aerial Vehicle (UAV) Intrusion Detection', 'Parkinson Speech Dataset with Multiple Types of Sound Recordings', 'Acute Inflammations', 'Vertebral Column', 'NIPS Conference Papers 1987-2015', 'Condition monitoring of hydraulic systems', 'MHEALTH Dataset', 'Codon usage', 'Cervical Cancer Behavior Risk', 'Mice Protein Expression', 'Audit Data', 'YouTube Spam Collection', 'Hepatitis C Virus (HCV) for Egyptian patients', 'Early biomarkers of Parkinson’s disease based on natural connected speech', 'Facebook Live Sellers in Thailand', 'Parking Birmingham', 'Paper Reviews', 'Incident management process enriched event log', 'Sports articles for objectivity analysis', 'CNNpred: CNN-based stock market prediction using a diverse set of variables', 'Facebook Comment Volume Dataset', 'LT-FS-ID: Intrusion detection in WSNs', 'WISDM Smartphone and Smartwatch Activity and Biometrics Dataset', 'Communities and Crime Unnormalized', 'YearPredictionMSD', 'Machine Learning based ZZAlpha Ltd. Stock Recommendations 2012-2014', 'Cargo 2000 Freight Tracking and Tracing', 'Diabetic Retinopathy Debrecen Data Set', 'Water Treatment Plant', 'User Knowledge Modeling', 'Similarity Prediction', 'Amazon Commerce reviews set', 'QSAR fish toxicity', 'Gas sensor array under dynamic gas mixtures', 'PAMAP2 Physical Activity Monitoring', 'EEG Steady-State Visual Evoked Potential Signals', '2D elastodynamic metamaterials', 'Buzz in social media', 'Bag of Words', 'Bar Crawl: Detecting Heavy Drinking', 'Parkinson Disease Spiral Drawings Using Digitized Graphics Tablet', 'Skin Segmentation', 'Autistic Spectrum Disorder Screening Data for Children', 'Greenhouse Gas Observing Network', 'Concrete Slump Test', 'Gait Classification', 'YouTube Multiview Video Games Dataset', 'Air quality', 'EMG data for gestures', 'Bar Crawl: Detecting Heavy Drinking', 'Parkinson Disease Spiral Drawings Using Digitized Graphics Tablet', 'Skin Segmentation', 'Autistic Spectrum Disorder Screening Data for Children', 'Greenhouse Gas Observing Network', 'Concrete Slump Test', 'Gait Classification', 'YouTube Multiview Video Games Dataset', 'Air quality', 'EMG data for gestures', 'Vehicle routing and scheduling problems', 'Sundanese Twitter Dataset', 'Caesarian Section Classification Dataset', 'Yacht Hydrodynamics', 'Leaf', 'Stock keeping units', '2.4 GHZ Indoor Channel Measurements', 'Deepfakes: Medical Image Tamper Detection', 'Gas sensors for home activity monitoring', 'Swarm Behaviour', 'Activity recognition using wearable physiological measurements', 'Bias correction of numerical prediction model temperature forecast', '181 early modern English plays: Transcriptions of early editions in TEI encoding', 'Steel Plates Faults', 'A study of Asian Religious and Biblical Texts', 'Bengali Hate Speech Detection Dataset', 'OPPORTUNITY Activity Recognition', 'University', 'Multi-view Brain Networks', 'Dishonest Internet users Dataset', 'Activity recognition using wearable physiological measurements', 'Bias correction of numerical prediction model temperature forecast', '181 early modern English plays: Transcriptions of early editions in TEI encoding', 'Steel Plates Faults', 'A study of Asian Religious and Biblical Texts', 'Bengali Hate Speech Detection Dataset', 'OPPORTUNITY Activity Recognition', 'University', 'Multi-view Brain Networks', 'Dishonest Internet users Dataset', 'Breast Tissue', 'HTRU2', 'SECOM', 'Exasens', 'Health News in Twitter', 'Breath Metabolomics', 'Geographical Original of Music', 'Restaurant & consumer data', 'Activity recognition with healthy older people using a batteryless wearable sensor', 'Thoracic Surgery Data', 'Wheat kernels', 'Physicochemical Properties of Protein Tertiary Structure', 'Climate Model Simulation Crashes', 'Grammatical Facial Expressions', 'BuddyMove Data Set', 'Dresses_Attribute_Sales', 'Crop mapping using fused optical-radar data set', 'PPG-DaLiA', 'Japanese Vowels', 'Real-time Election Results: Portugal 2019', '3D Road Network (North Jutland, Denmark)', 'SUSY', 'Teaching Assistant Evaluation', 'Dataset for Sensorless Drive Diagnosis', 'Robot Execution Failures', 'Somerville Happiness Survey', 'Student Loan Relational', 'Daphnet Freezing of Gait', 'Madelon', 'Qualitative_Bankruptcy', 'Gas sensor array exposed to turbulent gas mixtures', 'Optical Interconnection Network', 'Facebook Large Page-Page Network', 'UJIIndoorLoc', 'Activity Recognition system based on Multisensor data fusion (AReM)', 'Guitar Chords finger positions', 'Wikipedia Math Essentials', 'CSM (Conventional and Social Media Movies) Dataset 2014 and 2015', 'Parkinson Dataset with replicated acoustic features', 'Twin gas sensor arrays', 'Weight Lifting Exercises monitored with Inertial Measurement Units', 'Mechanical Analysis', 'Chemical Composition of Ceramic Samples', 'TV News Channel Commercial Detection Dataset', 'Arcene', 'GPS Trajectories', 'Gas sensor array under flow modulation', 'HCC Survival', 'BlogFeedback', 'Motion Capture Hand Postures', 'Carbon Nanotubes', 'Shill Bidding Dataset', 'URL Reputation', 'sEMG for Basic Hand movements', 'News Aggregator', 'Internet Usage Data', 'SkillCraft1 Master Table Dataset', 'Gas sensor arrays in open sampling settings', 'Simulated Falls and Daily Living Activities Data Set', 'Farm Ads', 'Carbon Nanotubes', 'Shill Bidding Dataset', 'URL Reputation', 'sEMG for Basic Hand movements', 'News Aggregator', 'Internet Usage Data', 'SkillCraft1 Master Table Dataset', 'Gas sensor arrays in open sampling settings', 'Simulated Falls and Daily Living Activities Data Set', 'KEGG Metabolic Relation Network (Directed)', 'LastFM Asia Social Network', 'Character Font Images', 'Dorothea', 'Autistic Spectrum Disorder Screening Data for Adolescent', 'Single elder home monitoring: Gas and position', 'APS Failure at Scania Trucks', 'Synthetic Control Chart Time Series', 'Anuran Calls (MFCCs)', ': Simulated Data set of Iraqi tourism places', 'Localization Data for Person Activity', 'User Identification From Walking Activity', 'Twitter Data set for Arabic Sentiment Analysis', 'Semeion Handwritten Digit', 'Ultrasonic flowmeter diagnostics', 'seismic-bumps', 'DrivFace', 'Wisesight Sentiment Corpus', 'QSAR biodegradation', 'BLE RSSI Dataset for Indoor localization and Navigation', 'Wall-Following Robot Navigation Data', 'EMG Physical Action Data Set', 'Sirtuin6 Small Molecules', 'HEPMASS', 'Libras Movement', 'Gesture Phase Segmentation', 'Cryotherapy Dataset', 'Forest type mapping', 'Character Trajectories', 'Hybrid Indoor Positioning Dataset from WiFi RSSI, Bluetooth and magnetometer', 'Twenty Newsgroups', 'NoisyOffice', 'Legal Case Reports', 'Avila', 'Container Crane Controller Data Set', 'Detect Malacious Executable(AntiVirus)', 'KEGG Metabolic Reaction Network (Undirected)', 'Wearable Computing: Classification of Body Postures and Movements (PUC-Rio)', 'Divorce Predictors data set', 'Multimodal Damage Identification for Humanitarian Computing', 'GNFUV Unmanned Surface Vehicles Sensor Data Set 2', 'QSAR Bioconcentration classes dataset', 'SGEMM GPU kernel performance', 'Artificial Characters', 'Malware static and dynamic features VxHeaven and Virus Total', 'EMG dataset in Lower Limb', 'wiki4HE', 'Horton General Hospital', 'StoneFlakes', 'GNFUV Unmanned Surface Vehicles Sensor Data', 'p53 Mutants', 'QSAR Bioconcentration classes dataset', 'SGEMM GPU kernel performance', 'Artificial Characters', 'Malware static and dynamic features VxHeaven and Virus Total', 'EMG dataset in Lower Limb', 'wiki4HE', 'Horton General Hospital', 'StoneFlakes', 'GNFUV Unmanned Surface Vehicles Sensor Data', 'p53 Mutants', 'Online Video Characteristics and Transcoding Time Dataset', 'Person Classification Gait Data', 'Sentence Classification', 'Urban Land Cover', 'Labeled Text Forum Threads Dataset', 'MiniBooNE particle identification', 'UrbanGB, urban road accidents coordinates labelled by the urban center', 'Sponge', 'SCADI', 'YouTube Comedy Slam Preference Data', 'ICU', 'Low Resolution Spectrometer', 'LSVT Voice Rehabilitation', 'Vicon Physical Action Data Set', 'Devanagari Handwritten Character Dataset', 'HIV-1 protease cleavage', 'User Profiling and Abusive Language Detection Dataset', 'Record Linkage Comparison Patterns', 'Physical Unclonable Functions', 'Wave Energy Converters', 'Stock keeping units', 'Wilt', 'Indoor User Movement Prediction from RSS data', 'Activity Recognition from Single Chest-Mounted Accelerometer', 'REALDISP Activity Recognition Dataset', 'CLINC150', 'Early biomarkers of Parkinsonâ€™s disease based on natural connected speech Data Set', 'Gisette', 'QSAR oral toxicity', 'Activities of Daily Living (ADLs) Recognition Using Binary Sensors', 'IDA2016Challenge', 'GitHub MUSAE', 'Gastrointestinal Lesions in Regular Colonoscopy', 'DSRC Vehicle Communications', 'QSAR fish bioconcentration factor (BCF)', 'Dataset for ADL Recognition with Wrist-worn Accelerometer', \"ser Knowledge Modeling Data (Students' Knowledge Levels on DC Electrical Machines)\", 'MicroMass', 'Exasens', 'Reuters Transcribed Subset', 'Demospongiae', 'Repeat Consumption Matrices', 'microblogPCU', 'Turkish Spam V01', 'UJIIndoorLoc-Mag', 'Quadruped Mammals', 'OpinRank Review Dataset', 'Predict keywords activities in a online social media', 'Bach Choral Harmony', 'Relative location of CT slices on axial axis', 'OCT data & Color Fundus Images of Left & Right Eyes', 'Crowdsourced Mapping', 'Youtube cookery channels viewers comments in Hinglish', 'UJI Pen Characters', 'Dexter', 'Quality Assessment of Digital Colposcopies', 'LastFM Asia Social Network', 'Spoken Arabic Digit', 'Folio', 'PEMS-SF', 'MoCap Hand Postures', 'Z-Alizadeh Sani', 'Kinship', 'Shoulder Implant Manufacture Classification', 'DeliciousMIL: A Data Set for Multi-Label Multi-Instance Learning with Instance Labels', 'Hill-Valley', 'Dodgers Loop Sensor', 'ICMLA 2014 Accepted Papers Data Set', 'AAAI 2014 Accepted Papers', 'KDC-4007 dataset Collection', 'Russian Corpus of Biographical Texts', 'First-order theorem proving', 'Meta-data', 'chipseq', 'Victorian Era Authorship Attribution', 'SIFT10M', 'NYSK', 'AAAI 2013 Accepted Papers', 'Shoulder Implant X-Ray Manufacturer Classification', 'Miskolc IIS Hybrid IPS', 'Nasarian CAD Dataset', 'BLE RSSI dataset for Indoor localization', 'Burst Header Packet (BHP) flooding attack on Optical Burst Switching (OBS) Network', 'Roman Urdu Data Set', 'Mesotheliomaâ€™s disease data set', 'CalIt2 Building People Counts', 'Nomao', 'QSAR androgen receptor', 'TTC-3600: Benchmark dataset for Turkish text categorization', 'Pseudo Periodic Synthetic Time Series', 'USPTO Algorithm Challenge, run by NASA-Harvard Tournament Lab and TopCoder Problem: Pat', 'Mturk User-Perceived Clusters over Images', 'Coil 1999 Competition Data', 'PubChem Bioassay Data', 'Reuter_50_50', 'DBWorld e-mails', 'Newspaper and magazine images segmentation dataset', 'Online Handwritten Assamese Characters Dataset', 'MEU-Mobile KSD', 'Abscisic Acid Signaling Network', 'extention of Z-Alizadeh sani dataset', 'IIWA14-R820-Gazebo-Dataset-10Trajectories', 'AutoUniv', 'Discrete Tone Image Dataset', 'Badges', 'Northix', 'Firm-Teacher_Clave-Direction_Classification', 'NSF Research Award Abstracts 1990-2003', 'Opinion Corpus for Lebanese Arabic Reviews (OCLAR)', 'BAUM-1', 'Pioneer-1 Mobile Robot Data', 'MSNBC.com Anonymous Web Data', 'University of Tehran Question Dataset 2016 (UTQD.2016)', 'Document Understanding', 'PANDOR', 'Function Finding', 'UbiqLog (smartphone lifelogging)', 'Qualitative Structure Activity Relationships', 'M. Tuberculosis Genes', 'Prodigy', 'Connectionist Bench (Nettalk Corpus)', 'BAUM-2', 'Logic Theorist', 'QtyT40I10D100K', 'UJI Pen Characters (Version 2)', 'Opinosis Opinion &frasl; Review', 'Sattriya_Dance_Single_Hand_Gestures Dataset', 'Undocumented', 'PMU-UD', 'chestnut â€“ LARVIC', 'EBL Domain Theories', 'Moral Reasoner', 'DGP2 - The Second Data Generation Program']\n"
     ]
    }
   ],
   "source": [
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e402bcd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', '-', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Domain-Theory', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Domain-Theory', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Sequential, Time-Series, Text', 'Text', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', '-', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Sequential, Domain-Theory', 'Multivariate', 'Multivariate', 'Multivariate, Data-Generator', 'Multivariate', '-', 'Multivariate', 'Sequential, Domain-Theory', 'Multivariate', '-', 'Multivariate', '-', 'Multivariate', 'Multivariate', 'Multivariate, Sequential, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', '-', 'Multivariate', 'Multivariate', 'Image', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', '-', 'Multivariate', '-', '-', 'Multivariate', 'Multivariate', 'Image', 'Multivariate, Domain-Theory', 'Multivariate, Time-Series', 'Multivariate, Data-Generator', 'Multivariate', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate, Spatial', '-', 'Multivariate, Data-Generator', 'Multivariate, Data-Generator', '-', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Sequential', 'Multivariate', 'Other', 'Tabular, Multivariate', 'Multivariate', '-', 'Multivariate', 'Multivariate', 'Tabular, Multivariate, Other', 'Tabular, Multivariate, Time-Series', 'Multivariate, Text', 'Multivariate', 'Image', '-', 'Multivariate', 'Multivariate', 'Multivariate', 'Univariate', 'Transactional, Sequential', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate, Text, Domain-Theory', 'Multivariate, Time-Series', 'Multivariate', 'Time-Series', 'Multivariate', 'Multivariate', 'Tabular', '-', 'Multivariate, Time-Series', 'Multivariate, Text', '-', 'Other', 'Multivariate', '-', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Time-Series', '-', 'Multivariate, Sequential', 'Text', 'Multivariate, Time-Series', 'Multivariate, Time-Series', 'Multivariate', '-', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate, Time-Series', 'Time-Series', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Time-Series', 'Sequential, Multivariate, Time-Series', 'Image', 'Multivariate, Time-Series', 'Multivariate', 'Time-Series', 'Time-Series', '-', 'Image', 'Multivariate', 'Multivariate, Text', 'Multivariate', 'Multivariate', 'Tabular', 'Multivariate', 'Time-Series', 'Time-Series', '-', 'Image', 'Multivariate', 'Multivariate, Text', 'Multivariate', 'Multivariate', 'Tabular', 'Tabular', 'Multivariate', 'Multivariate', 'Multivariate, Univariate, Time-Series', 'Multivariate', 'Multivariate, Sequential, Time-Series', 'Multivariate', 'Time-Series', 'Multivariate, Univariate', 'Multivariate', 'Multivariate', 'Tabular, Sequential, Multivariate, Time-Series', 'Text', 'Multivariate', 'Multivariate', 'Multivariate', 'Time-Series', 'Multivariate', 'Multivariate', 'Time-Series, Domain-Theory', 'Multivariate', 'Tabular', 'Multivariate, Time-Series, Text', 'Text', 'Multivariate, Sequential, Time-Series', 'Time-Series', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate, Sequential, Time-Series', 'Text', 'Multivariate, Text', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Sequential', 'Multivariate', '-', 'Multivariate', '-', 'Multivariate, Text', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate, Sequential, Time-Series', 'Multivariate, Sequential, Time-Series', 'Time-Series', 'Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Image', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate, Sequential, Time-Series', 'Tabular', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Univariate, Domain-Theory', '-', '-', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate, Sequential, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Text', 'Multivariate, Time-Series', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate, Univariate', 'Multivariate', 'Multivariate', 'Text', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Univariate, Sequential, Time-Series', 'Text', 'Multivariate, Sequential', 'Multivariate, Text', 'Sequential, Time-Series', 'Multivariate', 'Tabular', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Sequential, Time-Series', 'Multivariate, Sequential', 'Multivariate', 'Multivariate', 'Multivariate', 'Tabular, Image', 'Multivariate, Text, Domain-Theory', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate, Time-Series', 'Multivariate, Time-Series', 'Tabular', 'Time-Series, Multivariate', 'Text', 'Multivariate, Time-Series', 'Multivariate', 'Univariate', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate, Text', 'Multivariate, Time-Series', 'Time-Series', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', 'Tabular', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Tabular', 'Univariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Tabular', 'Multivariate', 'Multivariate, Text', 'Text', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Tabular', 'Multivariate', 'Multivariate, Text', 'Text', 'Multivariate', 'Multivariate, Time-Series', 'Image', 'Multivariate', 'Time-Series', 'Multivariate', 'Multivariate', 'Sequential', 'Text', 'Multivariate, Time-Series', 'Multivariate', '-', 'Sequential', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Sequential', 'Multivariate, Text', 'Text', 'Multivariate, Time-Series', 'Multivariate, Time-Series', 'Multivariate, Time-Series', 'Multivariate, Time-Series, Text', 'Sequential, Text', '-', 'Multivariate', 'Multivariate', 'Multivariate, Time-Series', '-', '-', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Sequential, Time-Series', 'Text', 'Time-Series', 'Multivariate', 'Multivariate', 'Multivariate, Time-Series, Domain-Theory', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Univariate', 'Multivariate', 'Multivariate, Time-Series', 'Time-Series', 'Multivariate', '-', 'Multivariate', 'Multivariate, Time-Series', 'Time-Series', 'Text', 'Sequential, Time-Series', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate, Sequential, Time-Series, Text', 'Multivariate, Text', 'Multivariate', 'Multivariate, Sequential, Time-Series', 'Univariate', 'Multivariate, Sequential, Time-Series, Domain-Theory', 'Multivariate, Univariate, Text', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Tabular', 'Multivariate', 'Time-Series', 'Multivariate', 'Multivariate', 'Univariate, Sequential, Time-Series', 'Univariate, Sequential, Time-Series', 'Text', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Text', 'Multivariate', 'Multivariate, Sequential, Time-Series', 'Multivariate, Sequential', 'Time-Series', 'Tabular', 'Multivariate', 'Multivariate, Sequential', 'Multivariate, Sequential, Time-Series', 'Univariate', 'Multivariate', 'Time-Series', 'Multivariate, Sequential, Time-Series', '-', 'Multivariate', 'Text', 'Multivariate', 'Univariate, Domain-Theory', 'Multivariate', 'Multivariate, Univariate, Text', 'Sequential', 'Multivariate, Univariate', 'Multivariate, Text', 'Multivariate, Sequential, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', '-', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', '-', 'Multivariate', 'Multivariate', 'Multivariate', '-', 'Multivariate', 'Univariate', 'Time-Series', 'Multivariate', 'Multivariate', 'Text', 'Multivariate', 'Text', 'Multivariate', 'Univariate', 'Multivariate', 'Multivariate', 'Text', '-', 'Multivariate', 'Multivariate', 'Time-Series', '-', 'Multivariate', '-', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Sequential, Time-Series', 'Univariate, Sequential, Time-Series', 'Multivariate, Time-Series', 'Text', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Sequential, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Sequential, Text', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Text', 'Multivariate', 'Multivariate', 'Multivariate, Univariate, Sequential, Text', 'Text', 'Multivariate, Sequential, Time-Series', 'Multivariate, Data-Generator', '-', '-', 'Sequential', 'Domain-Theory', 'Multivariate', 'Multivariate', 'Multivariate, Text', 'Multivariate, Sequential', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', '-', 'Relational', 'Multivariate', 'Text', 'Sequential', '-', 'Multivariate', 'Multivariate', 'Multivariate, Text', 'Text', 'Multivariate', 'Multivariate', 'Sequential', 'Text', 'Multivariate', 'Multivariate, Sequential, Text', 'Multivariate', 'Multivariate', 'Text', 'Multivariate', 'Sequential, Time-Series', 'Text', 'Text', 'Multivariate', '-', 'Univariate', 'Multivariate', 'Text', '-', 'Domain-Theory', 'Multivariate, Text', '-', 'Multivariate', 'Multivariate, Text, Domain-Theory', 'Text', '-', 'Multivariate, Sequential', 'Multivariate', 'Multivariate', '-', '-', 'Multivariate', 'Multivariate', 'Univariate, Text', 'Multivariate, Univariate, Text', 'Multivariate', '-', 'Text', 'Time-Series', '-', '-', 'Text', '-', 'Multivariate', '-', 'Multivariate', '-', '-', '-', '-', 'Time-Series', '-', '-', 'Multivariate, Sequential', '-', 'Multivariate', '-', 'Univariate', '-', '-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "print(data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5d16cffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Classification', 'Classification', 'Classification', 'Classification', '-', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Regression', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Regression', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Regression', 'Classification, Regression, Clustering', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification, Regression', 'Classification', 'Classification, Regression', '-', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', '-', 'Classification', 'Classification', 'Classification', '-', 'Classification', 'Recommender-Systems', 'Regression, Description', 'Classification', 'Classification, Clustering', 'Classification', 'Classification, Clustering', 'Classification', 'Regression', 'Regression', 'Classification', 'Classification', '-', 'Regression', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification, Regression, Clustering', 'Classification', 'Classification', 'Classification', 'Clustering', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', '-', 'Regression', '-', '-', 'Classification', 'Classification', 'Other', 'Classification', 'Regression, Clustering', 'Classification', 'Classification, Regression, Clustering', 'Classification', 'Classification', 'Classification', '-', 'Classification', 'Classification', '-', 'Classification, Clustering', 'Classification, Clustering', 'Classification, Regression, Clustering', 'Classification', 'Classification', 'Classification', 'Classification', 'Regression', 'Classification, Other', 'Regression', 'Classification', 'Regression', 'Classification', 'Classification, Clustering', 'Classification', 'Classification, Regression, Clustering', 'Classification', 'Classification', '-', 'Regression', 'Classification', 'Classification', 'Regression', 'Recommender-Systems', 'Classification', 'Classification', 'Classification, Clustering', 'Classification', 'Regression', 'Classification', 'Classification', 'Classification', 'Classification', '-', 'Classification, Regression', 'Classification', '-', 'Regression', 'Classification, Regression', '-', 'Classification, Regression', 'Classification, Clustering', 'Classification, Clustering', 'Classification, Clustering', '-', 'Classification, Regression, Clustering', 'Other', 'Classification', 'Classification, Regression, Causal-Discovery', 'Regression', '-', 'Regression', 'Classification, Regression', 'Classification', 'Classification, Regression, Clustering', 'Regression', 'Clustering, Other', 'Classification', 'Classification, Regression', 'Regression', 'Regression', 'Classification', 'Regression', 'Classification', 'Classification', 'Classification', 'Classification, Regression, Clustering', 'Classification', 'Clustering', 'Classification, Regression', 'Regression, Clustering', 'Classification, Clustering, Other', 'Classification', 'Classification', 'Classification, Regression, Clustering', 'Classification, Clustering', 'Classification', 'Classification', 'Classification', 'Classification, Regression', 'Regression, Clustering', 'Classification, Clustering, Other', 'Classification', 'Classification', 'Classification, Regression, Clustering', 'Classification, Clustering', 'Regression', 'Regression', 'Classification, Regression, Clustering', 'Classification', 'Classification', 'Regression', 'Classification, Regression', 'Classification', 'Regression', 'Classification, Clustering', 'Classification, Clustering', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification, Clustering', 'Regression', 'Classification', 'Classification', 'Classification', 'Regression', 'Regression, Clustering, Causal-Discovery', 'Regression', 'Classification, Regression', 'Classification', 'Classification, Clustering', 'Classification, Regression, Causal-Discovery', 'Classification', 'Classification, Clustering', 'Classification', 'Classification, Clustering', 'Classification', 'Classification, Regression, Clustering, Causa', 'Classification', 'Classification, Regression, Clustering', 'Classification, Regression, Clustering', 'Classification, Regression', 'Classification', 'Classification, Regression, Clustering', 'Classification', 'Classification, Clustering', 'Regression', 'Classification', 'Classification, Regression', 'Classification', 'Classification, Clustering', 'Regression, Clustering', 'Classification', 'Classification, Regression, Clustering', 'Classification', 'Classification, Clustering, Causal-Discovery', 'Classification', 'Regression', 'Regression', 'Classification', 'Classification', 'Regression', 'Classification', 'Classification, Regression', 'Classification, Regression', 'Classification', 'Other', 'Classification, Clustering', 'Classification, Regression, Clustering', 'Classification', 'Classification, Clustering', '-', 'Classification, Regression', 'Classification', 'Classification, Regression', 'Clustering', 'Classification, Clustering', 'Classification, Regression, Clustering', 'Classification', 'Classification, Regression', 'Classification', 'Classification', 'Clustering', 'Classification, Regression', 'Classification', 'Classification, Clustering', 'Classification, Clustering', 'Classification, Clustering', 'Classification', 'Classification', 'Classification', 'Classification, Regression', 'Clustering', 'Classification, Regression, Clustering', 'Classification, Regression', 'Regression, Clustering', 'Classification', 'Classification, Regression', 'Regression', 'Regression', 'Classification', 'Regression', 'Regression', 'Classification', 'Classification, Regression', 'Classification', 'Clustering', 'Classification, Clustering', 'Classification', 'Classification', 'Regression', 'Classification, Regression', 'Classification', 'Classification, Regression', 'Regression', 'Regression, Classification', 'Clustering', 'Classification, Regression', 'Classification, Regression, Clustering', 'Classification', 'Classification', 'Regression', 'Regression', 'Classification', 'Classification, Clustering', 'Regression', 'Classification', 'Classification', 'Classification', 'Classification', 'Regression', 'Classification, Regression', 'Regression', 'Classification', 'Regression', 'Classification', 'Regression', 'Clustering', 'Classification', 'Classification', 'Regression', 'Classification', 'Clustering', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Regression', 'Regression', 'Classification', 'Classification, Clustering', 'Classification', 'Classification', 'Classification', 'Classification, Clustering', 'Classification, Clustering', 'Classification', 'Classification, Clustering', 'Classification, Causal-Discovery', 'Classification, Clustering', 'Classification', 'Regression', 'Classification', 'Classification, Clustering', 'Classification', 'Classification', 'Classification', 'Regression', 'Classification, Clustering', 'Classification', 'Clustering', 'Classification, Clustering', 'Classification, Regression', '-', 'Classification', 'Classification', 'Classification', 'Regression', 'Classification', 'Classification, Clustering', 'Classification, Clustering', 'Classification, Clustering', 'Classification', 'Regression', 'Classification', 'Regression', 'Regression, Clustering', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', '-', 'Classification', 'Classification', 'Classification', 'Classification, Regression', 'Classification, Regression', 'Classification', 'Classification, Regression', 'Classification', 'Classification', 'Regression', 'Classification, Regression', 'Classification', 'Classification, Regression', 'Classification', 'Classification', 'Classification, Clustering', 'Classification, Clustering', 'Classification', 'Classification, Regression', 'Classification, Regression', 'Classification', 'Regression', 'Classification, Clustering', 'Regression', 'Classification, Clustering', 'Classification', 'Classification', 'Classification, Clustering', '-', 'Regression', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification, Regression, Clustering', 'Classification, Regression', 'Regression', 'Classification', 'Classification', 'Classification, Regression, Clustering', 'Classification', 'Clustering, Causal-Discovery', 'Classification, Regression, Clustering', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification, Clustering', 'Classification, Clustering', 'Classification, Clustering', 'Classification', 'Classification, Clustering', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification, Regression, Clustering', 'Classification', 'Classification', 'Classification, Clustering', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification, Clustering', 'Classification, Clustering', 'Classification', 'Classification', 'Classification, Clustering', 'Classification', '-', 'Classification, Regression', 'Classification', 'Classification', 'Classification, Regression', 'Classification', 'Classification, Regression, Clustering', 'Classification', 'Classification', 'Classification', 'Regression', 'Classification, Regression', 'Regression', 'Classification', 'Classification', '-', 'Regression, Clustering, Causal-Discovery', 'Causal-Discovery', 'Classification, Clustering, Causal-Discovery', 'Regression', 'Classification', 'Classification, Clustering', 'Classification', 'Classification', 'Regression, Clustering', 'Classification', 'Causal-Discovery', '-', 'Classification', 'Classification', 'Classification, Clustering', 'Regression', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Clustering', 'Clustering', 'Classification, Clustering', 'Classification', '-', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Regression', 'Clustering', 'Classification', 'Classification', 'Classification, Clustering', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification, Clustering', 'Classification', 'Classification', 'Classification', 'Clustering', 'Regression', 'Classification, Clustering', 'Classification', 'Classification', 'Classification, Clustering', 'Classification', 'Classification', 'Clustering', 'Classification, Causal-Discovery', 'Classification', 'Classification, Regression, Clustering', 'Classification', '-', '-', 'Classification', 'Regression', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification, Clustering', 'Classification', 'Classification, Clustering', 'Classification', 'Relational-Learning', 'Classification', 'Classification', 'Classification', '-', 'Classification, Clustering', 'Clustering', 'Classification, Regression', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Causal-Discovery', 'Clustering', 'Clustering', 'Classification', 'Classification, Clustering, Causal-Discovery', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', '-', 'Classification', 'Classification', 'Classification, Clustering', '-', 'Classification', 'Clustering', '-', 'Classification', 'Classification, Clustering', 'Classification', 'Classification', 'Classification', 'Classification', 'Causal-Discovery', 'Classification', 'Regression', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', '-', 'Classification', 'Classification', '-', '-', 'Classification', '-', 'Recommendation', 'Function-Learning', 'Causal-Discovery', '-', '-', '-', '-', 'Classification', '-', '-', 'Classification', '-', 'Classification', '-', 'Classification', 'Classification, Clustering', '-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "print(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "782f38a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Real', 'Categorical, Integer, Real', 'Categorical, Integer', 'Integer, Real', 'Categorical, Integer', 'Real', 'Integer, Real', 'Categorical', 'Real', 'Categorical', 'Categorical, Integer, Real', 'Categorical', 'Real', 'Categorical, Integer', 'Categorical, Integer', 'Integer', 'Categorical, Real', 'Categorical, Real', 'Integer', 'Integer, Real', 'Categorical, Integer, Real', 'Categorical', 'Real', 'Categorical, Integer', 'Categorical, Integer', 'Integer', 'Categorical, Real', 'Integer, Real', 'Integer', 'Categorical, Integer, Real', 'Integer', 'Categorical', 'Real', 'Real', 'Categorical, Integer, Real', 'Real', 'Categorical', 'Integer', 'Integer, Real', 'Categorical', 'Real', 'Categorical', 'Categorical, Integer', 'Categorical, Integer, Real', 'Integer', 'Categorical', 'Real', '-', 'Categorical, Integer, Real', 'Categorical', 'Integer', 'Categorical', 'Categorical', 'Integer', 'Real', 'Real', 'Integer', 'Categorical, Integer, Real', 'Categorical', 'Categorical', 'Categorical, Real', '-', 'Real', 'Categorical', 'Categorical, Integer', 'Categorical, Integer', 'Integer, Real', 'Categorical, Integer', 'Integer, Real', 'Integer', 'Categorical', 'Real', 'Integer, Real', 'Categorical', '-', 'Categorical, Integer', 'Integer', 'Integer', 'Categorical, Integer', 'Categorical', 'Categorical', 'Categorical, Integer, Real', 'Categorical, Integer', 'Integer', 'Categorical', 'Categorical, Integer', 'Categorical, Integer, Real', 'Categorical', 'Categorical, Integer, Real', 'Categorical', 'Integer, Real', 'Categorical, Integer', 'Categorical', 'Categorical, Integer', 'Integer', 'Categorical, Integer, Real', 'Real', 'Categorical, Integer, Real', 'Categorical', '-', 'Categorical, Real, Integer', 'Real', 'Categorical, Integer', '-', 'Categorical', 'Categorical, Real', 'Categorical', 'Real', 'Categorical', 'Real', '-', '-', 'Categorical, Integer', 'Integer, Real', 'Integer, Real', 'Categorical, Integer', 'Categorical', 'Real', '-', 'Real, Categorical, Integer', '-', 'Real', 'Real', 'Categorical', 'Real, Integer', 'Real', 'Integer', 'Integer', 'Real', 'Integer, Real', 'Categorical, Integer', 'Categorical', 'Categorical, Integer', 'Integer, Real', 'Categorical', 'Integer', 'Real', 'Real', 'Real', 'Integer, Real', '-', '-', 'Categorical', '-', '-', 'Integer, Real', 'Categorical', 'Categorical, Integer, Real', '-', 'Integer, Real', 'Categorical, Integer', 'Integer, Real', 'Real', 'Integer', 'Integer, Real', '-', 'Integer, Real', '-', 'Real', 'Real', 'Integer, Real', '-', 'Real', 'Real', 'Real', 'Integer, Real', 'Real', '-', 'Real', 'Real', 'Integer, Real', 'Real', 'Real', 'Integer, Real', 'Real', 'Integer', '-', 'Real, Categorical, Integer', '-', 'Integer, Real', 'Integer', 'Real', '-', 'Integer', '-', 'Integer, Real', 'Real', 'Real', 'Categorical, Integer', '-', 'Integer', 'Integer, Real', '-', 'Integer, Real', 'Real', 'Real', '-', 'Integer, Real', 'Real', 'Integer', '-', 'Real', 'Integer', 'Real', 'Integer', 'Integer, Real', 'Integer, Real', 'Integer, Real', 'Integer', 'Integer, Real', 'Real', 'Real, Categorical', '-', 'Real', 'Real', '-', '-', 'Real', 'Real', '-', 'Integer, Real', '-', '-', '-', 'Real', 'Integer', 'Real', '-', 'Real', 'Real', 'Real', 'Integer, Real', '-', 'Integer', 'Real', '-', 'Integer, Real', 'Integer', 'Real', 'Real', 'Real', 'Real', 'Integer', 'Real', 'Real', 'Integer', 'Real', 'Real', 'Real', 'Integer, Real', 'Real', 'Real', '-', '-', 'Real', '-', 'Real', 'Real', 'Real', 'Real', 'Real', 'Real', 'Integer, Real', 'Integer', '-', 'Integer', 'Real', 'Integer, Real', 'Categorical', 'Integer, Real', 'Integer', 'Real', 'Integer, Real', 'Categorical, Integer', 'Real', 'Integer', 'Real', 'Real', '-', 'Integer', 'Real', 'Real', '-', 'Integer, Real', 'Integer, Real', 'Integer', 'Real', 'Integer', 'Integer', 'Integer', 'Real', 'Integer, Real', '-', 'Real', 'Real', 'Real', 'Real', 'Integer', 'Integer, Real', 'Integer, Real', 'Integer', '-', 'Real', 'Real', 'Real', 'Real', 'Integer', '-', 'Integer, Real', 'Integer', 'Real', 'Integer', 'Real', 'Integer', 'Real', 'Real', 'Real', 'Integer, Real', 'Real', 'Real', '-', 'Integer, Real', 'Integer', 'Real', 'Integer, Real', 'Real', '-', 'Integer, Real', 'Real', 'Real', 'Integer, Real', '-', 'Integer', 'Real', 'Real', 'Integer, Real', 'Real', 'Real', 'Real', 'Real', 'Real', 'Real', '-', 'Integer, Real', 'Integer', '-', 'Real', 'Categorical, Integer', 'Integer', '-', 'Real', 'Real', 'Real', 'Integer', 'Real', 'Integer, Real', 'Real', 'Integer, Real', '-', 'Real', 'Real', 'Real', '-', '-', 'Real', 'Real', 'Real', '-', 'Real', 'Integer, Real', 'Real', 'Real', 'Real', 'Real', 'Real', '-', 'Real', 'Real', 'Real', 'Integer, Real', 'Real', 'Real', 'Categorical, Integer', 'Real', 'Integer', 'Integer', '-', 'Real', 'Real', '-', 'Real', 'Integer, Real', '-', 'Integer, Real', 'Real', '-', 'Real', 'Integer', '-', 'Real', 'Real', 'Categorical, Integer, Real', 'Real', 'Real', 'Real', 'Real', 'Real', 'Integer, Real', 'Integer, Real', 'Real', 'Real', '-', 'Integer, Real', 'Real', '-', 'Categorical, Integer', 'Integer, Real', 'Real', 'Integer', '-', 'Integer, Real', 'Real', 'Integer', 'Real', 'Integer', 'Real', 'Integer, Real', 'Integer, Real', 'Real', 'Integer, Real', '-', 'Integer, Real', 'Integer', 'Integer', 'Real', 'Integer, Real', 'Real', 'Real', '-', 'Real', 'Real', '-', 'Integer', 'Real', 'Real', 'Real', '-', 'Integer, Real', 'Integer', 'Real', 'Real', '-', 'Real', 'Real', 'Real', 'Integer, Real', '-', 'Real', 'Real', '-', 'Real', '-', 'Real', 'Real', 'Real', 'Integer, Real', 'Integer, Real', 'Integer', 'Integer', 'Real', '-', 'Integer', 'Categorical, Integer, Real', 'Integer, Real', 'Real', '-', 'Integer', 'Real', 'Real', 'Real', 'Real', 'Integer', 'Real', 'Real', '-', 'Integer', '-', 'Real', 'Real', 'Real', 'Integer, Real', 'Real', 'Integer', '-', 'Integer', 'Real', 'Real', 'Categorical, Integer', '-', '-', 'Real', 'Integer, Real', 'Real', 'Real', 'Integer', 'Categorical', '-', 'Real', 'Integer', 'Real', 'Integer, Real', '-', 'Real', 'Real', 'Real', '-', 'Real', 'Integer', '-', '-', 'Integer', '-', 'Real', 'Real', 'Integer, Real', '-', 'Real', 'Real', 'Integer', '-', 'Integer', 'Real', 'Integer, Real', '-', 'Integer, Real', 'Real', '-', 'Integer, Real', '-', 'Real', 'Real', '-', '-', 'Integer', 'Integer', 'Real', '-', 'Real', '-', 'Real', 'Integer, Real', 'Integer, Real', 'Categorical', '-', 'Integer', 'Real', 'Categorical, Integer', '-', '-', 'Integer', '-', 'Real', 'Categorical, Integer, Real', 'Integer', '-', 'Integer', '-', '-', 'Real', 'Integer', '-', 'Integer', 'Integer', '-', 'Real', 'Categorical, Integer', 'Real', '-', 'Integer', '-', 'Integer', 'Integer', 'Categorical, Real', 'Integer, Real', 'Real', '-', '-', 'Integer', 'Integer, Real', 'Integer', 'Integer, Real', 'Integer', 'Categorical, Integer, Real', '-', '-', 'Integer, Real', '-', '-', 'Integer', '-', 'Categorical, Real', 'Categorical', '-', '-', 'Categorical', 'Real', '-', '-', '-', '-', 'Categorical', '-', '-', 'Integer', 'Integer', '-', '-', '-', '-', '-', '-', '-', 'Real']\n"
     ]
    }
   ],
   "source": [
    "print(attribute_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fe6bc34c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['150 Instances', '303 Instances', '48.84K Instances', '13.61K Instances', '-', '3.81K Instances', '178 Instances', '1.73K Instances', '569 Instances', '8.12K Instances', '4.18K Instances', '286 Instances', '214 Instances', '1K Instances', '48.84K Instances', '699 Instances', '7.2K Instances', '398 Instances', '5.62K Instances', '4.6K Instances', '205 Instances', '345 Instances', '101 Instances', '4.42K Instances', '4.9K Instances', '690 Instances', '32 Instances', '351 Instances', '10.99K Instances', '3.28K Instances', '20K Instances', '435 Instances', '2.31K Instances', '45.21K Instances', '155 Instances', '1.48K Instances', '958 Instances', '209 Instances', '1.07M Instances', '21.58K Instances', '208 Instances', '12.96K Instances', '581.01K Instances', '690 Instances', '649 Instances', '307 Instances', '198 Instances', '-', '452 Instances', '47 Instances', '6.44K Instances', '625 Instances', '106 Instances', '58K Instances', '7.8K Instances', '5K Instances', '946 Instances', '122 Instances', '432 Instances', '3.19K Instances', '270 Instances', '-', '336 Instances', '37.71K Instances', '9K Instances', '366 Instances', '541.91K Instances', '1.47K Instances', '12.33K Instances', '476 Instances', '1.39K Instances', '9.36K Instances', '30K Instances', '24 Instances', '10K Instances', '167 Instances', '306 Instances', '640 Instances', '194 Instances', '267 Instances', '3.2K Instances', '368 Instances', '4M Instances', '2.11K Instances', '339 Instances', '299.29K Instances', '132 Instances', '2.46M Instances', '798 Instances', '16 Instances', '2K Instances', '28.06K Instances', '148 Instances', '100 Instances', '23 Instances', '57 Instances', '1.02K Instances', '512 Instances', '226 Instances', '50K Instances', '125 Instances', '2.08M Instances', '-', '10K Instances', '226 Instances', '6.65K Instances', '67.56K Instances', '68.04K Instances', '-', '5K Instances', '-', '10.3K Instances', '101.77K Instances', '299 Instances', '5.47K Instances', '90 Instances', '128 Instances', '2.31K Instances', '705 Instances', '839 Instances', '2.1K Instances', '528 Instances', '517 Instances', '160 Instances', '338 Instances', '1.52M Instances', '215.06K Instances', '6.6K Instances', '9.11K Instances', '178.08K Instances', '191.78K Instances', '15 Instances', '108 Instances', '17.39K Instances', '50.67K Instances', '267 Instances', '2.57K Instances', '5.57K Instances', '2.26M Instances', '414 Instances', '14.22K Instances', '520 Instances', '10 Instances', '174 Instances', '-', '1.2K Instances', '332 Instances', '-', '2.1K Instances', '39.8K Instances', '256.93K Instances', '768 Instances', '210 Instances', '440 Instances', '740 Instances', '-', '165.47K Instances', '37.06K Instances', '6.46M Instances', '10K Instances', '8.76K Instances', '-', '1.03K Instances', '63M Instances', '19.02K Instances', '127 Instances', '19.74K Instances', '1.8K Instances', '1.89K Instances', '14.06M Instances', '43.82K Instances', '1.99K Instances', '197 Instances', '420.77K Instances', '1.37K Instances', '6.82K Instances', '1.8K Instances', '181.8K Instances', '14M Instances', '811 Instances', '3.15K Instances', '370 Instances', '268.88K Instances', '2.46K Instances', '70K Instances', '1K Instances', '980 Instances', '10.5K Instances', '1.03M Instances', '344 Instances', '60 Instances', '11.5K Instances', '1M Instances', '756 Instances', '2.7K Instances', '100 Instances', '171 Instances', '93.24K Instances', '9.57K Instances', '230.32K Instances', '29.33K Instances', '400 Instances', '500 Instances', '536 Instances', '116 Instances', '48.2K Instances', '615 Instances', '750 Instances', '170 Instances', '858 Instances', '1.7K Instances', '31.99K Instances', '3K Instances', '801 Instances', '315 Instances', '12.68K Instances', '7.6K Instances', '2.13K Instances', '557 Instances', '30K Instances', '5.88K Instances', '2.04K Instances', '7.11K Instances', '147.27K Instances', '4.76K Instances', '10K Instances', '9.12K Instances', '14 Instances', '106.57K Instances', '748 Instances', '13.91K Instances', '13.96M Instances', '8.01M Instances', '4.14K Instances', '244 Instances', '300 Instances', '1K Instances', '1.35K Instances', '7.06M Instances', '21.26K Instances', '11M Instances', '10K Instances', '704 Instances', '5.46K Instances', '36.73K Instances', '961 Instances', '45.78K Instances', '20.56K Instances', '27.17M Instances', '14.98K Instances', '521 Instances', '731 Instances', '102.94K Instances', '666 Instances', '372 Instances', '60K Instances', '12K Instances', '4.1M Instances', '2.54K Instances', '608 Instances', '43.93M Instances', '125 Instances', '583 Instances', '560 Instances', '-', '504 Instances', '10.93K Instances', '187 Instances', '22.63K Instances', '2.92M Instances', '-', '17.26K Instances', '1.04K Instances', '120 Instances', '310 Instances', '11.46K Instances', '2.21K Instances', '120 Instances', '13.03K Instances', '72 Instances', '1.08K Instances', '777 Instances', '1.96K Instances', '1.39K Instances', '130 Instances', '7.05K Instances', '35.72K Instances', '405 Instances', '141.71K Instances', '1K Instances', '1.99K Instances', '40.95K Instances', '182 Instances', '15.63M Instances', '2.22K Instances', '515.35K Instances', '314.08K Instances', '3.94K Instances', '1.15K Instances', '527 Instances', '403 Instances', '200 Instances', '1.5K Instances', '908 Instances', '4.18M Instances', '3.85M Instances', '9.2K Instances', '20.52K Instances', '140K Instances', '8M Instances', '14.06M Instances', '77 Instances', '245.06K Instances', '292 Instances', '2.92K Instances', '103 Instances', '48 Instances', '120K Instances', '9.36K Instances', '30K Instances', '65.53K Instances', '189 Instances', '120 Instances', '288K Instances', '135 Instances', '1.5K Instances', '90 Instances', '52.85K Instances', '329 Instances', '11.93K Instances', '18 Instances', '2.51K Instances', '80 Instances', '308 Instances', '340 Instances', '2.28K Instances', '7.84K Instances', '20K Instances', '919.44K Instances', '24.02K Instances', '4.48K Instances', '7.75K Instances', '181 Instances', '1.94K Instances', '590 Instances', '4.5K Instances', '2.55K Instances', '285 Instances', '70 Instances', '322 Instances', '106 Instances', '17.9K Instances', '1.57K Instances', '399 Instances', '800 Instances', '120K Instances', '2K Instances', '1.98K Instances', '-', '13.91K Instances', '5.74K Instances', '546 Instances', '5.82K Instances', '1.85K Instances', '58K Instances', '104 Instances', '1.06K Instances', '138 Instances', '75.13K Instances', '470 Instances', '314 Instances', '45.73K Instances', '540 Instances', '27.97K Instances', '249 Instances', '501 Instances', '325.83K Instances', '8.3M Instances', '640 Instances', '21.64K Instances', '434.87K Instances', '5M Instances', '151 Instances', '58.51K Instances', '463 Instances', '143 Instances', '1K Instances', '237 Instances', '4.4K Instances', '250 Instances', '180 Instances', '640 Instances', '22.47K Instances', '21.05K Instances', '42.24K Instances', '2.63K Instances', '731 Instances', '217 Instances', '240 Instances', '640 Instances', '39.24K Instances', '209 Instances', '88 Instances', '129.69K Instances', '900 Instances', '163 Instances', '58 Instances', '165 Instances', '60.02K Instances', '78.1K Instances', '10.72K Instances', '6.32K Instances', '2.4M Instances', '3K Instances', '422.94K Instances', '10.1K Instances', '3.4K Instances', '18K Instances', '3.06K Instances', '4.14K Instances', '74 Instances', '40 Instances', '107.89K Instances', '4.14K Instances', '1.08K Instances', '597 Instances', '153.54K Instances', '90 Instances', '1.71M Instances', '53.41K Instances', '7.62K Instances', '745K Instances', '1.95K Instances', '104 Instances', '444.63K Instances', '60K Instances', '600 Instances', '7.2K Instances', '232 Instances', '164.86K Instances', '-', '2K Instances', '1.59K Instances', '540 Instances', '2.58K Instances', '606 Instances', '26.74K Instances', '1.06K Instances', '6.61K Instances', '5.46K Instances', '10K Instances', '100 Instances', '10.5M Instances', '360 Instances', '9.9K Instances', '90 Instances', '326 Instances', '2.86K Instances', '1.54K Instances', '20K Instances', '216 Instances', '-', '20.87K Instances', '15 Instances', '373 Instances', '65.55K Instances', '165.63K Instances', '170 Instances', '5.88K Instances', '10.19K Instances', '779 Instances', '241.6K Instances', '6K Instances', '2.96K Instances', '132 Instances', '913 Instances', '139 Instances', '79 Instances', '1.67K Instances', '16.77K Instances', '6.26K Instances', '467 Instances', '1.6K Instances', '260K Instances', '100 Instances', '17.76M Instances', '401 Instances', '111.74K Instances', '182 Instances', '26.14K Instances', '168.29K Instances', '48 Instances', '-', '168 Instances', '200 Instances', '130.07K Instances', '360.18K Instances', '76 Instances', '70 Instances', '1.14M Instances', '-', '531 Instances', '126 Instances', '3K Instances', '92K Instances', '6.59K Instances', '65.92K Instances', '5.75M Instances', '6M Instances', '288K Instances', '2.28K Instances', '4.89K Instances', '13.2K Instances', '-', '1.42K Instances', '23.7K Instances', '-', '13.5K Instances', '8.99K Instances', '2.75K Instances', '76K Instances', '37.7K Instances', '76 Instances', '10K Instances', '1.06K Instances', '-', '403 Instances', '931 Instances', '399 Instances', '200 Instances', '503 Instances', '130K Instances', '221.58K Instances', '826 Instances', '40K Instances', '-', '-', '51 Instances', '5.67K Instances', '53.5K Instances', '50 Instances', '10.55K Instances', '9.8K Instances', '1.36K Instances', '2.6K Instances', '287 Instances', '7.62K Instances', '8.8K Instances', '637 Instances', '440 Instances', '78.1K Instances', '303 Instances', '104 Instances', '597 Instances', '12.23K Instances', '606 Instances', '50.4K Instances', '105 Instances', '399 Instances', '4.01K Instances', '200 Instances', '6.12K Instances', '528 Instances', '4.96K Instances', '93.6K Instances', '11.16M Instances', '10.42K Instances', '150 Instances', '597 Instances', '1.54K Instances', '150 Instances', '23.57K Instances', '1.08K Instances', '20K Instances', '324 Instances', '10.08K Instances', '34.47K Instances', '1.69K Instances', '3.6K Instances', '100K Instances', '306 Instances', '180 Instances', '340 Instances', '-', '2.5K Instances', '64 Instances', '101 Instances', '8.24K Instances', '2.86K Instances', '300 Instances', '303 Instances', '-', '-', '71 Instances', '294 Instances', '115 Instances', '10.8K Instances', '129K Instances', '3.92K Instances', '1.18K Instances', '-', '989.82K Instances', '1.18K Instances', '-', '-', '352 Instances', '9.78M Instances', '-', '-', '-', '20.01K Instances', '1.05K Instances', '-', '3.96M Instances', '11.64K Instances', '51 Instances', '1.45K Instances', '-', '5.18K Instances', '1.45K Instances', '-', '202 Instances', '-']\n"
     ]
    }
   ],
   "source": [
    "print(no_of_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6601008f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4 Attributes', '13 Attributes', '14 Attributes', '17 Attributes', '20 Attributes', '8 Attributes', '13 Attributes', '6 Attributes', '30 Attributes', '22 Attributes', '8 Attributes', '9 Attributes', '9 Attributes', '20 Attributes', '14 Attributes', '9 Attributes', '5 Attributes', '7 Attributes', '64 Attributes', '57 Attributes', '25 Attributes', '5 Attributes', '16 Attributes', '36 Attributes', '12 Attributes', '15 Attributes', '56 Attributes', '34 Attributes', '16 Attributes', '1.56K Attributes', '16 Attributes', '16 Attributes', '19 Attributes', '17 Attributes', '19 Attributes', '8 Attributes', '9 Attributes', '9 Attributes', '8 Attributes', '5 Attributes', '60 Attributes', '8 Attributes', '54 Attributes', '14 Attributes', '33 Attributes', '35 Attributes', '34 Attributes', '-', '279 Attributes', '35 Attributes', '36 Attributes', '4 Attributes', '58 Attributes', '8 Attributes', '617 Attributes', '21 Attributes', '18 Attributes', '4 Attributes', '6 Attributes', '61 Attributes', '13 Attributes', '-', '8 Attributes', '294 Attributes', '86 Attributes', '33 Attributes', '8 Attributes', '9 Attributes', '18 Attributes', '168 Attributes', '10 Attributes', '15 Attributes', '24 Attributes', '4 Attributes', '-', '4 Attributes', '3 Attributes', '-', '30 Attributes', '22 Attributes', '36 Attributes', '27 Attributes', '42 Attributes', '17 Attributes', '17 Attributes', '40 Attributes', '12 Attributes', '68 Attributes', '38 Attributes', '4 Attributes', '649 Attributes', '6 Attributes', '18 Attributes', '6 Attributes', '4 Attributes', '16 Attributes', '10 Attributes', '39 Attributes', '-', '-', '-', '9 Attributes', '22 Attributes', '22 Attributes', '69 Attributes', '15 Attributes', '42 Attributes', '89 Attributes', '7 Attributes', '40 Attributes', '-', '561 Attributes', '47 Attributes', '13 Attributes', '10 Attributes', '8 Attributes', '-', '19 Attributes', '-', '23 Attributes', '-', '10 Attributes', '13 Attributes', '5 Attributes', '4 Attributes', '15 Attributes', '6 Attributes', '168 Attributes', '-', '12 Attributes', '481 Attributes', '6 Attributes', '13 Attributes', '16 Attributes', '-', '44 Attributes', '22 Attributes', '-', '6 Attributes', '7 Attributes', '-', '17 Attributes', '32 Attributes', '451 Attributes', '-', '15 Attributes', '5 Attributes', '-', '-', '61 Attributes', '61 Attributes', '8 Attributes', '7 Attributes', '8 Attributes', '21 Attributes', '-', '14 Attributes', '-', '8 Attributes', '14 Attributes', '14 Attributes', '-', '9 Attributes', '12 Attributes', '11 Attributes', '42 Attributes', '29 Attributes', '237 Attributes', '32 Attributes', '15 Attributes', '13 Attributes', '128 Attributes', '23 Attributes', '18 Attributes', '5 Attributes', '96 Attributes', '-', '7 Attributes', '-', '53 Attributes', '13 Attributes', '140.26K Attributes', '-', '30 Attributes', '-', '21 Attributes', '11 Attributes', '64 Attributes', '11 Attributes', '-', '13 Attributes', '179 Attributes', '-', '754 Attributes', '3 Attributes', '10 Attributes', '1.2K Attributes', '11 Attributes', '4 Attributes', '13 Attributes', '86 Attributes', '25 Attributes', '19 Attributes', '8 Attributes', '10 Attributes', '9 Attributes', '14 Attributes', '16 Attributes', '54 Attributes', '36 Attributes', '124 Attributes', '8 Attributes', '-', '20.53K Attributes', '12 Attributes', '23 Attributes', '-', '23 Attributes', '5 Attributes', '20K Attributes', '26 Attributes', '7 Attributes', '280 Attributes', '4 Attributes', '14 Attributes', '7 Attributes', '5.63K Attributes', '7 Attributes', '518 Attributes', '5 Attributes', '129 Attributes', '37 Attributes', '-', '8 Attributes', '12 Attributes', '22 Attributes', '21 Attributes', '10 Attributes', '115 Attributes', '81 Attributes', '28 Attributes', '14 Attributes', '21 Attributes', '25 Attributes', '11 Attributes', '6 Attributes', '5 Attributes', '7 Attributes', '7 Attributes', '15 Attributes', '20 Attributes', '1.07K Attributes', '116 Attributes', '11 Attributes', '105 Attributes', '-', '3 Attributes', '20 Attributes', '73 Attributes', '8 Attributes', '16 Attributes', '8 Attributes', '10 Attributes', '2 Attributes', '-', '20 Attributes', '561 Attributes', '39 Attributes', '70 Attributes', '10 Attributes', '-', '55 Attributes', '26 Attributes', '6 Attributes', '6 Attributes', '5.81K Attributes', '43.68K Attributes', '23 Attributes', '69 Attributes', '19 Attributes', '82 Attributes', '18 Attributes', '5 Attributes', '29 Attributes', '65 Attributes', '12 Attributes', '4 Attributes', '10 Attributes', '36 Attributes', '59 Attributes', '84 Attributes', '54 Attributes', '4 Attributes', '6 Attributes', '147 Attributes', '90 Attributes', '-', '98 Attributes', '20 Attributes', '38 Attributes', '5 Attributes', '-', '10K Attributes', '7 Attributes', '19 Attributes', '52 Attributes', '16 Attributes', '1 Attributes', '77 Attributes', '100K Attributes', '3 Attributes', '7 Attributes', '4 Attributes', '21 Attributes', '5.23K Attributes', '10 Attributes', '321 Attributes', '1M Attributes', '15 Attributes', '6 Attributes', '12 Attributes', '23 Attributes', '-', '49 Attributes', '18 Attributes', '6 Attributes', '1.18K Attributes', '86 Attributes', '12 Attributes', '16 Attributes', '9 Attributes', '1 Attributes', '5 Attributes', '7 Attributes', '16 Attributes', '9 Attributes', '5 Attributes', '200K Attributes', '11 Attributes', '2.4K Attributes', '533 Attributes', '7 Attributes', '-', '27 Attributes', '8.27K Attributes', '-', '242 Attributes', '17 Attributes', '70 Attributes', '5 Attributes', '10 Attributes', '9 Attributes', '591 Attributes', '4 Attributes', '9 Attributes', '25 Attributes', '7 Attributes', '8 Attributes', '-', '128 Attributes', '561 Attributes', '9 Attributes', '33 Attributes', '2 Attributes', '25K Attributes', '1.66K Attributes', '68 Attributes', '47 Attributes', '9 Attributes', '17 Attributes', '15 Attributes', '9 Attributes', '18 Attributes', '100 Attributes', '7 Attributes', '13 Attributes', '175 Attributes', '11 Attributes', '12 Attributes', '29 Attributes', '4 Attributes', '18 Attributes', '5 Attributes', '49 Attributes', '90 Attributes', '7 Attributes', '-', '9 Attributes', '500 Attributes', '7 Attributes', '150K Attributes', '10 Attributes', '4.71K Attributes', '529 Attributes', '6 Attributes', '5 Attributes', '1.07K Attributes', '12 Attributes', '46 Attributes', '480K Attributes', '152 Attributes', '8 Attributes', '19 Attributes', '12 Attributes', '10K Attributes', '15 Attributes', '120.43K Attributes', '49 Attributes', '281 Attributes', '38 Attributes', '8 Attributes', '13 Attributes', '3.23M Attributes', '2.5K Attributes', '5 Attributes', '72 Attributes', '20 Attributes', '1.95M Attributes', '138 Attributes', '54.88K Attributes', '102 Attributes', '7 Attributes', '482 Attributes', '24 Attributes', '857 Attributes', '1 Attributes', '25 Attributes', '8 Attributes', '9 Attributes', '24 Attributes', '7.84K Attributes', '411 Attributes', '100K Attributes', '21 Attributes', '16 Attributes', '171 Attributes', '-', '22 Attributes', '16 Attributes', '8 Attributes', '-', '2 Attributes', '256 Attributes', '173 Attributes', '19 Attributes', '6.4K Attributes', '4 Attributes', '41 Attributes', '15 Attributes', '24 Attributes', '8 Attributes', '-', '28 Attributes', '91 Attributes', '50 Attributes', '7 Attributes', '27 Attributes', '3 Attributes', '65 Attributes', '-', '216 Attributes', '-', '10 Attributes', '3 Attributes', '513 Attributes', '29 Attributes', '18 Attributes', '54 Attributes', '-', '6 Attributes', '14 Attributes', '18 Attributes', '7 Attributes', '1.09K Attributes', '5 Attributes', '53 Attributes', '6 Attributes', '8 Attributes', '5 Attributes', '5.41K Attributes', '710 Attributes', '79 Attributes', '64 Attributes', '8 Attributes', '6 Attributes', '2.16M Attributes', '1 Attributes', '-', '13 Attributes', '6 Attributes', '11 Attributes', '321 Attributes', '-', '148 Attributes', '9 Attributes', '50 Attributes', '2 Attributes', '45 Attributes', '206 Attributes', '3 Attributes', '-', '102 Attributes', '309 Attributes', '27 Attributes', '-', '1 Attributes', '3 Attributes', '12 Attributes', '129 Attributes', '49 Attributes', '9 Attributes', '6 Attributes', '4 Attributes', '-', '120 Attributes', '-', '-', '5K Attributes', '1.02K Attributes', '-', '171 Attributes', '4.01K Attributes', '698 Attributes', '5 Attributes', '7 Attributes', '3 Attributes', '5 Attributes', '1.3K Attributes', '4 Attributes', '-', '-', '21K Attributes', '20 Attributes', '2 Attributes', '13 Attributes', '72 Attributes', '-', '35 Attributes', '17 Attributes', '386 Attributes', '2 Attributes', '29 Attributes', '3 Attributes', '-', '20K Attributes', '69 Attributes', '7.84K Attributes', '13 Attributes', '20 Attributes', '138.67K Attributes', '38 Attributes', '56 Attributes', '12 Attributes', '1 Attributes', '8.52K Attributes', '101 Attributes', '3 Attributes', '5 Attributes', '6 Attributes', '-', '2 Attributes', '51 Attributes', '22 Attributes', '-', '1K Attributes', '128 Attributes', '7 Attributes', '5 Attributes', '1 Attributes', '67 Attributes', '52 Attributes', '5 Attributes', '22 Attributes', '2 Attributes', '34 Attributes', '4 Attributes', '120 Attributes', '1.02K Attributes', '4.81K Attributes', '-', '5 Attributes', '500 Attributes', '17 Attributes', '-', '10K Attributes', '4.7K Attributes', '-', '-', '71 Attributes', '43 Attributes', '59 Attributes', '-', '-', '11 Attributes', '1 Attributes', '200 Attributes', '20 Attributes', '-', '3.92K Attributes', '-', '-', '-', '3 Attributes', '-', '-', '-', '-', '-', '-', '-', '4 Attributes', '-', '-', '4 Attributes', '-', '-', '-', '-', '9 Attributes', '3 Attributes', '-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "print(no_of_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "43d8c15b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7/1/1988', '7/1/1988', '5/1/1996', '9/14/2020', '-', '10/6/2019', '7/1/1991', '6/1/1997', '11/1/1995', '4/27/1987', '12/1/1995', '7/11/1988', '9/1/1987', '11/17/1994', '5/1/1996', '7/15/1992', '1/1/1987', '7/7/1993', '7/1/1998', '7/1/1999', '5/19/1987', '5/15/1990', '5/15/1990', '12/13/2021', '10/7/2009', '-', '5/1/1992', '1/1/1989', '7/1/1998', '7/1/1998', '1/1/1991', '4/27/1987', '11/1/1990', '2/14/2012', '11/1/1988', '9/1/1996', '8/19/1991', '10/1/1987', '9/21/2019', '9/26/1997', '-', '6/1/1997', '8/1/1998', '-', '11/27/2014', '7/11/1988', '12/1/1995', '10/1/1992', '1/1/1998', '1/1/1987', '2/13/1993', '4/22/1994', '6/30/1990', '-', '9/12/1994', '11/10/1988', '-', '10/13/1999', '10/1/1992', '1/1/1992', '-', '-', '9/1/1996', '11/1/1998', '7/3/2000', '1/1/1998', '11/6/2015', '7/7/1997', '8/31/2018', '9/12/1994', '3/1/1989', '3/23/2016', '1/26/2016', '8/1/1990', '7/7/1999', '5/1/1993', '3/4/1999', '6/24/1999', '5/15/1990', '10/1/2001', '8/1/1989', '8/6/1989', '1/1/1999', '8/27/2019', '11/1/1988', '3/7/2000', '2/28/1989', '-', '-', '-', '-', '6/1/1994', '11/1/1988', '-', '8/5/1993', '11/1/1988', '8/3/1989', '8/1/1995', '12/3/1987', '1/7/2023', '3/19/1992', '8/30/2012', '10/3/1988', '12/26/2019', '8/18/1992', '4/20/1999', '2/4/1995', '7/1/1999', '11/10/1988', '11/10/1988', '7/14/2001', '12/10/2012', '5/3/2014', '2/5/2020', '7/1/1995', '6/1/1993', '-', '11/1/1990', '5/15/2022', '12/14/2022', '6/18/2021', '-', '2/29/2008', '3/1/1989', '12/20/2022', '3/22/2023', '10/4/2018', '9/12/1994', '1/29/2023', '6/30/1999', '7/20/1998', '11/1/1988', '8/1/1990', '12/20/2013', '3/9/2000', '10/1/2001', '2/26/2002', '6/22/2012', '2/23/2023', '8/18/2018', '7/5/2021', '7/12/2020', '6/24/1994', '5/12/2022', '-', '8/3/2020', '10/20/1998', '7/15/1995', '5/15/2022', '5/31/2015', '11/9/1999', '11/30/2012', '9/29/2012', '3/31/2014', '4/5/2018', '-', '12/9/2019', '1/6/2023', '2/21/2023', '8/30/2020', '3/1/2020', '-', '8/3/2007', '9/14/2018', '5/1/2007', '6/1/2014', '2/15/2017', '7/3/2021', '10/17/2016', '2/24/2020', '1/19/2017', '7/13/2009', '6/26/2008', '9/20/2019', '4/16/2013', '6/28/2020', '6/20/2022', '12/15/2022', '11/27/2021', '7/16/2017', '4/9/2020', '3/13/2015', '5/8/2022', '3/26/2015', '10/17/2021', '6/20/2020', '12/19/2018', '4/11/2016', '1/1/2007', '11/15/2021', '11/21/2017', '5/24/2017', '5/16/2022', '11/5/2018', '11/10/2022', '1/17/2013', '5/5/2022', '2/20/2018', '3/26/2014', '9/24/2015', '4/25/2022', '7/3/2015', '8/5/2016', '6/1/2013', '3/6/2018', '5/7/2019', '6/10/2020', '10/23/2014', '7/24/2019', '3/3/2017', '12/9/2020', '11/25/2022', '5/30/2015', '6/9/2016', '4/22/2016', '9/15/2020', '6/20/2022', '9/7/2010', '4/21/2021', '9/13/2011', '10/29/2009', '3/1/2022', '6/3/2019', '3/15/2020', '7/4/2019', '7/2/2019', '7/8/2013', '4/24/2020', '5/24/2017', '10/3/2008', '10/23/2013', '9/20/2019', '12/3/2021', '10/2/2018', '10/22/2019', '9/16/2018', '11/29/2019', '11/2/2016', '3/19/2018', '10/12/2018', '2/12/2014', '11/16/2018', '12/24/2017', '12/19/2018', '11/29/2019', '10/29/2007', '12/22/2013', '2/29/2016', '10/16/2019', '6/10/2013', '2/17/2021', '4/20/2021', '8/14/2016', '12/10/2018', '2/19/2018', '11/25/2021', '7/27/2015', '4/15/2019', '4/21/2008', '11/22/2022', '10/26/2015', '7/22/2019', '5/21/2012', '7/22/2014', '2/1/1991', '7/23/2017', '7/29/2015', '4/21/2020', '12/31/2008', '6/17/2020', '12/21/2015', '4/12/2020', '6/12/2014', '2/11/2009', '8/9/2011', '11/23/2016', '4/26/2018', '12/7/2014', '10/3/2020', '7/17/2019', '8/4/2015', '7/14/2018', '3/26/2017', '9/30/2019', '2/15/2017', '4/22/2019', '1/2/2019', '10/23/2017', '7/14/2019', '4/9/2018', '12/26/2019', '3/11/2016', '3/9/2022', '10/6/2019', '3/2/2011', '2/7/2011', '6/6/2015', '11/3/2016', '11/3/2014', '6/1/1993', '6/26/2013', '10/28/2022', '6/11/2011', '9/23/2019', '3/20/2015', '8/6/2012', '7/13/2018', '11/26/2021', '5/27/2013', '3/12/2008', '2/24/2020', '7/20/2017', '7/17/2012', '12/24/2017', '4/16/2015', '4/30/2009', '10/14/2020', '10/16/2013', '7/22/2016', '1/7/2019', '2/4/2019', '7/17/2020', '4/14/2019', '6/30/2019', '12/12/2018', '3/4/2014', '5/5/2022', '7/18/2017', '3/4/2020', '9/11/2014', '10/7/2019', '11/27/2021', '11/2/2018', '1/3/2013', '2/24/2014', '4/10/2019', '11/30/2018', '3/11/2020', '7/15/2016', '6/16/2020', '12/4/2019', '2/18/2020', '9/28/2022', '10/26/2010', '12/24/2019', '4/24/2022', '6/9/2012', '7/1/1988', '8/6/2020', '3/20/2018', '5/10/2010', '2/14/2017', '11/19/2008', '4/22/2020', '9/3/2020', '12/4/2018', '12/4/2017', '8/15/2019', '-', '4/25/2012', '3/9/2016', '9/23/2019', '9/1/2013', '10/11/2019', '2/19/2018', '11/8/2019', '10/18/2014', '8/4/2012', '12/12/2016', '11/13/2013', '8/3/2020', '3/31/2013', '6/18/2013', '10/6/2014', '7/1/2018', '2/19/2014', '6/16/2020', '7/30/2019', '-', '12/5/2019', '4/16/2013', '2/12/2014', '6/7/1997', '2/24/2015', '4/23/1999', '5/24/2018', '1/1/1993', '3/7/2013', '2/29/2008', '2/9/2014', '10/10/2014', '3/29/2018', '7/22/2020', '9/18/2014', '5/18/2016', '6/5/2020', '4/20/2021', '10/11/2017', '4/10/2019', '5/19/2016', '11/24/2013', '6/1/1990', '1/29/2019', '3/27/2015', '2/29/2008', '2/29/2016', '9/10/2014', '11/29/2017', '5/29/2014', '1/27/2017', '4/5/2018', '3/10/2020', '10/15/2009', '11/18/2014', '2/28/2016', '6/30/1999', '10/22/2013', '6/5/2013', '6/6/2018', '10/18/2011', '6/29/2017', '3/5/2016', '11/29/2017', '1/9/2014', '8/3/2012', '5/20/2020', '1/10/2017', '1/4/2018', '7/11/2015', '11/28/2011', '7/31/2020', '8/14/2016', '2/29/2008', '12/24/2017', '5/28/2023', '12/8/2017', '6/8/1999', '2/24/2017', '1/10/2020', '11/3/2010', '3/2/2014', '4/11/2014', '11/11/2008', '1/13/2018', '4/3/2013', '5/26/2016', '8/25/2020', '6/21/2013', '1/25/2018', '8/4/2010', '7/27/2011', '10/28/2022', '1/28/2016', '8/17/2009', '6/18/2014', '1/4/2018', '5/25/2015', '8/20/2008', '12/18/2016', '9/9/1999', '1/3/2015', '10/19/2012', '6/20/2018', '1/1/2018', '3/3/2016', '11/28/2011', '4/9/2013', '7/24/2019', '6/1/2018', '9/13/2018', '10/11/2019', '2/27/2018', '7/1/1992', '1/31/2019', '2/5/2014', '5/4/2015', '11/13/2019', '5/20/2014', '5/6/2018', '2/9/2010', '9/20/2019', '4/27/2020', '12/3/2012', '6/22/2019', '7/6/2013', '5/16/2017', '7/23/2017', '9/6/2013', '7/17/2012', '6/15/2020', '5/19/2015', '3/2/2020', '11/5/2014', '3/27/2014', '4/8/2019', '12/13/2010', '11/22/2019', '-', '4/14/2018', '4/10/2012', '-', '3/1/1988', '2/19/2014', '7/27/2011', '9/1/2016', '4/25/2015', '4/25/2019', '3/10/2011', '10/8/2018', '6/30/2019', '4/10/2019', '3/13/2014', '2/4/2016', '3/2/2014', '7/25/2014', '5/8/2020', '7/23/2018', '2/29/2008', '10/1/2019', '10/28/2013', '1/17/2017', '10/7/2019', '10/15/2016', '12/13/2017', '11/27/2019', '2/11/2014', '6/20/2013', '8/12/2013', '4/22/2020', '3/8/2008', '1/21/2010', '3/22/2018', '3/17/2015', '4/7/2019', '9/10/2015', '8/25/1992', '7/26/2011', '12/12/2013', '5/20/2014', '7/7/2011', '11/1/2016', '5/25/2016', '7/3/2019', '6/1/2007', '2/29/2008', '3/8/2017', '8/22/2020', '9/13/2010', '7/5/2015', '5/22/2011', '11/22/2016', '11/17/2017', '7/1/1990', '5/15/2020', '10/27/2016', '3/20/2008', '12/1/2006', '2/19/2018', '7/30/2014', '4/27/2017', '6/3/2020', '4/17/2013', '3/1/1996', '2/21/2018', '5/31/2018', '2/23/2016', '10/11/2013', '7/30/2014', '5/20/2020', '7/4/2016', '2/28/2020', '5/29/2019', '8/28/2017', '8/29/2018', '1/11/2016', '12/1/2006', '7/4/2012', '10/1/2019', '2/8/2017', '2/8/1999', '10/13/2013', '11/2/2016', '9/9/1999', '3/29/2011', '9/8/2011', '11/6/2011', '7/15/2014', '4/1/2011', '5/14/2016', '4/3/2008', '11/17/2017', '6/9/2020', '11/3/2010', '1/20/2018', '9/1/1994', '8/15/2012', '4/24/2015', '11/18/2003', '6/17/2019', '11/9/2018', '1/28/1999', '-', '9/27/2017', '11/1/1994', '10/2/2018', '9/1/1990', '6/16/2016', '-', '7/14/2001', '-', '10/11/1954', '11/9/2018', '-', '10/21/2012', '1/22/2009', '7/6/2010', '7/22/2019', '-', '8/5/2018', '5/30/2017', '-', '6/1/1994', '-']\n"
     ]
    }
   ],
   "source": [
    "print(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ead844b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623 623 623 623 623 623 623\n"
     ]
    }
   ],
   "source": [
    "# Checking the lengths of all the lists\n",
    "print(len(dataset_name),len(data_type),len(task),len(attribute_type),len(no_of_instances),len(no_of_attributes),len(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7b496fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute Type</th>\n",
       "      <th>No. of Instances</th>\n",
       "      <th>No. of Attributes</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>150 Instances</td>\n",
       "      <td>4 Attributes</td>\n",
       "      <td>7/1/1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heart Disease</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>303 Instances</td>\n",
       "      <td>13 Attributes</td>\n",
       "      <td>7/1/1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>48.84K Instances</td>\n",
       "      <td>14 Attributes</td>\n",
       "      <td>5/1/1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dry Bean Dataset</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>13.61K Instances</td>\n",
       "      <td>17 Attributes</td>\n",
       "      <td>9/14/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>-</td>\n",
       "      <td>20 Attributes</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>PMU-UD</td>\n",
       "      <td>Univariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>-</td>\n",
       "      <td>5.18K Instances</td>\n",
       "      <td>9 Attributes</td>\n",
       "      <td>8/5/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>chestnut â€“ LARVIC</td>\n",
       "      <td>-</td>\n",
       "      <td>Classification, Clustering</td>\n",
       "      <td>-</td>\n",
       "      <td>1.45K Instances</td>\n",
       "      <td>3 Attributes</td>\n",
       "      <td>5/30/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>EBL Domain Theories</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Moral Reasoner</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>202 Instances</td>\n",
       "      <td>-</td>\n",
       "      <td>6/1/1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>DGP2 - The Second Data Generation Program</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>623 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Dataset Name     Data Type  \\\n",
       "0                                         Iris  Multivariate   \n",
       "1                                Heart Disease  Multivariate   \n",
       "2                                        Adult  Multivariate   \n",
       "3                             Dry Bean Dataset  Multivariate   \n",
       "4                                     Diabetes             -   \n",
       "..                                         ...           ...   \n",
       "618                                     PMU-UD    Univariate   \n",
       "619                        chestnut â€“ LARVIC             -   \n",
       "620                        EBL Domain Theories             -   \n",
       "621                             Moral Reasoner             -   \n",
       "622  DGP2 - The Second Data Generation Program             -   \n",
       "\n",
       "                           Task              Attribute Type  No. of Instances  \\\n",
       "0                Classification                        Real     150 Instances   \n",
       "1                Classification  Categorical, Integer, Real     303 Instances   \n",
       "2                Classification        Categorical, Integer  48.84K Instances   \n",
       "3                Classification               Integer, Real  13.61K Instances   \n",
       "4                             -        Categorical, Integer                 -   \n",
       "..                          ...                         ...               ...   \n",
       "618              Classification                           -   5.18K Instances   \n",
       "619  Classification, Clustering                           -   1.45K Instances   \n",
       "620                           -                           -                 -   \n",
       "621                           -                           -     202 Instances   \n",
       "622                           -                        Real                 -   \n",
       "\n",
       "    No. of Attributes       Year  \n",
       "0        4 Attributes   7/1/1988  \n",
       "1       13 Attributes   7/1/1988  \n",
       "2       14 Attributes   5/1/1996  \n",
       "3       17 Attributes  9/14/2020  \n",
       "4       20 Attributes          -  \n",
       "..                ...        ...  \n",
       "618      9 Attributes   8/5/2018  \n",
       "619      3 Attributes  5/30/2017  \n",
       "620                 -          -  \n",
       "621                 -   6/1/1994  \n",
       "622                 -          -  \n",
       "\n",
       "[623 rows x 7 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = {\n",
    "    'Dataset Name':dataset_name,\n",
    "    'Data Type':data_type,\n",
    "    'Task':task,\n",
    "    'Attribute Type':attribute_type,\n",
    "    'No. of Instances':no_of_instances,\n",
    "    'No. of Attributes':no_of_attributes,\n",
    "    'Year':year\n",
    "}\n",
    "\n",
    "# Creating dataframe\n",
    "df = pd.DataFrame(datasets)\n",
    "\n",
    "# displaying dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e313ee29",
   "metadata": {},
   "source": [
    "9. Program to scrape the data from Url = https://www.naukri.com/hr-recruiters-consultants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "81a3e14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sending a request to a website for scraping data\n",
    "page = requests.get(\"https://www.naukri.com/hr-recruiters-consultants \")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6691ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating WebDriver instance for controlling the Chrome browser using Selenium\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "78e52a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening webpage in the browser window controlled by the WebDriver instance\n",
    "driver.get(\"https://www.naukri.com/hr-recruiters-consultants \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7b8b1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for storing the details of the jobs\n",
    "job_title = []\n",
    "company_name = []\n",
    "job_description = []\n",
    "experience_required = []\n",
    "salary = []\n",
    "job_location =[]\n",
    "skills_required = []\n",
    "job_posted_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a44f7e22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Recruitment Opportunity For HR Executive', 'Human Resources Executive', 'Job opportunity For freshers/HR Recruiter/HR Executive', 'HR Executive || Nagpur || Third Party Payroll', 'HR Manager/ Assistant manager/Executive', 'HR Executive', 'HR Executive', 'Direct Walk in Interview For HR Executive(Female)', 'HR Executive', 'Jr HR Executive', 'Executive/ Assistant Manager HR Generalist - Pune ( Dress Code )', 'Opening For Management Trainee / Executive - HR', 'Recruiter - HR', 'Assistant Manager - HR (Field Level Recruitment)', 'Hiring Freshers : HR Executive: Recruiter-Gurugram : ACS', 'HR Trainee/Fresher(MBA)-Navi Mumbai(IMD joiners only)', 'HR Trainee/Fresher(MBA)-Navi Mumbai(IMD joiners only)-Seawoods', 'Hiring Hr recruiters, day shift, spot offer and immediate joining,', 'Human Resource Recruiter @Jabalpur: Salary Upto 2Lpa', 'Human Resource Recruiter @Jabalpur: Salary Upto 2Lpa']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the jobs title in job_title list\n",
    "job = driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "for i in job:\n",
    "    job_title.append(i.text)\n",
    "print(job_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b166a678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kone', 'Marriott', 'Black And White Business Solutions', 'Tata AIA Life Insurance', 'Nexus Hr Consultancy', 'Definedge Solutions', 'Bonami Software', 'm2 Vending', 'Reputed Group of Higher Education Insititions. Group Managing Colleges, Schools and Other Businesses', 'SPECTRAFORCE', 'OASIS', 'Sahajanand Medical Technologies', 'Fashion Tv India', 'Muthoot Microfin', 'Advance Career Solutions', '3i Infotech', '3i Infotech', 'Ignites Human Capital', 'KVC Consultants Ltd.', 'KVC Consultants Ltd.']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the companies name in company_name list\n",
    "company = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company:\n",
    "    company_name.append(i.text)\n",
    "print(company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "50a4b5e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Assist with all internal and external HR related inquiries or requests.Maintain both ha...', 'Ensure accurate maintenance of all employee records and files (eg, interview documents,...', 'LOOKING FOR - FRESHERS Experience : HR Recruiter Fresher (Work from Office) . Freshers ...', 'HIRING FOR HR RECRUITER (Male) Experience Required - 6 Months to 1 Year', 'Involved in the End-to-End recruitment Process, Payroll processing, Payroll software Ex...', 'Proven work experience as an HR Executive . Experience with full-cycle recruiting . Dem...', 'Qualifications . Must have excellent oral and written communication skills Should have ...', 'Direct walk in interview for HR Executive(Female) Walk-in-Interview for HR Executive (F...', 'Roles and ResponsibilityPrimary responsibilities will include liaising with existing em...', 'Willingness to work night shifts As a certified Minority Business Enterprise, SPECTRAFO...', 'Dress Code will be Blazer, Skirt and Bun for the hairs . Only Female Candidates product...', 'Following is the requirement : . Company - Sahajanand Medical Technologies Ltd. Assist ...', '1. Filling positions within the organization. 2. Designing and Implementing recruiting ...', '. Post Graduation - MBA / PGDM in HR Full Time. Sound Knowledge in MS Excel. Minimum 2-...', 'Currently Im hiring Enthusiastic Freshers for ACS (Advance Career Solutions Private Lim...', 'Urgent requirement for HR Trainee / Fresher(MBA-Must)-Navi Mumbai(Seawoods)Experience :...', 'Urgent requirement for HR Trainee / Fresher(MBA-Must)-Navi Mumbai(Seawoods)Experience :...', 'Recruiter(Day shift) Jayanagar . HR Recruiter / HR Analyst / Recruitment / Hiring / Sta...', 'Greetings, If you are looking forward to a career in the field of HR, We have got the o...', 'Greetings, If you are looking forward to a career in the field of HR, We have got the o...']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the jobs description in job_description list\n",
    "description = driver.find_elements(By.XPATH,\"//div[@class='ellipsis job-description']\")\n",
    "for i in description:\n",
    "    job_description.append(i.text)\n",
    "print(job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8ab48db7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0-2 Yrs', '0-3 Yrs', '0-1 Yrs', '0-2 Yrs', '0-5 Yrs', '0-3 Yrs', '0-1 Yrs', '30 Jun - 06 Jul', '0-3 Yrs', '0-2 Yrs', '0-3 Yrs', '0-2 Yrs', '0-3 Yrs', '0-2 Yrs', '0-1 Yrs', '0-2 Yrs', '0-2 Yrs', '0-2 Yrs', '0-5 Yrs', '0-5 Yrs']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the experience required for job in experienced_required list\n",
    "experience = driver.find_elements(By.XPATH,\"//span[@class='ellipsis fleft expwdth' or @class='ellipsis fleft walkDateWdth']\")\n",
    "for i in experience:\n",
    "    experience_required.append(i.text)\n",
    "print(experience_required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5f46dc30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.25-2 Lacs PA', 'Not disclosed', 'Not disclosed', '2-2.5 Lacs PA', '2.25-7.25 Lacs PA', 'Not disclosed', '4-5 Lacs PA', '1.25-2.25 Lacs PA', '2-3 Lacs PA', 'Not disclosed', '5-6 Lacs PA', 'Not disclosed', 'Not disclosed', '2.5-4 Lacs PA', '50,000-2.5 Lacs PA', 'Not disclosed', 'Not disclosed', '2-2.5 Lacs PA', '1-1.5 Lacs PA', '1-1.5 Lacs PA']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the salary details in salary list\n",
    "slry = driver.find_elements(By.XPATH,\"//span[@class='ellipsis fleft ']\")\n",
    "for i in slry:\n",
    "    salary.append(i.text)\n",
    "print(salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "264f7adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chennai, Tamil Nadu', 'Chennai', 'Bangalore/Bengaluru(Kodihalli +3)', 'Nagpur, Maharashtra', 'Nagpur, Pune, Aurangabad', 'Pune', 'New Delhi, Delhi / NCR', 'Gurgaon/ Gurugram, Haryana', 'Bangalore/Bengaluru(Bannerghatta +1)', 'Hybrid - Mohali, Punjab', 'Pune, Maharashtra(Koregaon Park)', 'Mumbai (All Areas)', 'Mumbai', 'Bhubaneswar, Odisha, Hubli, Karnataka, Sambalpur, Odisha, Ambala, Haryana, Belagavi/ Belgaum, Karnataka', 'Gurgaon/ Gurugram, Haryana', 'Navi Mumbai, Maharashtra', 'Navi Mumbai, Maharashtra', 'Bangalore/ Bengaluru, Karnataka', 'Jabalpur', 'Jabalpur']\n"
     ]
    }
   ],
   "source": [
    "# Fetching data and storing the jobs location in job_location list\n",
    "location = driver.find_elements(By.XPATH,\"//span[@class='ellipsis fleft locWdth' or @class='ellipsis fleft locWdth2']\")\n",
    "for i in location:\n",
    "    job_location.append(i.text)\n",
    "print(job_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "75613b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HR Coordination, hr generalist activities, Payroll, Payroll administration, Assistance, Coordination, Generalist Activities, Filing', 'Resource, Training, Management, Hrsd, Compliance, Monitoring, Employee relations, Payroll', 'Communication Skills, Bulk Hiring, Management Skills, Recruitment, Interpersonal Skills, Hiring, Leadership skills, Management', 'Human Resource, Hiring, Talent Acquisition, Third party, Resource, Acquisition, Hrsd, Payroll', 'Payroll Management, Joining Formalities, Exit Formalities, Payroll Software, HR Generalist Activities, PF, Bonus act, Statutory', 'Training, Recruitment, Development, Hrsd, Training, Payroll management, CVS, Payroll', 'hr, Talent Management, HR Generalist Activities, Campus Hiring, Generalist Activities, Management, Corporate HR, Human resource management', 'HR Generalist Activities, Joining formalities, Interviewing, Hrsd, Exit, Induction, Screening, Conducting', 'Human Resource Management, HR, HR Operations, MBA, Human Resource, Grievance handling, HR coordination, Generalist Activities', 'background verification, Attendance Maintenance, HR Administration, Vendor Management, communication skills, Leave administration, HR assistance, Onboarding', 'hr generalist activities, HR Information System, HR Coordination, HR Administration, HR Strategy, Generalist Activities, Corporate, Management', 'Recruitment, Talent Acquisition, Training, MIS, Hrsd, Engagement, Acquisition, Management', 'Hrsd, Social media, Networking, Media, Recruitment, HR Executive', 'NBFC, recruitment, Mass Hiring, Bulk Hiring, Life cycle, Talent acquisition, Bulk, Telephonic', 'communication skills, Recruitment, Hrsd, Ac, Acd, Hiring', 'Joining Formalities, HR, ms office, Recruitment, Training, Hrsd, Microsoft, Office', 'Joining Formalities, HR, ms office, Recruitment, Training, Hrsd, Microsoft, Office', 'HR, Hrsd, Hiring, Shortlisting, Source, Recruitment, Screening, Spot', 'hr, Inside sales, Telecalling, Hiring, Sales, Salary, Human resource management, English', 'hr, IT recruitment, Salary, Management, BPO Recruitment, Communication skills, Sales, Hrsd']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the skills required for job in skills_required list\n",
    "skills = driver.find_elements(By.XPATH,\"//ul[@class='tags has-description']\")\n",
    "for i in skills:\n",
    "    skills_required.append(i.text.replace('\\n',', '))\n",
    "print(skills_required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c00324cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 Day Ago', '1 Day Ago', '1 Day Ago', '4 Days Ago', '1 Day Ago', '1 Day Ago', '1 Day Ago', '1 Day Ago', 'Just Now', '1 Day Ago', '9 Days Ago', '4 Days Ago', '1 Day Ago', '7 Days Ago', '2 Days Ago', '1 Day Ago', '1 Day Ago', '1 Day Ago', '1 Day Ago', '1 Day Ago']\n"
     ]
    }
   ],
   "source": [
    "# Scraping and storing the jobs posted time in job_posted_time list\n",
    "posted_time = driver.find_elements(By.XPATH,\"//span[@class='fleft postedDate']\")\n",
    "for i in posted_time:\n",
    "    job_posted_time.append(i.text)\n",
    "print(job_posted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e4ed118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20 20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "# Checking the lengths of the lists\n",
    "print(len(job_title),len(company_name),len(job_description),len(experience_required),len(salary),len(job_location),len(skills_required),len(job_posted_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ffbd663d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Job Description</th>\n",
       "      <th>Experience Required</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Location</th>\n",
       "      <th>Skills Required</th>\n",
       "      <th>Posted Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recruitment Opportunity For HR Executive</td>\n",
       "      <td>Kone</td>\n",
       "      <td>Assist with all internal and external HR relat...</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>1.25-2 Lacs PA</td>\n",
       "      <td>Chennai, Tamil Nadu</td>\n",
       "      <td>HR Coordination, hr generalist activities, Pay...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Human Resources Executive</td>\n",
       "      <td>Marriott</td>\n",
       "      <td>Ensure accurate maintenance of all employee re...</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Resource, Training, Management, Hrsd, Complian...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Job opportunity For freshers/HR Recruiter/HR E...</td>\n",
       "      <td>Black And White Business Solutions</td>\n",
       "      <td>LOOKING FOR - FRESHERS Experience : HR Recruit...</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru(Kodihalli +3)</td>\n",
       "      <td>Communication Skills, Bulk Hiring, Management ...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HR Executive || Nagpur || Third Party Payroll</td>\n",
       "      <td>Tata AIA Life Insurance</td>\n",
       "      <td>HIRING FOR HR RECRUITER (Male) Experience Requ...</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>2-2.5 Lacs PA</td>\n",
       "      <td>Nagpur, Maharashtra</td>\n",
       "      <td>Human Resource, Hiring, Talent Acquisition, Th...</td>\n",
       "      <td>4 Days Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HR Manager/ Assistant manager/Executive</td>\n",
       "      <td>Nexus Hr Consultancy</td>\n",
       "      <td>Involved in the End-to-End recruitment Process...</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>2.25-7.25 Lacs PA</td>\n",
       "      <td>Nagpur, Pune, Aurangabad</td>\n",
       "      <td>Payroll Management, Joining Formalities, Exit ...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HR Executive</td>\n",
       "      <td>Definedge Solutions</td>\n",
       "      <td>Proven work experience as an HR Executive . Ex...</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Training, Recruitment, Development, Hrsd, Trai...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HR Executive</td>\n",
       "      <td>Bonami Software</td>\n",
       "      <td>Qualifications . Must have excellent oral and ...</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "      <td>4-5 Lacs PA</td>\n",
       "      <td>New Delhi, Delhi / NCR</td>\n",
       "      <td>hr, Talent Management, HR Generalist Activitie...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Direct Walk in Interview For HR Executive(Female)</td>\n",
       "      <td>m2 Vending</td>\n",
       "      <td>Direct walk in interview for HR Executive(Fema...</td>\n",
       "      <td>30 Jun - 06 Jul</td>\n",
       "      <td>1.25-2.25 Lacs PA</td>\n",
       "      <td>Gurgaon/ Gurugram, Haryana</td>\n",
       "      <td>HR Generalist Activities, Joining formalities,...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HR Executive</td>\n",
       "      <td>Reputed Group of Higher Education Insititions....</td>\n",
       "      <td>Roles and ResponsibilityPrimary responsibiliti...</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>2-3 Lacs PA</td>\n",
       "      <td>Bangalore/Bengaluru(Bannerghatta +1)</td>\n",
       "      <td>Human Resource Management, HR, HR Operations, ...</td>\n",
       "      <td>Just Now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jr HR Executive</td>\n",
       "      <td>SPECTRAFORCE</td>\n",
       "      <td>Willingness to work night shifts As a certifie...</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Hybrid - Mohali, Punjab</td>\n",
       "      <td>background verification, Attendance Maintenanc...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Executive/ Assistant Manager HR Generalist - P...</td>\n",
       "      <td>OASIS</td>\n",
       "      <td>Dress Code will be Blazer, Skirt and Bun for t...</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>5-6 Lacs PA</td>\n",
       "      <td>Pune, Maharashtra(Koregaon Park)</td>\n",
       "      <td>hr generalist activities, HR Information Syste...</td>\n",
       "      <td>9 Days Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Opening For Management Trainee / Executive - HR</td>\n",
       "      <td>Sahajanand Medical Technologies</td>\n",
       "      <td>Following is the requirement : . Company - Sah...</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Mumbai (All Areas)</td>\n",
       "      <td>Recruitment, Talent Acquisition, Training, MIS...</td>\n",
       "      <td>4 Days Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Recruiter - HR</td>\n",
       "      <td>Fashion Tv India</td>\n",
       "      <td>1. Filling positions within the organization. ...</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Hrsd, Social media, Networking, Media, Recruit...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Assistant Manager - HR (Field Level Recruitment)</td>\n",
       "      <td>Muthoot Microfin</td>\n",
       "      <td>. Post Graduation - MBA / PGDM in HR Full Time...</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>2.5-4 Lacs PA</td>\n",
       "      <td>Bhubaneswar, Odisha, Hubli, Karnataka, Sambalp...</td>\n",
       "      <td>NBFC, recruitment, Mass Hiring, Bulk Hiring, L...</td>\n",
       "      <td>7 Days Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hiring Freshers : HR Executive: Recruiter-Guru...</td>\n",
       "      <td>Advance Career Solutions</td>\n",
       "      <td>Currently Im hiring Enthusiastic Freshers for ...</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "      <td>50,000-2.5 Lacs PA</td>\n",
       "      <td>Gurgaon/ Gurugram, Haryana</td>\n",
       "      <td>communication skills, Recruitment, Hrsd, Ac, A...</td>\n",
       "      <td>2 Days Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HR Trainee/Fresher(MBA)-Navi Mumbai(IMD joiner...</td>\n",
       "      <td>3i Infotech</td>\n",
       "      <td>Urgent requirement for HR Trainee / Fresher(MB...</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Navi Mumbai, Maharashtra</td>\n",
       "      <td>Joining Formalities, HR, ms office, Recruitmen...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HR Trainee/Fresher(MBA)-Navi Mumbai(IMD joiner...</td>\n",
       "      <td>3i Infotech</td>\n",
       "      <td>Urgent requirement for HR Trainee / Fresher(MB...</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Navi Mumbai, Maharashtra</td>\n",
       "      <td>Joining Formalities, HR, ms office, Recruitmen...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hiring Hr recruiters, day shift, spot offer an...</td>\n",
       "      <td>Ignites Human Capital</td>\n",
       "      <td>Recruiter(Day shift) Jayanagar . HR Recruiter ...</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>2-2.5 Lacs PA</td>\n",
       "      <td>Bangalore/ Bengaluru, Karnataka</td>\n",
       "      <td>HR, Hrsd, Hiring, Shortlisting, Source, Recrui...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Human Resource Recruiter @Jabalpur: Salary Upt...</td>\n",
       "      <td>KVC Consultants Ltd.</td>\n",
       "      <td>Greetings, If you are looking forward to a car...</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>1-1.5 Lacs PA</td>\n",
       "      <td>Jabalpur</td>\n",
       "      <td>hr, Inside sales, Telecalling, Hiring, Sales, ...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Human Resource Recruiter @Jabalpur: Salary Upt...</td>\n",
       "      <td>KVC Consultants Ltd.</td>\n",
       "      <td>Greetings, If you are looking forward to a car...</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>1-1.5 Lacs PA</td>\n",
       "      <td>Jabalpur</td>\n",
       "      <td>hr, IT recruitment, Salary, Management, BPO Re...</td>\n",
       "      <td>1 Day Ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Job Title  \\\n",
       "0            Recruitment Opportunity For HR Executive   \n",
       "1                           Human Resources Executive   \n",
       "2   Job opportunity For freshers/HR Recruiter/HR E...   \n",
       "3       HR Executive || Nagpur || Third Party Payroll   \n",
       "4             HR Manager/ Assistant manager/Executive   \n",
       "5                                        HR Executive   \n",
       "6                                        HR Executive   \n",
       "7   Direct Walk in Interview For HR Executive(Female)   \n",
       "8                                        HR Executive   \n",
       "9                                     Jr HR Executive   \n",
       "10  Executive/ Assistant Manager HR Generalist - P...   \n",
       "11    Opening For Management Trainee / Executive - HR   \n",
       "12                                     Recruiter - HR   \n",
       "13   Assistant Manager - HR (Field Level Recruitment)   \n",
       "14  Hiring Freshers : HR Executive: Recruiter-Guru...   \n",
       "15  HR Trainee/Fresher(MBA)-Navi Mumbai(IMD joiner...   \n",
       "16  HR Trainee/Fresher(MBA)-Navi Mumbai(IMD joiner...   \n",
       "17  Hiring Hr recruiters, day shift, spot offer an...   \n",
       "18  Human Resource Recruiter @Jabalpur: Salary Upt...   \n",
       "19  Human Resource Recruiter @Jabalpur: Salary Upt...   \n",
       "\n",
       "                                         Company Name  \\\n",
       "0                                                Kone   \n",
       "1                                            Marriott   \n",
       "2                  Black And White Business Solutions   \n",
       "3                             Tata AIA Life Insurance   \n",
       "4                                Nexus Hr Consultancy   \n",
       "5                                 Definedge Solutions   \n",
       "6                                     Bonami Software   \n",
       "7                                          m2 Vending   \n",
       "8   Reputed Group of Higher Education Insititions....   \n",
       "9                                        SPECTRAFORCE   \n",
       "10                                              OASIS   \n",
       "11                    Sahajanand Medical Technologies   \n",
       "12                                   Fashion Tv India   \n",
       "13                                   Muthoot Microfin   \n",
       "14                           Advance Career Solutions   \n",
       "15                                        3i Infotech   \n",
       "16                                        3i Infotech   \n",
       "17                              Ignites Human Capital   \n",
       "18                               KVC Consultants Ltd.   \n",
       "19                               KVC Consultants Ltd.   \n",
       "\n",
       "                                      Job Description Experience Required  \\\n",
       "0   Assist with all internal and external HR relat...             0-2 Yrs   \n",
       "1   Ensure accurate maintenance of all employee re...             0-3 Yrs   \n",
       "2   LOOKING FOR - FRESHERS Experience : HR Recruit...             0-1 Yrs   \n",
       "3   HIRING FOR HR RECRUITER (Male) Experience Requ...             0-2 Yrs   \n",
       "4   Involved in the End-to-End recruitment Process...             0-5 Yrs   \n",
       "5   Proven work experience as an HR Executive . Ex...             0-3 Yrs   \n",
       "6   Qualifications . Must have excellent oral and ...             0-1 Yrs   \n",
       "7   Direct walk in interview for HR Executive(Fema...     30 Jun - 06 Jul   \n",
       "8   Roles and ResponsibilityPrimary responsibiliti...             0-3 Yrs   \n",
       "9   Willingness to work night shifts As a certifie...             0-2 Yrs   \n",
       "10  Dress Code will be Blazer, Skirt and Bun for t...             0-3 Yrs   \n",
       "11  Following is the requirement : . Company - Sah...             0-2 Yrs   \n",
       "12  1. Filling positions within the organization. ...             0-3 Yrs   \n",
       "13  . Post Graduation - MBA / PGDM in HR Full Time...             0-2 Yrs   \n",
       "14  Currently Im hiring Enthusiastic Freshers for ...             0-1 Yrs   \n",
       "15  Urgent requirement for HR Trainee / Fresher(MB...             0-2 Yrs   \n",
       "16  Urgent requirement for HR Trainee / Fresher(MB...             0-2 Yrs   \n",
       "17  Recruiter(Day shift) Jayanagar . HR Recruiter ...             0-2 Yrs   \n",
       "18  Greetings, If you are looking forward to a car...             0-5 Yrs   \n",
       "19  Greetings, If you are looking forward to a car...             0-5 Yrs   \n",
       "\n",
       "                Salary                                           Location  \\\n",
       "0       1.25-2 Lacs PA                                Chennai, Tamil Nadu   \n",
       "1        Not disclosed                                            Chennai   \n",
       "2        Not disclosed                  Bangalore/Bengaluru(Kodihalli +3)   \n",
       "3        2-2.5 Lacs PA                                Nagpur, Maharashtra   \n",
       "4    2.25-7.25 Lacs PA                           Nagpur, Pune, Aurangabad   \n",
       "5        Not disclosed                                               Pune   \n",
       "6          4-5 Lacs PA                             New Delhi, Delhi / NCR   \n",
       "7    1.25-2.25 Lacs PA                         Gurgaon/ Gurugram, Haryana   \n",
       "8          2-3 Lacs PA               Bangalore/Bengaluru(Bannerghatta +1)   \n",
       "9        Not disclosed                            Hybrid - Mohali, Punjab   \n",
       "10         5-6 Lacs PA                   Pune, Maharashtra(Koregaon Park)   \n",
       "11       Not disclosed                                 Mumbai (All Areas)   \n",
       "12       Not disclosed                                             Mumbai   \n",
       "13       2.5-4 Lacs PA  Bhubaneswar, Odisha, Hubli, Karnataka, Sambalp...   \n",
       "14  50,000-2.5 Lacs PA                         Gurgaon/ Gurugram, Haryana   \n",
       "15       Not disclosed                           Navi Mumbai, Maharashtra   \n",
       "16       Not disclosed                           Navi Mumbai, Maharashtra   \n",
       "17       2-2.5 Lacs PA                    Bangalore/ Bengaluru, Karnataka   \n",
       "18       1-1.5 Lacs PA                                           Jabalpur   \n",
       "19       1-1.5 Lacs PA                                           Jabalpur   \n",
       "\n",
       "                                      Skills Required Posted Time  \n",
       "0   HR Coordination, hr generalist activities, Pay...   1 Day Ago  \n",
       "1   Resource, Training, Management, Hrsd, Complian...   1 Day Ago  \n",
       "2   Communication Skills, Bulk Hiring, Management ...   1 Day Ago  \n",
       "3   Human Resource, Hiring, Talent Acquisition, Th...  4 Days Ago  \n",
       "4   Payroll Management, Joining Formalities, Exit ...   1 Day Ago  \n",
       "5   Training, Recruitment, Development, Hrsd, Trai...   1 Day Ago  \n",
       "6   hr, Talent Management, HR Generalist Activitie...   1 Day Ago  \n",
       "7   HR Generalist Activities, Joining formalities,...   1 Day Ago  \n",
       "8   Human Resource Management, HR, HR Operations, ...    Just Now  \n",
       "9   background verification, Attendance Maintenanc...   1 Day Ago  \n",
       "10  hr generalist activities, HR Information Syste...  9 Days Ago  \n",
       "11  Recruitment, Talent Acquisition, Training, MIS...  4 Days Ago  \n",
       "12  Hrsd, Social media, Networking, Media, Recruit...   1 Day Ago  \n",
       "13  NBFC, recruitment, Mass Hiring, Bulk Hiring, L...  7 Days Ago  \n",
       "14  communication skills, Recruitment, Hrsd, Ac, A...  2 Days Ago  \n",
       "15  Joining Formalities, HR, ms office, Recruitmen...   1 Day Ago  \n",
       "16  Joining Formalities, HR, ms office, Recruitmen...   1 Day Ago  \n",
       "17  HR, Hrsd, Hiring, Shortlisting, Source, Recrui...   1 Day Ago  \n",
       "18  hr, Inside sales, Telecalling, Hiring, Sales, ...   1 Day Ago  \n",
       "19  hr, IT recruitment, Salary, Management, BPO Re...   1 Day Ago  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_details = {\n",
    "    'Job Title':job_title,\n",
    "    'Company Name':company_name,\n",
    "    'Job Description':job_description,\n",
    "    'Experience Required':experience_required,\n",
    "    'Salary':salary,'Location':job_location,\n",
    "    'Skills Required':skills_required,\n",
    "    'Posted Time':job_posted_time\n",
    "}\n",
    "\n",
    "# Creating dataframe\n",
    "df = pd.DataFrame(jobs_details)\n",
    "\n",
    "# Displaying dataframe\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
