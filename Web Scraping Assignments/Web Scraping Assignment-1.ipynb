{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92fd859",
   "metadata": {},
   "source": [
    "# Web Scraping Assignment-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d4c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403ac37",
   "metadata": {},
   "source": [
    "1) Python program to display all the header tags from wikipedia.org with data frame of scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "083e2103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_tags(url):\n",
    "    \n",
    "    # Sending a request to a website for scraping data\n",
    "    page=requests.get(url)\n",
    "    page\n",
    "\n",
    "    # Extracting content of the website \n",
    "    soup=BeautifulSoup(page.content)\n",
    "    soup\n",
    "    \n",
    "    # Finding all the header tags\n",
    "    header_tag = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    header_tag\n",
    "\n",
    "    # Extracting the text from the header tags\n",
    "    header_tags=[]\n",
    "    header_texts=[]\n",
    "    for i in header_tag:\n",
    "        header_tags.append(i.name)\n",
    "        header_tags\n",
    "        header_texts.append(i.text.strip())\n",
    "        header_texts\n",
    "        \n",
    "    # Creating a DataFrame\n",
    "    df=pd.DataFrame({'Header Tags':header_tags, 'Header Texts':header_texts})\n",
    "\n",
    "    # Displaying the DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad1e375d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Header Tags</th>\n",
       "      <th>Header Texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h1</td>\n",
       "      <td>Main Page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h1</td>\n",
       "      <td>Welcome to Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>h2</td>\n",
       "      <td>From today's featured article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h2</td>\n",
       "      <td>Did you know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>h2</td>\n",
       "      <td>In the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>h2</td>\n",
       "      <td>On this day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>h2</td>\n",
       "      <td>Today's featured picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>h2</td>\n",
       "      <td>Other areas of Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>h2</td>\n",
       "      <td>Wikipedia's sister projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>h2</td>\n",
       "      <td>Wikipedia languages</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Header Tags                   Header Texts\n",
       "0          h1                      Main Page\n",
       "1          h1           Welcome to Wikipedia\n",
       "2          h2  From today's featured article\n",
       "3          h2               Did you know ...\n",
       "4          h2                    In the news\n",
       "5          h2                    On this day\n",
       "6          h2       Today's featured picture\n",
       "7          h2       Other areas of Wikipedia\n",
       "8          h2    Wikipedia's sister projects\n",
       "9          h2            Wikipedia languages"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_tags('https://en.wikipedia.org/wiki/Main_Page')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4092a",
   "metadata": {},
   "source": [
    "2) Python program to display list of respected former presidents of India(i.e. Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm with data frame of scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "872429eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def former_presidents(url):\n",
    "    \n",
    "    # Sending a request to a website for scraping data\n",
    "    page=requests.get(url)\n",
    "    page\n",
    "\n",
    "    # Extracting content of the website\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    soup\n",
    "    \n",
    "    # Extracting all former presidents names\n",
    "    president_name=[]\n",
    "    for i in soup.find_all('div',class_='presidentListing'):\n",
    "        president_name.append(i.text.split('\\n')[1].split('(')[0])\n",
    "    president_name\n",
    "    \n",
    "    # Extracting former presidents term of office details\n",
    "    term_of_office=[]\n",
    "    for i in soup.find_all('p',class_=''):\n",
    "        term_of_office.append(i.text.split(':')[1])\n",
    "    term_of_office\n",
    "    del term_of_office[1:8:2]\n",
    "    term_of_office\n",
    "    \n",
    "    # Creating DataFrame\n",
    "    df=pd.DataFrame({'President Name':president_name,'Term of Office':term_of_office})\n",
    "\n",
    "    # Displaying the DataFrame\n",
    "    print(\"List of respected former Presidents of India-\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9ee7d78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of respected former Presidents of India-\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>President Name</th>\n",
       "      <th>Term of Office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind</td>\n",
       "      <td>25 July, 2017 to 25 July, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee</td>\n",
       "      <td>25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil</td>\n",
       "      <td>25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam</td>\n",
       "      <td>25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan</td>\n",
       "      <td>25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma</td>\n",
       "      <td>25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman</td>\n",
       "      <td>25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh</td>\n",
       "      <td>25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy</td>\n",
       "      <td>25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed</td>\n",
       "      <td>24 August, 1974 to 11 February, 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri</td>\n",
       "      <td>3 May, 1969 to 20 July, 1969 and 24 August, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain</td>\n",
       "      <td>13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan</td>\n",
       "      <td>13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad</td>\n",
       "      <td>26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   President Name  \\\n",
       "0           Shri Ram Nath Kovind    \n",
       "1          Shri Pranab Mukherjee    \n",
       "2   Smt Pratibha Devisingh Patil    \n",
       "3         DR. A.P.J. Abdul Kalam    \n",
       "4           Shri K. R. Narayanan    \n",
       "5        Dr Shankar Dayal Sharma    \n",
       "6            Shri R Venkataraman    \n",
       "7               Giani Zail Singh    \n",
       "8      Shri Neelam Sanjiva Reddy    \n",
       "9       Dr. Fakhruddin Ali Ahmed    \n",
       "10  Shri Varahagiri Venkata Giri    \n",
       "11              Dr. Zakir Husain    \n",
       "12  Dr. Sarvepalli Radhakrishnan    \n",
       "13           Dr. Rajendra Prasad    \n",
       "\n",
       "                                       Term of Office  \n",
       "0                     25 July, 2017 to 25 July, 2022   \n",
       "1                     25 July, 2012 to 25 July, 2017   \n",
       "2                     25 July, 2007 to 25 July, 2012   \n",
       "3                     25 July, 2002 to 25 July, 2007   \n",
       "4                     25 July, 1997 to 25 July, 2002   \n",
       "5                     25 July, 1992 to 25 July, 1997   \n",
       "6                     25 July, 1987 to 25 July, 1992   \n",
       "7                     25 July, 1982 to 25 July, 1987   \n",
       "8                     25 July, 1977 to 25 July, 1982   \n",
       "9                24 August, 1974 to 11 February, 1977  \n",
       "10   3 May, 1969 to 20 July, 1969 and 24 August, 1...  \n",
       "11                        13 May, 1967 to 3 May, 1969  \n",
       "12                       13 May, 1962 to 13 May, 1967  \n",
       "13                   26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "former_presidents('https://presidentofindia.nic.in/former-presidents.htm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05662574",
   "metadata": {},
   "source": [
    "3) Python program to scrape cricket rankings from icc-cricket.com. with data frame of scraped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa39d29",
   "metadata": {},
   "source": [
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a60086e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_ODI_teams_men(url):\n",
    "    \n",
    "    # Sending a request to a website for scraping data\n",
    "    page=requests.get(url)\n",
    "    page\n",
    "\n",
    "    # Extracting content of the website\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    soup\n",
    "    \n",
    "    # Extrating data of top 10 ODI teams in men's cricket\n",
    "    ODI_team=[]\n",
    "    for i in soup.find_all('span',class_='u-hide-phablet'):\n",
    "        ODI_team.append(i.text)\n",
    "    ODI_team\n",
    "    ODI_team[0:10]\n",
    "    \n",
    "    # Extracting data of matches played by the teams\n",
    "    match=soup.find('td',class_='rankings-block__banner--matches')\n",
    "    match.text\n",
    "    matches=[]\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-center-text'):\n",
    "        matches.append(i.text)\n",
    "    matches\n",
    "    del matches[1::2]\n",
    "    matches\n",
    "    matches.insert(0,match.text)        \n",
    "    matches[0:10]\n",
    "    \n",
    "    # Extracting data of points obtained by the teams\n",
    "    point=soup.find('td',class_='rankings-block__banner--points')\n",
    "    point.text\n",
    "    points=[]\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-center-text'):\n",
    "        points.append(i.text)\n",
    "    points\n",
    "    del points[0::2]\n",
    "    points\n",
    "    points.insert(0,point.text)\n",
    "    points[0:10]\n",
    "    \n",
    "    # Extracting data of ratings obtained by the teams\n",
    "    rating=soup.find('td',class_='rankings-block__banner--rating u-text-right')\n",
    "    rating.text\n",
    "    ratings=[]\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-text-right rating'):\n",
    "        ratings.append(i.text)\n",
    "    ratings\n",
    "    ratings.insert(0,118)\n",
    "    ratings[0:10]\n",
    "    \n",
    "    # Creating DataFrame\n",
    "    df=pd.DataFrame({'Team':ODI_team[0:10],'Matches':matches[0:10],'Points':points[0:10],'Rating':ratings[0:10]},index=range(1,11))\n",
    "\n",
    "    # Displaying DataFrame\n",
    "    print(\"Top 10 ODI teams in men’s cricket\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32d9e992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in men’s cricket\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australia</td>\n",
       "      <td>23</td>\n",
       "      <td>2,714</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>20</td>\n",
       "      <td>2,316</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>33</td>\n",
       "      <td>3,807</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>27</td>\n",
       "      <td>2,806</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>England</td>\n",
       "      <td>24</td>\n",
       "      <td>2,426</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>19</td>\n",
       "      <td>1,910</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>25</td>\n",
       "      <td>2,451</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>10</td>\n",
       "      <td>878</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>21</td>\n",
       "      <td>1,682</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>25</td>\n",
       "      <td>1,797</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Team Matches Points Rating\n",
       "1      Australia      23  2,714    118\n",
       "2       Pakistan      20  2,316    116\n",
       "3          India      33  3,807    115\n",
       "4    New Zealand      27  2,806    104\n",
       "5        England      24  2,426    101\n",
       "6   South Africa      19  1,910    101\n",
       "7     Bangladesh      25  2,451     98\n",
       "8    Afghanistan      10    878     88\n",
       "9      Sri Lanka      21  1,682     80\n",
       "10   West Indies      25  1,797     72"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_ODI_teams_men('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d2ad76",
   "metadata": {},
   "source": [
    "b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89636fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_ODI_batsmen(url):\n",
    "    \n",
    "    # Sending a request to a website for scraping data\n",
    "    page=requests.get(url)\n",
    "    page\n",
    "\n",
    "    # Extracting content of the website\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    soup\n",
    "    \n",
    "    # Extracting data of the top 10 batsmen in ODI cricket\n",
    "    player=[]\n",
    "    for i in soup.find_all('div',class_='rankings-block__banner--name'):\n",
    "        player.append(i.text)\n",
    "    player\n",
    "    player[0]\n",
    "    players=[]\n",
    "    for i in soup.find_all('a',class_=''):\n",
    "        players.append(i.text)\n",
    "    players\n",
    "    players.insert(0,player[0])\n",
    "    players[0:10]\n",
    "    \n",
    "    # Extracting data of respective teams of the top 10 batsmen in ODI cricket\n",
    "    team=[]\n",
    "    for i in soup.find_all('div',class_='rankings-block__banner--nationality'):\n",
    "        team.append(i.text.split('\\n\\n')[1].split('\\n')[0])\n",
    "    team\n",
    "    team[0]\n",
    "    teams=[]\n",
    "    for i in soup.find_all('span',class_='table-body__logo-text'):\n",
    "        teams.append(i.text)\n",
    "    teams\n",
    "    teams.insert(0,team[0])\n",
    "    teams[0:10]\n",
    "    \n",
    "    # Extracting data of rating obtained by the top 10 batsmen in ODI cricket\n",
    "    rating=[]\n",
    "    for i in soup.find_all('div',class_='rankings-block__banner--rating'):\n",
    "        rating.append(i.text)\n",
    "    rating[0]\n",
    "    ratings=[]\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-text-right rating'):\n",
    "        ratings.append(i.text)\n",
    "    ratings\n",
    "    ratings.insert(0,rating[0])\n",
    "    ratings[0:10]\n",
    "    \n",
    "    # Crating DataFrame\n",
    "    df=pd.DataFrame({'Player':players[0:10],'Team':teams[0:10],'Rating':ratings[0:10]},index=range(1,11))\n",
    "\n",
    "    # Displaying DataFrame\n",
    "    print(\"Top 10 ODI batsmen in men’s cricket\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac2a07bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI batsmen in men’s cricket\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rassie van der Dussen</td>\n",
       "      <td>SA</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fakhar Zaman</td>\n",
       "      <td>PAK</td>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Imam-ul-Haq</td>\n",
       "      <td>PAK</td>\n",
       "      <td>745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shubman Gill</td>\n",
       "      <td>IND</td>\n",
       "      <td>738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Harry Tector</td>\n",
       "      <td>IRE</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Player Team Rating\n",
       "1              Babar Azam  PAK    886\n",
       "2   Rassie van der Dussen   SA    777\n",
       "3            Fakhar Zaman  PAK    755\n",
       "4             Imam-ul-Haq  PAK    745\n",
       "5            Shubman Gill  IND    738\n",
       "6            David Warner  AUS    726\n",
       "7            Harry Tector  IRE    722\n",
       "8             Virat Kohli  IND    719\n",
       "9         Quinton de Kock   SA    718\n",
       "10           Rohit Sharma  IND    707"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_ODI_batsmen('https://www.icc-cricket.com/rankings/mens/player-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44a0be",
   "metadata": {},
   "source": [
    "c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13d937dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_ODI_bowlers_men(url):\n",
    "    \n",
    "    # Sending a request to a website for scraping data\n",
    "    page=requests.get(url)\n",
    "    page\n",
    "\n",
    "    # Extracting content of the website\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    soup\n",
    "    \n",
    "    # Extracting data of the top 10 bowlers in ODI cricket\n",
    "    player=[]\n",
    "    for i in soup.find_all('div',class_='rankings-block__banner--name'):\n",
    "        player.append(i.text)\n",
    "    player\n",
    "    player[1]\n",
    "    players=[]\n",
    "    for i in soup.find_all('a',class_=''):\n",
    "        players.append(i.text)\n",
    "    players\n",
    "    players.insert(9,player[1])\n",
    "    players[9:19]\n",
    "    \n",
    "    # Extracting data of respective teams of the top 10 bowlers in ODI cricket\n",
    "    team=[]\n",
    "    for i in soup.find_all('div',class_='rankings-block__banner--nationality'):\n",
    "        team.append(i.text.split('\\n\\n')[1].split('\\n')[0])\n",
    "    team\n",
    "    team[1]\n",
    "    teams=[]\n",
    "    for i in soup.find_all('span',class_='table-body__logo-text'):\n",
    "        teams.append(i.text)\n",
    "    teams\n",
    "    teams.insert(9,team[1])\n",
    "    teams[9:19]\n",
    "    \n",
    "    # Extracting data of rating obtained by the top 10 batsmen in ODI cricket\n",
    "    rating=[]\n",
    "    for i in soup.find_all('div',class_='rankings-block__banner--rating'):\n",
    "        rating.append(i.text)\n",
    "    rating[1]\n",
    "    ratings=[]\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-text-right rating'):\n",
    "        ratings.append(i.text)\n",
    "    ratings\n",
    "    ratings.insert(9,rating[1])\n",
    "    ratings[9:19]\n",
    "    \n",
    "    # Creating dataframe\n",
    "    df=pd.DataFrame({'Player':players[9:19],'Team':teams[9:19],'Rating':ratings[9:19]},index=range(1,11))\n",
    "\n",
    "    # Diaplaying dataframe\n",
    "    print(\"Top 10 ODI bowlers in men’s cricket\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa8746b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI bowlers in men’s cricket\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mohammed Siraj</td>\n",
       "      <td>IND</td>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mitchell Starc</td>\n",
       "      <td>AUS</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Matt Henry</td>\n",
       "      <td>NZ</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rashid Khan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Adam Zampa</td>\n",
       "      <td>AUS</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mohammad Nabi</td>\n",
       "      <td>AFG</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shaheen Afridi</td>\n",
       "      <td>PAK</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Player Team Rating\n",
       "1     Josh Hazlewood  AUS    705\n",
       "2     Mohammed Siraj  IND    691\n",
       "3     Mitchell Starc  AUS    686\n",
       "4         Matt Henry   NZ    667\n",
       "5        Trent Boult   NZ    660\n",
       "6        Rashid Khan  AFG    659\n",
       "7         Adam Zampa  AUS    652\n",
       "8   Mujeeb Ur Rahman  AFG    637\n",
       "9      Mohammad Nabi  AFG    631\n",
       "10    Shaheen Afridi  PAK    630"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_ODI_bowlers_men('https://www.icc-cricket.com/rankings/mens/player-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4aa6db",
   "metadata": {},
   "source": [
    "4) Python program to scrape cricket rankings from icc-cricket.com. with data frame of scraped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93306cca",
   "metadata": {},
   "source": [
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1edbccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_ODI_teams_women(url):\n",
    "    \n",
    "    # Sending a request to a website for scraping data\n",
    "    page=requests.get(url)\n",
    "    page\n",
    "\n",
    "    # Extracting content of the website\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    soup\n",
    "    \n",
    "    # Extrating data of top 10 ODI teams in women's cricket\n",
    "    ODI_team=[]\n",
    "    for i in soup.find_all('span',class_='u-hide-phablet'):\n",
    "        ODI_team.append(i.text)\n",
    "    ODI_team\n",
    "    ODI_team[0:10]\n",
    "    \n",
    "    # Extracting data of matches played by the teams\n",
    "    match=soup.find('td',class_='rankings-block__banner--matches')\n",
    "    match.text\n",
    "    matches=[]\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-center-text'):\n",
    "        matches.append(i.text)\n",
    "    matches\n",
    "    del matches[1::2]\n",
    "    matches\n",
    "    matches.insert(0,match.text)\n",
    "    matches[0:10]\n",
    "    \n",
    "    # Extracting data of points obtained by the teams\n",
    "    point=soup.find('td',class_='rankings-block__banner--points')\n",
    "    point.text\n",
    "    points=[]\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-center-text'):\n",
    "        points.append(i.text)\n",
    "    points\n",
    "    del points[0::2]\n",
    "    points\n",
    "    points.insert(0,point.text)\n",
    "    points[0:10]\n",
    "    \n",
    "    # Extracting data of ratings obtained by the teams\n",
    "    rating=soup.find('td',class_='rankings-block__banner--rating u-text-right')\n",
    "    rating.text\n",
    "    ratings=[]\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-text-right rating'):\n",
    "        ratings.append(i.text)\n",
    "    ratings\n",
    "    ratings.insert(0,172)\n",
    "    ratings[0:10]\n",
    "    \n",
    "    # Creating DataFrame\n",
    "    df=pd.DataFrame({'Team':ODI_team[0:10],'Matches':matches[0:10],'Points':points[0:10],'Rating':ratings[0:10]},index=range(1,11))\n",
    "\n",
    "    # Displaying DataFrame\n",
    "    print(\"Top 10 ODI teams in women’s cricket\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52d8f84a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in women’s cricket\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australia</td>\n",
       "      <td>21</td>\n",
       "      <td>3,603</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>England</td>\n",
       "      <td>28</td>\n",
       "      <td>3,342</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>26</td>\n",
       "      <td>3,098</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>India</td>\n",
       "      <td>27</td>\n",
       "      <td>2,820</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>25</td>\n",
       "      <td>2,553</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>27</td>\n",
       "      <td>2,535</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>11</td>\n",
       "      <td>821</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>14</td>\n",
       "      <td>977</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>27</td>\n",
       "      <td>1,678</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>9</td>\n",
       "      <td>479</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Team Matches Points Rating\n",
       "1      Australia      21  3,603    172\n",
       "2        England      28  3,342    119\n",
       "3   South Africa      26  3,098    119\n",
       "4          India      27  2,820    104\n",
       "5    New Zealand      25  2,553    102\n",
       "6    West Indies      27  2,535     94\n",
       "7       Thailand      11    821     75\n",
       "8     Bangladesh      14    977     70\n",
       "9       Pakistan      27  1,678     62\n",
       "10     Sri Lanka       9    479     53"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_ODI_teams_women('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf6c1c8",
   "metadata": {},
   "source": [
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdd9b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_ODI_batters_women(url):\n",
    "   \n",
    "    # Sending a request to a website for scraping data\n",
    "    page=requests.get(url)\n",
    "    page\n",
    "\n",
    "    # Extracting content of the website\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    soup\n",
    "    \n",
    "    # Extracting data of the top 10 women’s ODI Batting players\n",
    "    player=[]\n",
    "    for i in soup.find_all('div',class_='rankings-block__banner--name'):\n",
    "        player.append(i.text)\n",
    "    player\n",
    "    player[0]\n",
    "    players=[]\n",
    "    for i in soup.find_all('a',class_=''):\n",
    "        players.append(i.text)\n",
    "    players\n",
    "    players.insert(0,player[0])\n",
    "    players[0:10]\n",
    "    \n",
    "    # Extracting data of respective teams of the top 10 women’s ODI Batting players\n",
    "    team=[]\n",
    "    for i in soup.find_all('div',class_='rankings-block__banner--nationality'):\n",
    "        team.append(i.text.split('\\n\\n')[1].split('\\n')[0])\n",
    "    team\n",
    "    team[0]\n",
    "    teams=[]\n",
    "    for i in soup.find_all('span',class_='table-body__logo-text'):\n",
    "        teams.append(i.text)\n",
    "    teams\n",
    "    teams.insert(0,team[0])\n",
    "    teams[0:10]\n",
    "    \n",
    "    # Extracting data of rating obtained by the top 10 women’s ODI Batting players\n",
    "    rating=[]\n",
    "    for i in soup.find_all('div',class_='rankings-block__banner--rating'):\n",
    "        rating.append(i.text)\n",
    "    rating[0]\n",
    "    ratings=[]\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-text-right rating'):\n",
    "        ratings.append(i.text)\n",
    "    ratings\n",
    "    ratings.insert(0,rating[0])\n",
    "    ratings[0:10]\n",
    "    \n",
    "    # Creating DataFrame\n",
    "    df=pd.DataFrame({'Player':players[0:10],'Team':teams[0:10],'Rating':ratings[0:10]},index=range(1,11))\n",
    "\n",
    "    # Displaying DataFrame\n",
    "    print(\"Top 10 women’s ODI Batting players\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c11e056b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 women’s ODI Batting players\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Harmanpreet Kaur</td>\n",
       "      <td>IND</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chamari Athapaththu</td>\n",
       "      <td>SL</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tammy Beaumont</td>\n",
       "      <td>ENG</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Stafanie Taylor</td>\n",
       "      <td>WI</td>\n",
       "      <td>588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Player Team Rating\n",
       "1           Beth Mooney  AUS    754\n",
       "2       Laura Wolvaardt   SA    732\n",
       "3        Natalie Sciver  ENG    731\n",
       "4           Meg Lanning  AUS    717\n",
       "5      Harmanpreet Kaur  IND    716\n",
       "6       Smriti Mandhana  IND    714\n",
       "7   Chamari Athapaththu   SL    673\n",
       "8          Ellyse Perry  AUS    626\n",
       "9        Tammy Beaumont  ENG    595\n",
       "10      Stafanie Taylor   WI    588"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_ODI_batters_women('https://www.icc-cricket.com/rankings/womens/player-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25520355",
   "metadata": {},
   "source": [
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81edc19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_ODI_all_rounders_women(url):\n",
    "    \n",
    "    # Sending a request to a website for scraping data\n",
    "    page=requests.get(url)\n",
    "    page\n",
    "\n",
    "    # Extracting content of the website\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    soup\n",
    "    \n",
    "    # Extracting data of the top 10 women’s ODI all-rounder\n",
    "    player=[]\n",
    "    for i in soup.find_all('div',class_='rankings-block__banner--name'):\n",
    "        player.append(i.text)\n",
    "    player\n",
    "    player[2]\n",
    "    players=[]\n",
    "    for i in soup.find_all('a',class_=''):\n",
    "        players.append(i.text)\n",
    "    players\n",
    "    players.insert(18,player[2])\n",
    "    players[18:28]\n",
    "    \n",
    "    # Extracting data of respective teams of the top 10 women’s ODI all-rounder\n",
    "    team=[]\n",
    "    for i in soup.find_all('div',class_='rankings-block__banner--nationality'):\n",
    "        team.append(i.text.split('\\n\\n')[1].split('\\n')[0])\n",
    "    team\n",
    "    team[2]\n",
    "    teams=[]\n",
    "    for i in soup.find_all('span',class_='table-body__logo-text'):\n",
    "        teams.append(i.text)\n",
    "    teams\n",
    "    teams.insert(18,team[0])\n",
    "    teams[18:28]\n",
    "    \n",
    "    # Extracting data of rating obtained by the top 10 women’s ODI all-rounder\n",
    "    rating=[]\n",
    "    for i in soup.find_all('div',class_='rankings-block__banner--rating'):\n",
    "        rating.append(i.text)\n",
    "    rating[2]\n",
    "    ratings=[]\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-text-right rating'):\n",
    "        ratings.append(i.text)\n",
    "    ratings\n",
    "    ratings.insert(18,rating[2])\n",
    "    ratings[18:28]\n",
    "    \n",
    "    # Creating DataFrame\n",
    "    df=pd.DataFrame({'Player':players[18:28],'Team':teams[18:28],'Rating':ratings[18:28]},index=range(1,11))\n",
    "\n",
    "    # Displaying DataFrame\n",
    "    print(\"Top 10 women’s ODI all rounders\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56e796e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 women’s ODI all rounders\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>AUS</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amelia Kerr</td>\n",
       "      <td>NZ</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ashleigh Gardner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nida Dar</td>\n",
       "      <td>PAK</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sophie Ecclestone</td>\n",
       "      <td>ENG</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Player Team Rating\n",
       "1     Hayley Matthews  AUS    373\n",
       "2      Natalie Sciver  ENG    371\n",
       "3        Ellyse Perry  AUS    366\n",
       "4      Marizanne Kapp   SA    349\n",
       "5         Amelia Kerr   NZ    336\n",
       "6       Deepti Sharma  IND    322\n",
       "7    Ashleigh Gardner  AUS    292\n",
       "8       Jess Jonassen  AUS    250\n",
       "9            Nida Dar  PAK    232\n",
       "10  Sophie Ecclestone  ENG    205"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_ODI_all_rounders_women('https://www.icc-cricket.com/rankings/womens/player-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bcae94",
   "metadata": {},
   "source": [
    "5) Python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world with data frame of scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b73312",
   "metadata": {},
   "source": [
    "i) Headline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57976c5",
   "metadata": {},
   "source": [
    "ii) Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c91156",
   "metadata": {},
   "source": [
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03c772ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def news(url):\n",
    "\n",
    "    # Sending a request to a website for scraping data\n",
    "    page=requests.get(url)\n",
    "    page\n",
    "\n",
    "    # Extracting content of the website\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    soup\n",
    "    \n",
    "    # i) Headline\n",
    "    \n",
    "    # Extracting headlines data\n",
    "    headlines=[]\n",
    "    for i in soup.find_all('a',class_='LatestNews-headline'):\n",
    "        headlines.append(i.text)\n",
    "    headlines\n",
    "    \n",
    "    # ii) Time\n",
    "    \n",
    "    # Extracting time data\n",
    "    time=[]\n",
    "    for i in soup.find_all('time',class_='LatestNews-timestamp'):\n",
    "        time.append(i.text)\n",
    "    time\n",
    "    \n",
    "    # iii) News Link\n",
    "    \n",
    "    # Extracting news link data\n",
    "    news_link=[]\n",
    "    for i in soup.find_all('a',class_='LatestNews-headline'):\n",
    "        news_link.append(i.get('href'))\n",
    "    news_link\n",
    "    \n",
    "    # Creating DataFrame\n",
    "    df=pd.DataFrame({'Headlines':headlines,'Time':time,'News Link':news_link})\n",
    "\n",
    "    # Displaying DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5328d2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Time</th>\n",
       "      <th>News Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Turkey votes in runoff election after candidat...</td>\n",
       "      <td>57 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/05/28/turkey-electio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>White House and Republicans reach a tentative ...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/05/27/white-house-an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>State Farm to stop accepting homeowners insura...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/05/27/state-farm-to-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How Janie Deegan built Janie's Life-Changing B...</td>\n",
       "      <td>15 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/05/27/how-janie-deeg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top 10 cheapest places in the U.S. to buy a be...</td>\n",
       "      <td>15 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/05/27/cheapest-place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mark Cuban calls Elon Musk’s Twitter algorithm...</td>\n",
       "      <td>15 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/05/27/mark-cuban-say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3 investing tips as the federal debt ceiling '...</td>\n",
       "      <td>16 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/05/27/how-to-invest-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Microsoft keyboard users are ‘devastated’ afte...</td>\n",
       "      <td>16 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/05/27/microsoft-keyb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Steve Adcock: The 3 'stupidest' myths I've hea...</td>\n",
       "      <td>16 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/05/27/steve-adcock-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chip stocks AMD and Nvidia are among the most ...</td>\n",
       "      <td>17 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/05/27/chip-stocks-am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Analysts are pounding the table for these must...</td>\n",
       "      <td>17 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/05/27/analysts-are-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lower-income consumers pay for wealthy's credi...</td>\n",
       "      <td>17 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/05/27/lower-income-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How Mastercard has outperformed Visa</td>\n",
       "      <td>17 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/05/27/how-mastercard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Why commercial real estate firms are joining t...</td>\n",
       "      <td>17 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/05/27/commercial-rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Japan stocks are on fire this year. Why the ra...</td>\n",
       "      <td>17 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/05/27/japan-stocks-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Marvell Technology shares surge after earnings...</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>https://www.cnbc.com/2023/05/26/marvell-techno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The tech trade is back, driven by A.I. craze a...</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>https://www.cnbc.com/2023/05/26/tech-stocks-ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Next week hints at only short-lived debt deal ...</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>https://www.cnbc.com/2023/05/26/next-week-hint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Treasury says it could run out of money June 5...</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>https://www.cnbc.com/2023/05/26/treasury-says-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Disney rips DeSantis bid to disqualify judge i...</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>https://www.cnbc.com/2023/05/26/disney-rips-de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Why the pause on student loan payments has bee...</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>https://www.cnbc.com/2023/05/26/the-pause-on-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Facebook-Giphy sale shows how fear of regulato...</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>https://www.cnbc.com/2023/05/26/facebook-giphy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>JPMorgan Chase cut about 500 jobs this week, i...</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>https://www.cnbc.com/2023/05/26/job-cuts-jpmor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Is there a 'right' age for kids to be on socia...</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>https://www.cnbc.com/2023/05/26/the-right-age-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>A.I. excitement leads to a winning week for Nv...</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>https://www.cnbc.com/2023/05/26/ai-excitement-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Nvidia shares jumped 25% this week — and got c...</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>https://www.cnbc.com/2023/05/26/nvidia-shares-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Needham says this stock plays the 'almost perf...</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>https://www.cnbc.com/2023/05/26/needham-this-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AI is the latest buzzword in tech—but before i...</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>https://www.cnbc.com/2023/05/26/chatgpt-machin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Taylor Swift to Metallica: Top 10 most in-dema...</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>https://www.cnbc.com/2023/05/26/taylor-swift-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Investors shifted into these gold and small-ca...</td>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>https://www.cnbc.com/2023/05/26/investors-shif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headlines          Time  \\\n",
       "0   Turkey votes in runoff election after candidat...    57 Min Ago   \n",
       "1   White House and Republicans reach a tentative ...   4 Hours Ago   \n",
       "2   State Farm to stop accepting homeowners insura...  10 Hours Ago   \n",
       "3   How Janie Deegan built Janie's Life-Changing B...  15 Hours Ago   \n",
       "4   Top 10 cheapest places in the U.S. to buy a be...  15 Hours Ago   \n",
       "5   Mark Cuban calls Elon Musk’s Twitter algorithm...  15 Hours Ago   \n",
       "6   3 investing tips as the federal debt ceiling '...  16 Hours Ago   \n",
       "7   Microsoft keyboard users are ‘devastated’ afte...  16 Hours Ago   \n",
       "8   Steve Adcock: The 3 'stupidest' myths I've hea...  16 Hours Ago   \n",
       "9   Chip stocks AMD and Nvidia are among the most ...  17 Hours Ago   \n",
       "10  Analysts are pounding the table for these must...  17 Hours Ago   \n",
       "11  Lower-income consumers pay for wealthy's credi...  17 Hours Ago   \n",
       "12               How Mastercard has outperformed Visa  17 Hours Ago   \n",
       "13  Why commercial real estate firms are joining t...  17 Hours Ago   \n",
       "14  Japan stocks are on fire this year. Why the ra...  17 Hours Ago   \n",
       "15  Marvell Technology shares surge after earnings...  May 26, 2023   \n",
       "16  The tech trade is back, driven by A.I. craze a...  May 26, 2023   \n",
       "17  Next week hints at only short-lived debt deal ...  May 26, 2023   \n",
       "18  Treasury says it could run out of money June 5...  May 26, 2023   \n",
       "19  Disney rips DeSantis bid to disqualify judge i...  May 26, 2023   \n",
       "20  Why the pause on student loan payments has bee...  May 26, 2023   \n",
       "21  Facebook-Giphy sale shows how fear of regulato...  May 26, 2023   \n",
       "22  JPMorgan Chase cut about 500 jobs this week, i...  May 26, 2023   \n",
       "23  Is there a 'right' age for kids to be on socia...  May 26, 2023   \n",
       "24  A.I. excitement leads to a winning week for Nv...  May 26, 2023   \n",
       "25  Nvidia shares jumped 25% this week — and got c...  May 26, 2023   \n",
       "26  Needham says this stock plays the 'almost perf...  May 26, 2023   \n",
       "27  AI is the latest buzzword in tech—but before i...  May 26, 2023   \n",
       "28  Taylor Swift to Metallica: Top 10 most in-dema...  May 26, 2023   \n",
       "29  Investors shifted into these gold and small-ca...  May 26, 2023   \n",
       "\n",
       "                                            News Link  \n",
       "0   https://www.cnbc.com/2023/05/28/turkey-electio...  \n",
       "1   https://www.cnbc.com/2023/05/27/white-house-an...  \n",
       "2   https://www.cnbc.com/2023/05/27/state-farm-to-...  \n",
       "3   https://www.cnbc.com/2023/05/27/how-janie-deeg...  \n",
       "4   https://www.cnbc.com/2023/05/27/cheapest-place...  \n",
       "5   https://www.cnbc.com/2023/05/27/mark-cuban-say...  \n",
       "6   https://www.cnbc.com/2023/05/27/how-to-invest-...  \n",
       "7   https://www.cnbc.com/2023/05/27/microsoft-keyb...  \n",
       "8   https://www.cnbc.com/2023/05/27/steve-adcock-t...  \n",
       "9   https://www.cnbc.com/2023/05/27/chip-stocks-am...  \n",
       "10  https://www.cnbc.com/2023/05/27/analysts-are-p...  \n",
       "11  https://www.cnbc.com/2023/05/27/lower-income-a...  \n",
       "12  https://www.cnbc.com/2023/05/27/how-mastercard...  \n",
       "13  https://www.cnbc.com/2023/05/27/commercial-rea...  \n",
       "14  https://www.cnbc.com/2023/05/27/japan-stocks-a...  \n",
       "15  https://www.cnbc.com/2023/05/26/marvell-techno...  \n",
       "16  https://www.cnbc.com/2023/05/26/tech-stocks-ar...  \n",
       "17  https://www.cnbc.com/2023/05/26/next-week-hint...  \n",
       "18  https://www.cnbc.com/2023/05/26/treasury-says-...  \n",
       "19  https://www.cnbc.com/2023/05/26/disney-rips-de...  \n",
       "20  https://www.cnbc.com/2023/05/26/the-pause-on-s...  \n",
       "21  https://www.cnbc.com/2023/05/26/facebook-giphy...  \n",
       "22  https://www.cnbc.com/2023/05/26/job-cuts-jpmor...  \n",
       "23  https://www.cnbc.com/2023/05/26/the-right-age-...  \n",
       "24  https://www.cnbc.com/2023/05/26/ai-excitement-...  \n",
       "25  https://www.cnbc.com/2023/05/26/nvidia-shares-...  \n",
       "26  https://www.cnbc.com/2023/05/26/needham-this-s...  \n",
       "27  https://www.cnbc.com/2023/05/26/chatgpt-machin...  \n",
       "28  https://www.cnbc.com/2023/05/26/taylor-swift-b...  \n",
       "29  https://www.cnbc.com/2023/05/26/investors-shif...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news('https://www.cnbc.com/world/?region=world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289204c7",
   "metadata": {},
   "source": [
    "6) Python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles with data frame of scraped data of below mentioned details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c609c",
   "metadata": {},
   "source": [
    "i) Paper Title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beb6bee",
   "metadata": {},
   "source": [
    "ii) Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102a4f5",
   "metadata": {},
   "source": [
    "iii) Published Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b06e6",
   "metadata": {},
   "source": [
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3db81746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AI_articles(url):\n",
    "\n",
    "    # Sending a request to a website for scraping data\n",
    "    page=requests.get(url)\n",
    "    page\n",
    "\n",
    "    # Extracting content of the website\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    soup\n",
    "    \n",
    "    # i) Paper Title\n",
    "    \n",
    "    # Extracting paper title data\n",
    "    paper_title=[]\n",
    "    for i in soup.find_all('h2',class_='sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg'):\n",
    "        paper_title.append(i.text)\n",
    "    paper_title\n",
    "    \n",
    "    # ii) Authors\n",
    "    \n",
    "    # Extracting authors data\n",
    "    authors=[]\n",
    "    for i in soup.find_all('span',class_='sc-1w3fpd7-0 dnCnAO'):\n",
    "        authors.append(i.text)\n",
    "    authors\n",
    "    \n",
    "    # iii) Published Date\n",
    "    \n",
    "    # Extracting data of published dates of respective articles\n",
    "    published_date=[]\n",
    "    for i in soup.find_all('span',class_='sc-1thf9ly-2 dvggWt'):\n",
    "        published_date.append(i.text)\n",
    "    published_date\n",
    "    \n",
    "    # iv) Paper URL\n",
    "    \n",
    "    # Extracting URLs of respective articles\n",
    "    paper_url=[]\n",
    "    for i in soup.find_all('a',class_='sc-5smygv-0 fIXTHm'):\n",
    "        paper_url.append(i.get('href'))\n",
    "    paper_url\n",
    "    \n",
    "    # Creating DataFrame\n",
    "    df=pd.DataFrame({'Paper Title':paper_title,'Authors':authors,'Published Date':published_date,'Paper URL':paper_url})\n",
    "\n",
    "    # Displaying DataFrame\n",
    "    print(\"List of the most downloaded articles from Artificial Intelligence in the last 90 days\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b4979fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of the most downloaded articles from Artificial Intelligence in the last 90 days\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Paper URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>David Silver, Satinder Singh, Doina Precup, Ri...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Tim Miller</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Margaret A. Boden</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Guni Sharon, Roni Stern, Ariel Felner, Nathan ...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Knowledge graphs as tools for explainable mach...</td>\n",
       "      <td>Ilaria Tiddi, Stefan Schlobach</td>\n",
       "      <td>January 2022</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Henry Prakken, Giovanni Sartor</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Richard S. Sutton, Doina Precup, Satinder Singh</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Kjersti Aas, Martin Jullum, Anders Løland</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Wenhan Luo, Junliang Xing and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Saurabh Arora, Prashant Doshi</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>Jasper van der Waa, Elisabeth Nieuwburg, Anita...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Explainable AI tools for legal reasoning about...</td>\n",
       "      <td>Joe Collenette, Katie Atkinson, Trevor Bench-C...</td>\n",
       "      <td>April 2023</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hard choices in artificial intelligence</td>\n",
       "      <td>Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz</td>\n",
       "      <td>November 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Assessing the communication gap between AI mod...</td>\n",
       "      <td>Oskar Wysocki, Jessica Katharine Davies and 5 ...</td>\n",
       "      <td>March 2023</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Nolan Bard, Jakob N. Foerster and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Ron Kohavi, George H. John</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Séverin Lemaignan, Mathieu Warnier and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The multifaceted impact of Ada Lovelace in the...</td>\n",
       "      <td>Luigia Carlucci Aiello</td>\n",
       "      <td>June 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Robot ethics: Mapping the issues for a mechani...</td>\n",
       "      <td>Patrick Lin, Keith Abney, George Bekey</td>\n",
       "      <td>April 2011</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Reward (Mis)design for autonomous driving</td>\n",
       "      <td>W. Bradley Knox, Alessandro Allievi and 3 more</td>\n",
       "      <td>March 2023</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Planning and acting in partially observable st...</td>\n",
       "      <td>Leslie Pack Kaelbling, Michael L. Littman, Ant...</td>\n",
       "      <td>May 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What do we want from Explainable Artificial In...</td>\n",
       "      <td>Markus Langer, Daniel Oster and 6 more</td>\n",
       "      <td>July 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paper Title  \\\n",
       "0                                    Reward is enough   \n",
       "1   Explanation in artificial intelligence: Insigh...   \n",
       "2              Creativity and artificial intelligence   \n",
       "3   Conflict-based search for optimal multi-agent ...   \n",
       "4   Knowledge graphs as tools for explainable mach...   \n",
       "5   Law and logic: A review from an argumentation ...   \n",
       "6   Between MDPs and semi-MDPs: A framework for te...   \n",
       "7   Explaining individual predictions when feature...   \n",
       "8       Multiple object tracking: A literature review   \n",
       "9   A survey of inverse reinforcement learning: Ch...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11  Explainable AI tools for legal reasoning about...   \n",
       "12            Hard choices in artificial intelligence   \n",
       "13  Assessing the communication gap between AI mod...   \n",
       "14  Explaining black-box classifiers using post-ho...   \n",
       "15  The Hanabi challenge: A new frontier for AI re...   \n",
       "16              Wrappers for feature subset selection   \n",
       "17  Artificial cognition for social human–robot in...   \n",
       "18  A review of possible effects of cognitive bias...   \n",
       "19  The multifaceted impact of Ada Lovelace in the...   \n",
       "20  Robot ethics: Mapping the issues for a mechani...   \n",
       "21          Reward (Mis)design for autonomous driving   \n",
       "22  Planning and acting in partially observable st...   \n",
       "23  What do we want from Explainable Artificial In...   \n",
       "\n",
       "                                              Authors  Published Date  \\\n",
       "0   David Silver, Satinder Singh, Doina Precup, Ri...    October 2021   \n",
       "1                                         Tim Miller    February 2019   \n",
       "2                                  Margaret A. Boden      August 1998   \n",
       "3   Guni Sharon, Roni Stern, Ariel Felner, Nathan ...   February 2015   \n",
       "4                     Ilaria Tiddi, Stefan Schlobach     January 2022   \n",
       "5                     Henry Prakken, Giovanni Sartor     October 2015   \n",
       "6    Richard S. Sutton, Doina Precup, Satinder Singh      August 1999   \n",
       "7          Kjersti Aas, Martin Jullum, Anders Løland   September 2021   \n",
       "8                Wenhan Luo, Junliang Xing and 4 more      April 2021   \n",
       "9                      Saurabh Arora, Prashant Doshi      August 2021   \n",
       "10  Jasper van der Waa, Elisabeth Nieuwburg, Anita...   February 2021   \n",
       "11  Joe Collenette, Katie Atkinson, Trevor Bench-C...      April 2023   \n",
       "12  Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz    November 2021   \n",
       "13  Oskar Wysocki, Jessica Katharine Davies and 5 ...      March 2023   \n",
       "14  Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...        May 2021   \n",
       "15          Nolan Bard, Jakob N. Foerster and 13 more      March 2020   \n",
       "16                        Ron Kohavi, George H. John    December 1997   \n",
       "17      Séverin Lemaignan, Mathieu Warnier and 3 more       June 2017   \n",
       "18   Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz        June 2021   \n",
       "19                            Luigia Carlucci Aiello        June 2016   \n",
       "20            Patrick Lin, Keith Abney, George Bekey       April 2011   \n",
       "21     W. Bradley Knox, Alessandro Allievi and 3 more      March 2023   \n",
       "22  Leslie Pack Kaelbling, Michael L. Littman, Ant...        May 1998   \n",
       "23             Markus Langer, Daniel Oster and 6 more       July 2021   \n",
       "\n",
       "                                            Paper URL  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_articles('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2db1a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "7) Python program to scrape mentioned details from dineout.co.in with data frame of scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7997c85",
   "metadata": {},
   "source": [
    "i) Restaurant name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5882b879",
   "metadata": {},
   "source": [
    "ii) Cuisine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b0039",
   "metadata": {},
   "source": [
    "iii) Location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f7ee7",
   "metadata": {},
   "source": [
    "iv) Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425996da",
   "metadata": {},
   "source": [
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cee81b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restaurants_details(url):\n",
    "    \n",
    "    # Sending a request to a website for scraping data\n",
    "    page=requests.get(url)\n",
    "    page\n",
    "\n",
    "    # Extracting content of the website\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    soup\n",
    "    \n",
    "    # i) Restaurant name\n",
    "    \n",
    "    # Extracting names of the restaurants\n",
    "    restaurant_name=[]\n",
    "    for i in soup.find_all('a',class_='restnt-name ellipsis'):\n",
    "        restaurant_name.append(i.text)\n",
    "    restaurant_name\n",
    "    \n",
    "    # ii) Cuisine\n",
    "    \n",
    "    # Extracting data of available cuisines in various restaurants\n",
    "    cuisine=[]\n",
    "    for i in soup.find_all('span',class_='double-line-ellipsis'):\n",
    "        cuisine.append(i.text.split('|')[1])\n",
    "    cuisine\n",
    "    \n",
    "    # iii) Location\n",
    "    \n",
    "    # Extracting data of locations of the restaurants\n",
    "    location=[]\n",
    "    for i in soup.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "        location.append(i.text)\n",
    "    location\n",
    "    \n",
    "    # iv) Ratings\n",
    "    \n",
    "    # Extracting data of ratings obtained by the restaurants\n",
    "    ratings=[]\n",
    "    for i in soup.find_all('div',class_='restnt-rating rating-4'):\n",
    "        ratings.append(i.text)\n",
    "    ratings\n",
    "    \n",
    "    # v) Image URL\n",
    "    \n",
    "    # Extraing image-URLs of the restaurants\n",
    "    image_url=[]\n",
    "    for i in soup.find_all('img',class_=\"no-img\"):\n",
    "        image_url.append(i.get('data-src'))\n",
    "    image_url\n",
    "    \n",
    "    # Creating DataFrame\n",
    "    df=pd.DataFrame({'Restaurant Name':restaurant_name,'Cuisine':cuisine,'Location':location,'Ratings':ratings,'Image-URL':image_url})\n",
    "\n",
    "    # Displaying DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e56f2f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Image-URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Castle's Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Restaurant Name                        Cuisine  \\\n",
       "0                   Castle Barbeque          Chinese, North Indian   \n",
       "1                   Jungle Jamboree   North Indian, Asian, Italian   \n",
       "2                        Cafe Knosh           Italian, Continental   \n",
       "3                 Castle's Barbeque          Chinese, North Indian   \n",
       "4              The Barbeque Company          North Indian, Chinese   \n",
       "5                       India Grill          North Indian, Italian   \n",
       "6                    Delhi Barbeque                   North Indian   \n",
       "7  The Monarch - Bar Be Que Village                   North Indian   \n",
       "8                 Indian Grill Room          North Indian, Mughlai   \n",
       "\n",
       "                                            Location Ratings  \\\n",
       "0                     Connaught Place, Central Delhi       4   \n",
       "1             3CS Mall,Lajpat Nagar - 3, South Delhi     3.9   \n",
       "2  The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
       "3             Pacific Mall,Tagore Garden, West Delhi     3.9   \n",
       "4                 Gardens Galleria,Sector 38A, Noida     3.9   \n",
       "5               Hilton Garden Inn,Saket, South Delhi     3.9   \n",
       "6     Taurus Sarovar Portico,Mahipalpur, South Delhi     3.7   \n",
       "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
       "8   Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
       "\n",
       "                                           Image-URL  \n",
       "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants_details('https://www.dineout.co.in/delhi-restaurants/buffet-special')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
